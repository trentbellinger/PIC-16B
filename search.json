[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Homework6/HW6.html",
    "href": "posts/Homework6/HW6.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "We first must import all necessary packages that will be used in the creation of the model.\n\n\n# packages to form the datasets:\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\n# packages to build and test the models\nimport keras\nfrom keras import layers, losses\nfrom keras import utils\nfrom keras.layers import TextVectorization\n\n# packages to visualize results:\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nprint(\"keras version \", keras.__version__)\n\nkeras version  3.0.5\n\n\n\nIt is also specified above that we are using keras 3 in this homework.\nNow that we have all the necessary packages imported, we will start looking at the data."
  },
  {
    "objectID": "posts/Homework6/HW6.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-6.-i-will-describe-the-creation-of-a-machine-learning-model-that-can-distinguish-between-fake-and-real-news.",
    "href": "posts/Homework6/HW6.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-6.-i-will-describe-the-creation-of-a-machine-learning-model-that-can-distinguish-between-fake-and-real-news.",
    "title": "Fake News Classification",
    "section": "",
    "text": "We first must import all necessary packages that will be used in the creation of the model.\n\n\n# packages to form the datasets:\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\n# packages to build and test the models\nimport keras\nfrom keras import layers, losses\nfrom keras import utils\nfrom keras.layers import TextVectorization\n\n# packages to visualize results:\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nprint(\"keras version \", keras.__version__)\n\nkeras version  3.0.5\n\n\n\nIt is also specified above that we are using keras 3 in this homework.\nNow that we have all the necessary packages imported, we will start looking at the data."
  },
  {
    "objectID": "posts/Homework6/HW6.html#make_dataset-function",
    "href": "posts/Homework6/HW6.html#make_dataset-function",
    "title": "Fake News Classification",
    "section": "make_dataset() function",
    "text": "make_dataset() function\n\nWe will create a function that takes in a pandas dataframe, converts all of its strings to lowercase, removes stop words, and outputs a tensorflow dataset that is batched to optimize training time.\nThis function is outlined below.\n\n\ndef make_dataset(df):\n    '''\n    Converts all strings to lowercase, removes stopwords, and returns a\n    tensorflow dataset.\n    Args:\n        df: a pandas dataframe of article data\n    Returns:\n        a tensorflow dataset with inputs title and text and output fake\n    '''\n    # convert the title and text columns to lowercase\n    df['title'] = df['title'].str.lower()\n    df['text'] = df['text'].str.lower()\n\n    # remove all stopwords from the title and text columns\n    stop = stopwords.words('english')\n    stop_fun = lambda x: ' '.join([word for word in x.split() if word not in stop])\n    df['title'] = df['title'].apply(stop_fun)\n    df['text'] = df['text'].apply(stop_fun)\n\n    # create a tensorflow dataset with inputs title and text and output fake\n    output = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                'title':df[['title']],\n                'text':df[['text']]\n            },\n            {\n                'fake':df[['fake']]\n            }\n        )\n    )\n    # batch the dataset by 100 and return\n    return output.batch(100)\n\n\nNow that we have a function to create a dataset, we will run it on our pandas dataframe from before.\n\n\n# create our main tensorflow dataset\ndf = make_dataset(df)\n\n\nWe now have a tensorflow dataset of our data, so we need to split it into training and validation to proceed with model creation."
  },
  {
    "objectID": "posts/Homework6/HW6.html#validation-data",
    "href": "posts/Homework6/HW6.html#validation-data",
    "title": "Fake News Classification",
    "section": "Validation Data",
    "text": "Validation Data\n\nWe will split the data into 20% validation and 80% training data to prepare for the creation of our models.\n\n\n# specify validation size to be 20% of dataset\nval_size = int(0.2*len(df))\n\n# get validation and training datasets\nval = df.take(val_size)\ntrain = df.skip(val_size)\n\n\nNow we have two datasets that we can train our model with, so we should establish a baseline performance expectation for our model"
  },
  {
    "objectID": "posts/Homework6/HW6.html#base-rate",
    "href": "posts/Homework6/HW6.html#base-rate",
    "title": "Fake News Classification",
    "section": "Base Rate",
    "text": "Base Rate\n\nWe will examine the proportion of fake and real news in our training data.\n\n\nout = np.empty(0)\n\n# for each entry to the dataset\nfor article, fake in train:\n    # add the \"fake\" column entries to the output array\n    out = np.append(out, fake['fake'].numpy().flatten())\n\nprint(\"Proportion of fake news: \", np.mean(out))\nprint(\"Proportion of real news: \", 1 - np.mean(out))\n\nProportion of fake news:  0.5243746169703047\nProportion of real news:  0.47562538302969526\n\n\n\nThe proportion of fake news in our training data is 0.5244, so our model must have accuracy higher than 0.5244 to outperform the base rate."
  },
  {
    "objectID": "posts/Homework6/HW6.html#text-vectorization",
    "href": "posts/Homework6/HW6.html#text-vectorization",
    "title": "Fake News Classification",
    "section": "Text Vectorization",
    "text": "Text Vectorization\n\nWe now need to convert the string inputs in our model into integers in order to input our data into the model.\nWe will do this by ordering the words by most to least common, keeping only the top 2000 most common words.\nThis process is outlined below.\n\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\n# convert the strings to lowercase and remove all punctuation\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n# create a layer that converrts strings to integers\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# apply the layer to the title and text columns in the training data\nvectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\nvectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\n\nNow that we have successfully vectorized the strings in our training data, we can proceed with model testing."
  },
  {
    "objectID": "posts/Homework6/HW6.html#model-using-only-title",
    "href": "posts/Homework6/HW6.html#model-using-only-title",
    "title": "Fake News Classification",
    "section": "Model Using Only Title",
    "text": "Model Using Only Title\n\nOur first model will only use the title of the article to predict whether it is fake or real.\nThe model is created below and its summary is outputted.\n\n\n# create model using only title as a predictor and output its summary\nmodel_title = get_model(\"title\")\nmodel_title.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ title (InputLayer)              │ (None, 1)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ text_vectorization              │ (None, 500)            │             0 │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (Embedding)           │ (None, 500, 3)         │         6,000 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 500, 3)         │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (None, 3)              │             0 │\n│ (GlobalAveragePooling1D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 3)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 32)             │           128 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 32)             │         1,056 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fake (Dense)                    │ (None, 2)              │            66 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 7,250 (28.32 KB)\n\n\n\n Trainable params: 7,250 (28.32 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nWe will now display the model in an easy-to read manner, done below.\n\n\n# plot the model layers in a flow chart format\nutils.plot_model(model_title, \"model_title.png\",\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\nAs we can see, the model has the following layers: input, text vectorization, embedding, dropout, global average pooling 1D, dropout, 3 dense.\nNow that we have created the model, we will fit the model using the training and validation sets above and 50 epochs.\n\n\n# fit the model on our training data, testing it on the validation data\nhistory_title = fit_model(model_title, train, val, 50)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 7ms/step - accuracy: 0.5240 - loss: 0.6922 - val_accuracy: 0.5173 - val_loss: 0.6892\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5633 - loss: 0.6810 - val_accuracy: 0.5731 - val_loss: 0.6190\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6860 - loss: 0.5798 - val_accuracy: 0.7824 - val_loss: 0.4840\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.7370 - loss: 0.5227 - val_accuracy: 0.7853 - val_loss: 0.4626\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.7598 - loss: 0.4886 - val_accuracy: 0.7938 - val_loss: 0.4436\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.7673 - loss: 0.4812 - val_accuracy: 0.8069 - val_loss: 0.4216\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7782 - loss: 0.4603 - val_accuracy: 0.8218 - val_loss: 0.4080\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7811 - loss: 0.4598 - val_accuracy: 0.8156 - val_loss: 0.4096\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7982 - loss: 0.4363 - val_accuracy: 0.8358 - val_loss: 0.3883\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.7953 - loss: 0.4386 - val_accuracy: 0.8407 - val_loss: 0.3711\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8021 - loss: 0.4322 - val_accuracy: 0.8147 - val_loss: 0.4076\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8009 - loss: 0.4259 - val_accuracy: 0.8498 - val_loss: 0.3626\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8193 - loss: 0.4018 - val_accuracy: 0.8564 - val_loss: 0.3437\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8054 - loss: 0.4173 - val_accuracy: 0.7980 - val_loss: 0.4135\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8258 - loss: 0.3883 - val_accuracy: 0.8284 - val_loss: 0.3629\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8246 - loss: 0.3837 - val_accuracy: 0.7960 - val_loss: 0.4116\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8334 - loss: 0.3684 - val_accuracy: 0.8069 - val_loss: 0.3955\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8409 - loss: 0.3552 - val_accuracy: 0.8751 - val_loss: 0.2955\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8478 - loss: 0.3413 - val_accuracy: 0.8496 - val_loss: 0.3242\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8517 - loss: 0.3375 - val_accuracy: 0.8418 - val_loss: 0.3339\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8643 - loss: 0.3227 - val_accuracy: 0.8213 - val_loss: 0.3710\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8608 - loss: 0.3216 - val_accuracy: 0.8547 - val_loss: 0.3114\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8611 - loss: 0.3118 - val_accuracy: 0.8589 - val_loss: 0.3041\n\n\n\nThe stop condition has stopped the fitting of the model after 23 epochs.\nAs we can see, the model stabilized between 0.82 and 0.86 validation accuracy. This is pretty good, but we can certainly do better using more predictors.\nWe will now visualize the accuracy of the model on the training and validation data.\n\n\n# plot training and validation accuracy calculated during model fitting\nplot_model_accuracy(history_title, \"Title\")\n\n\n\n\n\n\n\n\n\nAs we can see, the validation accuracy is rather unpredictable throughout the model fitting process, but it stabilizes at about 0.85.\nThroughout the model training process, the validation accuracy is about the same as the training accuracy. Hence, we do not have an issue with overfitting in this model.\nWe will now attempt to create a better model."
  },
  {
    "objectID": "posts/Homework6/HW6.html#model-using-only-text",
    "href": "posts/Homework6/HW6.html#model-using-only-text",
    "title": "Fake News Classification",
    "section": "Model Using Only Text",
    "text": "Model Using Only Text\n\nOur second model will only use the text of the article to predict whether it is fake or real.\nThe model is created below and its summary is outputted.\n\n\n# create model using only title as a predictor and output its summary\nmodel_text = get_model(\"text\")\nmodel_text.summary()\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 32)                  │           1,056 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              66 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 7,250 (28.32 KB)\n\n\n\n Trainable params: 7,250 (28.32 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nWe will now display the model in an easy-to-read manner, done below.\n\n\n# plot the model layers in a flow chart format\nutils.plot_model(model_text, \"model_text.png\",\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\nAs we can see, the model has the following layers: input, text vectorization, embedding, dropout, global average pooling 1D, dropout, 3 dense.\nNow that we have created the model, we will fit the model using the training and validation sets above and 50 epochs.\n\n\n# fit the model on our training data, testing it on the validation data\nhistory_text = fit_model(model_text, train, val, 50)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.5838 - loss: 0.6519 - val_accuracy: 0.8429 - val_loss: 0.3745\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.8719 - loss: 0.3239 - val_accuracy: 0.8991 - val_loss: 0.2387\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9171 - loss: 0.2319 - val_accuracy: 0.9127 - val_loss: 0.2092\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9284 - loss: 0.1982 - val_accuracy: 0.9193 - val_loss: 0.1918\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9406 - loss: 0.1791 - val_accuracy: 0.9231 - val_loss: 0.1851\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9416 - loss: 0.1665 - val_accuracy: 0.9242 - val_loss: 0.1828\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9450 - loss: 0.1587 - val_accuracy: 0.9562 - val_loss: 0.1539\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9481 - loss: 0.1480 - val_accuracy: 0.9367 - val_loss: 0.1609\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9499 - loss: 0.1396 - val_accuracy: 0.9371 - val_loss: 0.1575\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9550 - loss: 0.1334 - val_accuracy: 0.9624 - val_loss: 0.1386\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9546 - loss: 0.1280 - val_accuracy: 0.9427 - val_loss: 0.1455\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9603 - loss: 0.1199 - val_accuracy: 0.9416 - val_loss: 0.1459\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9563 - loss: 0.1214 - val_accuracy: 0.9418 - val_loss: 0.1447\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9596 - loss: 0.1140 - val_accuracy: 0.9631 - val_loss: 0.1337\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9611 - loss: 0.1102 - val_accuracy: 0.9407 - val_loss: 0.1492\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9622 - loss: 0.1072 - val_accuracy: 0.9400 - val_loss: 0.1528\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9638 - loss: 0.1045 - val_accuracy: 0.9391 - val_loss: 0.1529\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9659 - loss: 0.1006 - val_accuracy: 0.9431 - val_loss: 0.1425\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9635 - loss: 0.1035 - val_accuracy: 0.9480 - val_loss: 0.1295\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9687 - loss: 0.0925 - val_accuracy: 0.9422 - val_loss: 0.1474\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9652 - loss: 0.0954 - val_accuracy: 0.9498 - val_loss: 0.1258\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9670 - loss: 0.0943 - val_accuracy: 0.9718 - val_loss: 0.1146\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9689 - loss: 0.0931 - val_accuracy: 0.9478 - val_loss: 0.1313\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9719 - loss: 0.0845 - val_accuracy: 0.9398 - val_loss: 0.1534\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9690 - loss: 0.0870 - val_accuracy: 0.9431 - val_loss: 0.1481\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9710 - loss: 0.0833 - val_accuracy: 0.9467 - val_loss: 0.1355\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9701 - loss: 0.0813 - val_accuracy: 0.9433 - val_loss: 0.1454\n\n\n\nThe stop condition has stopped the fitting of the model after 27 epochs.\nAs we can see, the model stabilized between 0.93 and 0.95 validation accuracy. This is very good, but we may be able to do even better using both the title and text predictors.\nWe will now visualize the accuracy of the model on the training and validation data.\n\n\n# plot training and validation accuracy calculated during model fitting\nplot_model_accuracy(history_text, \"Text\")\n\n\n\n\n\n\n\n\n\nThroughout the model fitting process, the validation accuracy is slightly lower than the trainign accuracy. Hence, we may have a slight issue with overfitting in this model.\nThe validation accuracy seems to be very steady throughout the fitting process, staying between 0.92 and 0.95. Ths is very promising.\nWe will now attempt to create a better model."
  },
  {
    "objectID": "posts/Homework6/HW6.html#model-using-both-title-and-text",
    "href": "posts/Homework6/HW6.html#model-using-both-title-and-text",
    "title": "Fake News Classification",
    "section": "Model Using Both Title and Text",
    "text": "Model Using Both Title and Text\n\nOur third and final model will use both the title and the text of an article to predict whether it is fake or real.\nThe model is created below and its summary is outputted.\n\n\n# create model using title and text as predictors and output its summary\nmodel_both = get_model(\"both\")\nmodel_both.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)  │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text (InputLayer)   │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text_vectorization  │ (None, 500)       │          0 │ title[0][0],      │\n│ (TextVectorization) │                   │            │ text[0][0]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (None, 500, 3)    │      6,000 │ text_vectorizati… │\n│ (Embedding)         │                   │            │ text_vectorizati… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (Dropout) │ (None, 500, 3)    │          0 │ embedding[3][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (Dropout) │ (None, 500, 3)    │          0 │ embedding[4][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 3)         │          0 │ dropout_6[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 3)         │          0 │ dropout_8[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (Dropout) │ (None, 3)         │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_9 (Dropout) │ (None, 3)         │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (Dense)     │ (None, 32)        │        128 │ dropout_7[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_6 (Dense)     │ (None, 32)        │        128 │ dropout_9[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (None, 64)        │          0 │ dense_5[0][0],    │\n│ (Concatenate)       │                   │            │ dense_6[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_7 (Dense)     │ (None, 32)        │      2,080 │ concatenate_1[0]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ fake (Dense)        │ (None, 2)         │         66 │ dense_7[0][0]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 8,402 (32.82 KB)\n\n\n\n Trainable params: 8,402 (32.82 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nWe will now display the model in an easy-to-read manner, done below.\n\n\n# plot the model layers in a flow chart format\nutils.plot_model(model_both, \"model_both.png\",\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\nAs we can see, the model has two input layers: one for title and one for text. These layers share a text vectorization layer and an embedding layer. They then have separate layers for dropout, global average pooling, dropout, and dense. They are then cocatenated into a single layer. Then there is another two dense layers.\nNow that we have created the model, we will fit the model using the training and validation sets above and 50 epochs.\n\n\n# fit the model on our training data, testing it on the validation data\nhistory_both = fit_model(model_both, train, val, 50)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.7725 - loss: 0.5533 - val_accuracy: 0.9736 - val_loss: 0.1377\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9685 - loss: 0.1189 - val_accuracy: 0.9709 - val_loss: 0.0991\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9762 - loss: 0.0790 - val_accuracy: 0.9729 - val_loss: 0.0940\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9769 - loss: 0.0666 - val_accuracy: 0.9784 - val_loss: 0.0937\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9797 - loss: 0.0594 - val_accuracy: 0.9747 - val_loss: 0.0935\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9786 - loss: 0.0572 - val_accuracy: 0.9764 - val_loss: 0.0923\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9800 - loss: 0.0539 - val_accuracy: 0.9709 - val_loss: 0.1038\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9802 - loss: 0.0522 - val_accuracy: 0.9780 - val_loss: 0.1008\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9751 - loss: 0.0595 - val_accuracy: 0.9773 - val_loss: 0.0941\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9794 - loss: 0.0561 - val_accuracy: 0.9769 - val_loss: 0.0962\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9817 - loss: 0.0538 - val_accuracy: 0.9798 - val_loss: 0.0951\n\n\n\nThe stop condition has stopped the fitting of the model after 11 epochs.\nAs we can see, the model stabilized between 0.975 and 0.98 validation accuracy. This is our best result yet, so this will be our final model.\nWe will now visualize the accuracy of the model on the training and validation data.\n\n\n# plot training and validation accuracy calculated during model fitting\nplot_model_accuracy(history_both, \"Both Text and Title\")\n\n\n\n\n\n\n\n\n\nAs we can see, the validation accuracy is slightly lower than the training accuracy throughout the model fitting. This indicates that we may have a slight issue with overfitting in this model. However, the training and validation accuracy never differs by more than 0.01, so we can neglect this concern.\nThe validation accuracy is steady between 0.975 and 0.98 after the first 2 epochs. Ths is our best model yet, so we will use it as our final model."
  },
  {
    "objectID": "posts/Homework0/HW0.html",
    "href": "posts/Homework0/HW0.html",
    "title": "Homework 0",
    "section": "",
    "text": "We start by importing the necessary packages for this task. We will use pandas to load in the data into a data frame object, numpy to create an array for the visualization, and matplotlib.pyplot to visualize the data.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nIn order to figure out what type of visualization to make using the penguins dataset, we will first read in the dataset and look at its columns. This will allow us to formulate a question about the data that our visualization will answer.\n\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nThe dataset has columns studyName, Sample Number, Species, Region, Island, Stage, Individual ID, Clutch Completion, Date Egg, Culmen Length, Culmen Depth, Flipper Length, Body Mass, Sea, Delta 15 N, Delta 13 C, and Comments.\nFor this tutorial, we will specifically be looking at the Body Mass column because this can be indicative of the overall health and strength of the penguins.\nMore specifically, we will attempt to visualize how the body mass of a penguin depends on the sex of the penguin and the island that the penguin lives on.\nNow that we have our area of interest, we can formulate the question that our visualization will attempt to answer.\nThat question will be: How does the body mass of a penguin change based on its sex and the island that it lives on?\n\n\n\n\n\nWe start by grouping the observations in the dataset by island and sex and finding the median body mass of penguins for each category combination. This simple process is outlined below.\n\n\npenguins_by_island = penguins.groupby([\"Island\", \"Sex\"])[\"Body Mass (g)\"].median()\npenguins_by_island\n\nIsland     Sex   \nBiscoe     .         4875.0\n           FEMALE    4587.5\n           MALE      5350.0\nDream      FEMALE    3450.0\n           MALE      3950.0\nTorgersen  FEMALE    3400.0\n           MALE      4000.0\nName: Body Mass (g), dtype: float64\n\n\n\nAt first glance, it appears that male penguins at each island have higher mass that females on each island.\nIt also seems like the penguins on Biscoe Island have much higher mass than the other two islands.\nWe will see if we can visualize these observations.\n\n\n\n\n\nWe now have three variables that we want to visualize: median body mass, island, and sex.\nBody mass is a numeric variable, whereas island and sex are categorical variables.\nWith this specific combination of one numeric and two categorical variables, we can create a double bar graph to display the trends among these three variables.\nThe code for this bar graph is below. In short, this code block goes through this process:\n\n\nCreate a numpy array for the x-axis of the double bar plot.\nCreate the bars for the bar graph. The median body mass is the height of the bars and we have one bar for each sex-island combination.\nAdd labels to the visualization to make it more clear. The x-axis will be the different islands, the y-axis will be the median body mass, and the bars will be colored based on sex.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We will create a bar plot that groups by Island and Sex and shows the median \n# body mass of the penguins.\n\n# We will have 3 different ticks on the x-axis, each coprresponding to an island.\nX_axis = np.arange(3)\n\n# We plot the bars for the median body mass of female penguins by island, as \n# calculated above.\nplt.bar(X_axis - 0.2, [4587.5, 3450, 3400], 0.4, label = 'Female') \n\n# We plot the bars for the median body mass of male penguins by island, as \n# calculated above.\nplt.bar(X_axis + 0.2, [5350, 3950, 4000], 0.4, label = 'Male') \n  \nplt.xticks([0,1,2], [\"Biscoe\", \"Dream\", \"Torgersen\"]) \nplt.xlabel(\"Island\") \nplt.ylabel(\"Median Body Mass of Penguins\") \nplt.title(\"Body Mass of Penguins by Island and Sex\") \nplt.legend() \nplt.show() \n\n\n\n\n\n\n\n\n\nAs we can see from the visualization, Biscoe island has penguins with higher body mass than Dream island and Torgersen island. Also, male penguins on average have a greater body mass that female penguins on each island.\nThis visualization does not explain exactly why these trends are apparent, but it does allow us to speculate a little bit about the trends. One possible explanation is that there is more food for the penguins on Biscoe island, allowing the penguins to weight more."
  },
  {
    "objectID": "posts/Homework0/HW0.html#loading-in-and-viewing-the-data",
    "href": "posts/Homework0/HW0.html#loading-in-and-viewing-the-data",
    "title": "Homework 0",
    "section": "",
    "text": "In order to figure out what type of visualization to make using the penguins dataset, we will first read in the dataset and look at its columns. This will allow us to formulate a question about the data that our visualization will answer.\n\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nThe dataset has columns studyName, Sample Number, Species, Region, Island, Stage, Individual ID, Clutch Completion, Date Egg, Culmen Length, Culmen Depth, Flipper Length, Body Mass, Sea, Delta 15 N, Delta 13 C, and Comments.\nFor this tutorial, we will specifically be looking at the Body Mass column because this can be indicative of the overall health and strength of the penguins.\nMore specifically, we will attempt to visualize how the body mass of a penguin depends on the sex of the penguin and the island that the penguin lives on.\nNow that we have our area of interest, we can formulate the question that our visualization will attempt to answer.\nThat question will be: How does the body mass of a penguin change based on its sex and the island that it lives on?"
  },
  {
    "objectID": "posts/Homework0/HW0.html#data-preparation",
    "href": "posts/Homework0/HW0.html#data-preparation",
    "title": "Homework 0",
    "section": "",
    "text": "We start by grouping the observations in the dataset by island and sex and finding the median body mass of penguins for each category combination. This simple process is outlined below.\n\n\npenguins_by_island = penguins.groupby([\"Island\", \"Sex\"])[\"Body Mass (g)\"].median()\npenguins_by_island\n\nIsland     Sex   \nBiscoe     .         4875.0\n           FEMALE    4587.5\n           MALE      5350.0\nDream      FEMALE    3450.0\n           MALE      3950.0\nTorgersen  FEMALE    3400.0\n           MALE      4000.0\nName: Body Mass (g), dtype: float64\n\n\n\nAt first glance, it appears that male penguins at each island have higher mass that females on each island.\nIt also seems like the penguins on Biscoe Island have much higher mass than the other two islands.\nWe will see if we can visualize these observations."
  },
  {
    "objectID": "posts/Homework0/HW0.html#data-visualization",
    "href": "posts/Homework0/HW0.html#data-visualization",
    "title": "Homework 0",
    "section": "",
    "text": "We now have three variables that we want to visualize: median body mass, island, and sex.\nBody mass is a numeric variable, whereas island and sex are categorical variables.\nWith this specific combination of one numeric and two categorical variables, we can create a double bar graph to display the trends among these three variables.\nThe code for this bar graph is below. In short, this code block goes through this process:\n\n\nCreate a numpy array for the x-axis of the double bar plot.\nCreate the bars for the bar graph. The median body mass is the height of the bars and we have one bar for each sex-island combination.\nAdd labels to the visualization to make it more clear. The x-axis will be the different islands, the y-axis will be the median body mass, and the bars will be colored based on sex.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We will create a bar plot that groups by Island and Sex and shows the median \n# body mass of the penguins.\n\n# We will have 3 different ticks on the x-axis, each coprresponding to an island.\nX_axis = np.arange(3)\n\n# We plot the bars for the median body mass of female penguins by island, as \n# calculated above.\nplt.bar(X_axis - 0.2, [4587.5, 3450, 3400], 0.4, label = 'Female') \n\n# We plot the bars for the median body mass of male penguins by island, as \n# calculated above.\nplt.bar(X_axis + 0.2, [5350, 3950, 4000], 0.4, label = 'Male') \n  \nplt.xticks([0,1,2], [\"Biscoe\", \"Dream\", \"Torgersen\"]) \nplt.xlabel(\"Island\") \nplt.ylabel(\"Median Body Mass of Penguins\") \nplt.title(\"Body Mass of Penguins by Island and Sex\") \nplt.legend() \nplt.show() \n\n\n\n\n\n\n\n\n\nAs we can see from the visualization, Biscoe island has penguins with higher body mass than Dream island and Torgersen island. Also, male penguins on average have a greater body mass that female penguins on each island.\nThis visualization does not explain exactly why these trends are apparent, but it does allow us to speculate a little bit about the trends. One possible explanation is that there is more food for the penguins on Biscoe island, allowing the penguins to weight more."
  },
  {
    "objectID": "posts/Homework3/HW3.html#github-link",
    "href": "posts/Homework3/HW3.html#github-link",
    "title": "Flask Website With User Inputs",
    "section": "Github Link:",
    "text": "Github Link:\nhttps://github.com/trentbellinger/PIC16B-HW3"
  },
  {
    "objectID": "posts/Homework3/HW3.html#blog-post-link",
    "href": "posts/Homework3/HW3.html#blog-post-link",
    "title": "Flask Website With User Inputs",
    "section": "Blog Post Link:",
    "text": "Blog Post Link:\nhttps://trentbellinger.github.io/PIC-16B/posts/Homework3/HW3.html"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-app.py-header",
    "href": "posts/Homework3/HW3.html#the-app.py-header",
    "title": "Flask Website With User Inputs",
    "section": "The app.py Header",
    "text": "The app.py Header\n\nThe first thing that is needed in the python file for the app is an appropriate header that imports the necessary packages/functions and initializes the app. This header is shown below:\n\n\nfrom flask import Flask, render_template, request, g\nimport sqlite3\napp = Flask(__name__)"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-get_message_db-function",
    "href": "posts/Homework3/HW3.html#the-get_message_db-function",
    "title": "Flask Website With User Inputs",
    "section": "The get_message_db() Function:",
    "text": "The get_message_db() Function:\n\nThen, we move on to the creation of the first function in the python file. This function will create a SQL database for the messages that are inputted in the website if none has been created already. If a SQL database already exists, it will simply return that database. The code for this function is shown below:\n\n\ndef get_message_db():\n    '''\n    Handles the creation of a SQL database for the messages that are presented \n    in the website.\n    '''\n    try:\n        return g.message_db\n    except:\n        # if a database is not present, we create one\n        g.message_db = sqlite3.connect(\"messages_db.db\")\n        \n        # if a messages does not exist, create table with columns for handle and text\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (message TEXT, handle TEXT)'\n        \n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        \n        return g.message_db"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-insert_message-function",
    "href": "posts/Homework3/HW3.html#the-insert_message-function",
    "title": "Flask Website With User Inputs",
    "section": "The insert_message() Function:",
    "text": "The insert_message() Function:\n\nWe will now create a function that takes in a user’s input in the form of a request and inserts the user’s handle into the SQL database that is created with the get_message_db() function created above. The code for this function is shown below:\n\n\ndef insert_message(request):\n    '''\n    Extracts the message and handle from a request and inserts them into the messages \n    database.\n    Arguments:\n        request: a request that the user inputs to the webpage\n    Returns:\n        the message and handle from the request\n    '''\n    message = request.form[\"message\"]\n    handle = request.form[\"handle\"]\n    \n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # insert the handle and message into the database\n    ins = f'INSERT INTO messages (message, handle) VALUES (\"{message}\", \"{handle}\")'\n    cursor.execute(ins)\n    # commit the changes and close the connection\n    db.commit()\n    db.close()"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-random_messages-function",
    "href": "posts/Homework3/HW3.html#the-random_messages-function",
    "title": "Flask Website With User Inputs",
    "section": "The random_messages() Function:",
    "text": "The random_messages() Function:\n\nNow that we have a way to insert the user inputs into the database, we will create a function that allows us to extract random messages from the database.\nThe function random_messages(), shown below, outputs a list of the handles and messages for the n random entries of the database that are selected. If the database has less than n entries, then all of the handles and messages will be returned.\n\n\ndef random_messages(n):\n    '''\n    Returns a collection of n random messages that have been previously inputted \n    into the app.\n    Arguments:\n        n (int): the number of random messages to return\n    Returns:\n        messages (list): contains the name and message for the n random messages\n    '''\n    # Get the database connection\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # select the handle and message for n random entries in the messages table\n    cursor.execute(f\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT {n}\")\n    # save the message (output[0]) and the handle (output[1]) in a list\n    messages = [[output[0], output[1]] for output in cursor.fetchall()]\n    \n    # close the db connection\n    db.close()\n\n    return messages"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-render_submit_template-function",
    "href": "posts/Homework3/HW3.html#the-render_submit_template-function",
    "title": "Flask Website With User Inputs",
    "section": "The render_submit_template() Function:",
    "text": "The render_submit_template() Function:\n\nWe now have all the helper function for our website complete, so we can move on to creating functions to display certain pages in the website. OUr website will have two pages.\nThe first page will be a page that allows a user to submit their handle and message. This function most have two cases: GET and POST.\nIn the POST case, the function will insert the user’s inputted message into the database (this functionality is tied to the “Submit message” button in the website) and then render the submit.html file.\nIn the GET case, the function will simply render the submit.html file.\nThe creation of the submit.html file will be outlined later.\nThe code for the first page of the app is shown below:\n\n\n@app.route('/', methods = ['POST', 'GET'])\ndef render_submit_template():\n    '''Renders the submit.html file in the app, accounting for POST and GET requests.'''\n    if request.method == 'POST':\n        # for a POST request, we insert the message into the database\n        insert_message(request)\n        \n        msg = \"Thanks for your submission!\"\n        # render submit.html with a thank you message\n        return render_template('submit.html', msg = msg)\n    else:\n        # for a GET request, simply render submit.html\n        return render_template('submit.html')"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-view_random_messages-function",
    "href": "posts/Homework3/HW3.html#the-view_random_messages-function",
    "title": "Flask Website With User Inputs",
    "section": "The view_random_messages() Function:",
    "text": "The view_random_messages() Function:\n\nWe now want to create a page that allows users to see past submissions on the website.\nThis will be done by calling the random_messages() function created above and then rendering the view.html file with the outputtes messages.\nThe creation of the view.html file will be outliner later.\nThis code is shown below.\n\n\n@app.route('/view')\ndef view_random_messages():\n    '''\n    Creates a /view page of the app that displays 4 random messages that have been \n    previously inputted along with the name/handle of the person who submitted them.\n    '''\n    # get 4 random messages\n    messages = random_messages(4)\n    # render view.html with the 4 random messages\n    return render_template('view.html', messages = messages)"
  },
  {
    "objectID": "posts/Homework3/HW3.html#allowing-the-app-to-be-run-locally",
    "href": "posts/Homework3/HW3.html#allowing-the-app-to-be-run-locally",
    "title": "Flask Website With User Inputs",
    "section": "Allowing the App to be Run Locally:",
    "text": "Allowing the App to be Run Locally:\n\nFinally, we need a place to run the app. The following function allows the app to be run on a local device by simply running the python file in the terminal.\n\n\nif __name__ == '__main__':\n    '''\n    How to run the app. I had to use port=4999 for it to run on my \n    device (5000 was not working).\n    '''\n    app.run(host='0.0.0.0', port=4999, debug=True)\n\n\nThis concludes the creation of the app.py file. We will now move on the the creation of the html files, which are the display of the website."
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-base.html-file",
    "href": "posts/Homework3/HW3.html#the-base.html-file",
    "title": "Flask Website With User Inputs",
    "section": "The base.html File:",
    "text": "The base.html File:\n\nThe first html file that we must write is called base.html, which will contain the header for each page of the website.\nThe html code for this file is shown here. A breakdown of this code is given below the code.\n\n\n#1:  &lt;!doctype html&gt;\n#2:  &lt;html&gt;\n#3:  &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n\n#4:  &lt;h2&gt;A Simple Message Bank&lt;/h2&gt;\n\n#5:  &lt;nav&gt;\n#6:     &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n#7:        &lt;ul&gt;\n#8:           &lt;li&gt;&lt;a href=\"{{ url_for('render_submit_template') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n#9:           &lt;li&gt;&lt;a href=\"{{ url_for('view_random_messages')}}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n#10:       &lt;/ul&gt;\n\n#11: &lt;section class=\"content\"&gt;\n#12:    &lt;header&gt;\n#13.       {% block header %}{% endblock %}\n#14.    &lt;/header&gt;\n#15.       {% block content %}{% endblock %}\n#16. &lt;/section&gt;\n#17. &lt;/html&gt;\n\n\nLine 3: This line establishes the style of the website. The guidelines for the style are contained in the style.css file which we will go over later.\nLine 4: The header starts with a title: Simple Message Bank.\nLines 5-10: These lines create a block that will show the urls to move to another page on the website.\nLine 8: This line creates a link called “Submit a Message” that calls the render_submit_template() function created in the app.py file.\nLine 9: This line creates a link called “View Messages” that calls the view_random_messages() function created in the app.py file.\nLines 11-16: Create a block that will contain the information from the other html files.\nLine 13: This will contain the “header” block of the other html files.\nLine 15: This will cintain the “content” block of the other html files."
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-submit.html-file",
    "href": "posts/Homework3/HW3.html#the-submit.html-file",
    "title": "Flask Website With User Inputs",
    "section": "The submit.html File:",
    "text": "The submit.html File:\n\nWe will now create the submit.html file, which creates a webpage where the user can submit their handle and message.\nThe html code for this file is shown here. A breakdown of this code is given below the code.\n\n\n#1.  &lt;!doctype html&gt;\n#2.  &lt;html&gt;\n#3.  {% extends 'base.html' %}\n#4.     &lt;body&gt;\n         \n#5.        {% block header %}\n#6.           &lt;h2&gt;Submit&lt;/h2&gt;\n#7.        {% endblock %}\n#8.        {% block content %}\n#9.           &lt;form action = \"{{ url_for('render_submit_template') }}\" method = \"POST\"&gt;\n\n#10.             Your message:&lt;br&gt;\n#11.             &lt;input type = \"text\" name = \"message\" id = \"message\" /&gt;&lt;/br&gt;\n\n#12.             Your name or handle:&lt;br&gt;\n#13.             &lt;input type = \"text\" name = \"handle\" id = \"handle\" /&gt;&lt;/br&gt;\n\n#14.             &lt;input type = \"submit\" value = \"Submit message\" /&gt;&lt;br&gt;\n#15.          &lt;/form&gt;\n         \n#16.          &lt;h3&gt;{{ msg }}&lt;/h3&gt;\n#17.       {% endblock %}\n\n#18.    &lt;/body&gt;\n#19. &lt;/html&gt;\n\n\nLine 3: Makes the header of this webpage the header that is outlined in the base.html file above.\nLines 5-7: Fill in the “block header” portion of the base.html file.\nLine 6: Make “Submit” the title of this webpage block.\nLines 8-17: Fill in the “block content” portion of the base.html file.\nLine 9: Specify that the function of this webpage is to run the render_submit_template() function in the app.py file with the POST method.\nLines 10-11: Create a text box labeled “Your massage:” that allows a user to submit a message, binding their input to the object names “message”.\nLines 12-13: Create a text box labeled “Your name or handle:” that allows a user to submit a their name or handle, binding their input to the object names “handle”.\nLine 14: Create a button labeles “Submit message” that a user presses to submit thier entry, running the render_submit_template() function, which adds the entry to the SQL database.\nLine 16: Option for an additional message to be outputted once the user submits their message."
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-view.html-file",
    "href": "posts/Homework3/HW3.html#the-view.html-file",
    "title": "Flask Website With User Inputs",
    "section": "The view.html File:",
    "text": "The view.html File:\n\nWe will now create the view.html file, which creates a webpage where the user can view 4 random previous entries to the website. This is the last html file that is needed for our website.\nThe html code for this file is shown here. A breakdown of this code is given below the code.\n\n\n#1.  &lt;!doctype html&gt;\n#2.  &lt;html&gt;\n#3.  {% extends 'base.html' %}\n#4.     &lt;body&gt;\n\n#5.        {% block header %}\n#6.           &lt;h2&gt;Some Cool Messages&lt;/h2&gt;\n#7.        {% endblock %}\n         \n#8.        {% block content %}\n#9.           {% if messages %}\n#10.             &lt;p&gt;\n#11.                {% for message in messages %}\n#12.                   &lt;p&gt;\n#13.                      &lt;br&gt;&lt;strong&gt;\"{{message[1]}}\"&lt;/strong&gt;&lt;/br&gt;\n#14.                      &lt;i&gt;-{{message[0]}}&lt;/i&gt;\n#15.                   &lt;/p&gt;\n#16.                {% endfor %}\n#17.             &lt;/p&gt;\n#18.          {% else %}\n#19.             &lt;p&gt;No messages available.&lt;/p&gt;\n#20.          {% endif %}      \n#21.       {% endblock %}\n\n#22.       &lt;/form&gt;\n\n#23.    &lt;/body&gt;\n#24. &lt;/html&gt;\n\n\nLine 3: Makes the header of this webpage the header that is outlined in the base.html file above.\nLines 5-7: Fill in the “block header” portion of the base.html file.\nLine 6: Make “Some Cool Messages” the title of this webpage block.\nLines 8-21: Fill in the “block content” portion of the base.html file.\nLine 9: Create an if statement that only runs if a messages object is inputted.\nLine 11: Create a for loop that loops through the inputs of messages.\nLine 13: Put the message in quotes and make it bold.\nLine 14: Put the handle of the user under their quote in italics with a dash in front.\nLine 18: If there is no messages object inputted, display “No messages avaliable.”"
  },
  {
    "objectID": "posts/Homework3/HW3.html#where-to-place-the-created-files",
    "href": "posts/Homework3/HW3.html#where-to-place-the-created-files",
    "title": "Flask Website With User Inputs",
    "section": "Where to Place the Created Files:",
    "text": "Where to Place the Created Files:\n\nFirst, create a folder for the webpage on your device called hw3_flask, and put the app.py file in that folder.\nInside the hw3_flask folder, create another folder called static and put the style.css file in that folder.\nAlso inside the hw3_flask folder, create another folder called templates and put the base.html, submit.html, and view.html files in that folder."
  },
  {
    "objectID": "posts/Homework3/HW3.html#how-to-run-the-app",
    "href": "posts/Homework3/HW3.html#how-to-run-the-app",
    "title": "Flask Website With User Inputs",
    "section": "How to Run the App:",
    "text": "How to Run the App:\n\nGo to the terminal and set the working directory to the hw3_flask folder. I did this by running:\n\ncd /Users/trentbellinger/Desktop/PIC 16B/Homework/Homework 3/hw3_flask\n\nThen, run the following command terminal:\n\npython app.py\n\nThis will give you a line in the terminal output that says running on http://192.168.0.150:4999 (with different numbers).\nCopy and paste this link into your browser and interact with the website."
  },
  {
    "objectID": "posts/Homework4/HW4.html",
    "href": "posts/Homework4/HW4.html",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "",
    "text": "First, we import all packages necessary to complete the diffusion models.\n\n\n# create arrays to model the diffusion process\nimport numpy as np\n\n# visualize the arrays that are created\nfrom matplotlib import pyplot as plt\n\n# optimize the code, significantly decreasing run time\nimport jax\nfrom jax.experimental import sparse\nfrom jax import jit\nimport jax.numpy as jnp\n\n# display the code for functions created in a seperate .py file\nimport inspect\n\n\nWe will now set some global parameters for the diffusion model.\nN is the width and height of the grid that the model will be displayed on.\nepsilon is a small parameter that represents a time step in the discretized process.\n\n\nN = 101\nepsilon = 0.2\n\n\nThe diffusion model will begin with an initial numpy array containing heat centralized in the array.\nThe creation of this array is given in the function below along with a depiction of the initial state of the diffusion model.\n\n\ndef create_u0(N):\n    '''\n    Creates the initial array in our two-dimensional heat diffusion model.\n    Args:\n        N: an integer of how large the array should be\n    Returns:\n        u0: a N x N numpy array to represent the start of a 2D heat diffusion\n    '''\n    u0 = np.zeros((N, N))\n    u0[int(N/2), int(N/2)] = 1.0\n    return u0\n\nplt.imshow(create_u0(N))\n\n\n\n\n\n\n\n\n\nAfter creating the initial array, we want to model the diffusion through many (2700) time steps.\nWe also want to save every 300 steps of the process to visualize the differences as the heat diffusion progresses.\nThe function that will allow us to advance the simulation by 2700 steps is shown below.\n\n\ndef advance_2700(advance_fun, u0, **kwargs):\n    '''\n    Advances the heat diffusion process by 2700 steps, saving every 300 steps.\n    Args:\n        advance_fun: a function that will advance the diffusion process by 1 step\n        u0: a N x N numpy array that represents that start of the diffusion\n        **kwargs: additional arguments needed for the advance_fun function\n    Returns:\n        a list of the numpy array generated at every 300th step of the diffusion\n    '''\n    output = []\n    # for numbers 1 to 2700\n    for i in range(1, 2701):\n        \n        # advance the simulation by one step\n        u0 = advance_fun(u = u0, **kwargs)\n        \n        # if we are in one of the 300th steps\n        if i % 300 == 0:\n            # save the output in a list and print a message\n            output.append(u0)\n            print(f\"u{i} created\")\n    return output\n\n\nNow that we have a way to model 2700 diffusion steps, we want a way to visualize every 300th step of the process.\nThe function that accomplishes this is shown below.\n\n\ndef plot_every_300(u_list):\n    '''\n    Outputs a 3 by 3 visualization that shows every 300th step of a modeled \n    diffusion process with 2700 steps.\n    Args:\n        u_list: a list created by the advance_2700 function that contains every \n                300th step of the modeled diffusion process\n    Returns:\n        3 by 3 visualization that shows every 300th step of the modeled diffusion\n    '''\n    # create the figure\n    fig = plt.figure()\n    # for numbers 1 to 10\n    for i in range(1, 10):\n        \n        # add a subplot and display the (i*300)th step\n        fig.add_subplot(3, 3, i)\n        plt.imshow(u_list[i-1])\n        plt.axis(\"off\")\n        plt.title(f\"{i*300}th Iteration\")\n\n\nNow that we have a way to advance our simulation and plot our results, we will experiment with different advance_fun functions using different processes."
  },
  {
    "objectID": "posts/Homework4/HW4.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-4.-i-will-describe-my-modeling-of-a-two-dimensional-heat-diffusion-using-four-different-methods.",
    "href": "posts/Homework4/HW4.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-4.-i-will-describe-my-modeling-of-a-two-dimensional-heat-diffusion-using-four-different-methods.",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "",
    "text": "First, we import all packages necessary to complete the diffusion models.\n\n\n# create arrays to model the diffusion process\nimport numpy as np\n\n# visualize the arrays that are created\nfrom matplotlib import pyplot as plt\n\n# optimize the code, significantly decreasing run time\nimport jax\nfrom jax.experimental import sparse\nfrom jax import jit\nimport jax.numpy as jnp\n\n# display the code for functions created in a seperate .py file\nimport inspect\n\n\nWe will now set some global parameters for the diffusion model.\nN is the width and height of the grid that the model will be displayed on.\nepsilon is a small parameter that represents a time step in the discretized process.\n\n\nN = 101\nepsilon = 0.2\n\n\nThe diffusion model will begin with an initial numpy array containing heat centralized in the array.\nThe creation of this array is given in the function below along with a depiction of the initial state of the diffusion model.\n\n\ndef create_u0(N):\n    '''\n    Creates the initial array in our two-dimensional heat diffusion model.\n    Args:\n        N: an integer of how large the array should be\n    Returns:\n        u0: a N x N numpy array to represent the start of a 2D heat diffusion\n    '''\n    u0 = np.zeros((N, N))\n    u0[int(N/2), int(N/2)] = 1.0\n    return u0\n\nplt.imshow(create_u0(N))\n\n\n\n\n\n\n\n\n\nAfter creating the initial array, we want to model the diffusion through many (2700) time steps.\nWe also want to save every 300 steps of the process to visualize the differences as the heat diffusion progresses.\nThe function that will allow us to advance the simulation by 2700 steps is shown below.\n\n\ndef advance_2700(advance_fun, u0, **kwargs):\n    '''\n    Advances the heat diffusion process by 2700 steps, saving every 300 steps.\n    Args:\n        advance_fun: a function that will advance the diffusion process by 1 step\n        u0: a N x N numpy array that represents that start of the diffusion\n        **kwargs: additional arguments needed for the advance_fun function\n    Returns:\n        a list of the numpy array generated at every 300th step of the diffusion\n    '''\n    output = []\n    # for numbers 1 to 2700\n    for i in range(1, 2701):\n        \n        # advance the simulation by one step\n        u0 = advance_fun(u = u0, **kwargs)\n        \n        # if we are in one of the 300th steps\n        if i % 300 == 0:\n            # save the output in a list and print a message\n            output.append(u0)\n            print(f\"u{i} created\")\n    return output\n\n\nNow that we have a way to model 2700 diffusion steps, we want a way to visualize every 300th step of the process.\nThe function that accomplishes this is shown below.\n\n\ndef plot_every_300(u_list):\n    '''\n    Outputs a 3 by 3 visualization that shows every 300th step of a modeled \n    diffusion process with 2700 steps.\n    Args:\n        u_list: a list created by the advance_2700 function that contains every \n                300th step of the modeled diffusion process\n    Returns:\n        3 by 3 visualization that shows every 300th step of the modeled diffusion\n    '''\n    # create the figure\n    fig = plt.figure()\n    # for numbers 1 to 10\n    for i in range(1, 10):\n        \n        # add a subplot and display the (i*300)th step\n        fig.add_subplot(3, 3, i)\n        plt.imshow(u_list[i-1])\n        plt.axis(\"off\")\n        plt.title(f\"{i*300}th Iteration\")\n\n\nNow that we have a way to advance our simulation and plot our results, we will experiment with different advance_fun functions using different processes."
  },
  {
    "objectID": "posts/Homework4/HW4.html#speed",
    "href": "posts/Homework4/HW4.html#speed",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "Speed:",
    "text": "Speed:\n\nPart 1: 30.5 seconds\nPart 2: 0.984 seconds\nPart 3: 0.231 seconds\nPart 4: 0.082 seconds\nPart 2 is about 31 times faster than part 1.\nPart 3 is about 132 times faster than part 1 and about 4.3 times faster than part 2.\nPart 4 is about 372 times faster than part 1, about 12 times faster than part 3, and about 2.8 times faster than part 3.\nIn conclusion, part 4 is significantly preferred from a speed standpoint."
  },
  {
    "objectID": "posts/Homework4/HW4.html#ease-of-coding",
    "href": "posts/Homework4/HW4.html#ease-of-coding",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "Ease of Coding:",
    "text": "Ease of Coding:\n\nPart 1: This part is the hardest to code because it involves two functions and matrix multiplication.\nPart 2: This part was equally hard as part 1 to code because it also involves 2 functions and matrix multiplication.\nPart 3: This part was much easier than parts 1 and 2 to code because it was just following along with the heat diffusion equation.\nPart 4: This part was equally as difficult as part 3 to code because the logic was the exact same.\nIn conclusion, the functions that ran to fastest were actually much easier in terms of coding."
  },
  {
    "objectID": "posts/Homework4/HW4.html#part-4-is-preferred-due-to-its-speed-and-ease-of-coding.",
    "href": "posts/Homework4/HW4.html#part-4-is-preferred-due-to-its-speed-and-ease-of-coding.",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "Part 4 is preferred due to its speed and ease of coding.",
    "text": "Part 4 is preferred due to its speed and ease of coding."
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2a-class-initialization",
    "href": "posts/Homework2/HW2.html#step-2a-class-initialization",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(a): Class Initialization",
    "text": "Step 2(a): Class Initialization\n\nThe first code that we write in this file is the class initialization, which is shown below.\n\n\nimport scrapy\nclass TmdbSpider(scrapy.Spider):\n    '''\n    This class is for a spider that scrapes a movie website from TMDB and finds other \n    movies and TV shows that have similar actors to the selected movie.\n    '''\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n\nAs we can see, the class is initialized with global variable name=‘tmdb_spider’. The variables subdir, *args, and **kwargs are passed into the initialization of the class. The subdir instance variable is the subdirectory of the movie that we want to scrape data from on the TMDB website. We then initialize the instance variable start_urls which contains the url of the movie that we want to scrape data from."
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2b-the-parse-method",
    "href": "posts/Homework2/HW2.html#step-2b-the-parse-method",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(b): The parse() Method",
    "text": "Step 2(b): The parse() Method\n\nWe now move on to creating the parsing methods for the scraper.\nThe first of these methods is the parse method. This method assumes that we are on the selected movie’s TMDB page and navigates the spider to the Full Cast & Crew page. Once on this page, we call the parse_full_credits method, which will be defined later.\nThe code for the parse method is below.\n\n\ndef parse(self, response):\n    '''\n    Yields the cast page of the selected movie and calls the parse_full_credits \n    method.\n    '''  \n    # go the the cast page of the selected movie and call the parse_full_credits method\n    cast_url = f\"{self.start_urls[0]}cast\"\n    yield scrapy.Request(cast_url, callback = self.parse_full_credits)"
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2c-the-parse_full_credits-method",
    "href": "posts/Homework2/HW2.html#step-2c-the-parse_full_credits-method",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(c): The parse_full_credits() Method",
    "text": "Step 2(c): The parse_full_credits() Method\n\nThe parse_full_credits method assumes that we are on the selected movie’s Full Cast & Crew page on TMDB. It yields a scrapy request for each actor listed on the page (crew members are excluded). When each actor’s request is yielded, we call the parse_actor_page method, which will be defined later.\nTo accomplish this, we must first find a list of all the actor’s URLs on the page and then loop through this list and yield the URL to each actor’s page on TMDB.\nThe code for this method is below.\n\n\ndef parse_full_credits(self, response):\n    '''\n    Starts at the cast page for the selected movie and yields requests for the \n    TMDB page of each actor on the cast page (excluding crew members), calling \n    the parse_actor_page method.\n    '''\n    # get all the links for the actors on the cast page of the movie\n    actors_css = 'ol.people.credits:not(.crew) li div.info a::attr(href)'\n    actors = response.css(actors_css).extract()\n    \n    # loop through the actor links on the page, excluding crew members\n    for actor_link in actors:\n        \n        # join the actor url with the response url\n        url = response.urljoin(actor_link) \n        # create a request using the above url and call the parse_actor_page method\n        yield scrapy.Request(url, callback = self.parse_actor_page)"
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2d-the-parse_actor_page-method",
    "href": "posts/Homework2/HW2.html#step-2d-the-parse_actor_page-method",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(d): The parse_actor_page() Method",
    "text": "Step 2(d): The parse_actor_page() Method\n\nThe parse_actor_page method assumes that we are on the TMDB page for an actor. The method yields a dictionary of the form {“actor” : actor_name, “movie_or_TV_name” : movie_or_TV_name} for each acting role that the actor has had in their career (non-acting roles are excluded).\nTo accomplish this, we first find the actor’s name on the actor’s TMDB page. We then need to loop through the different credits tables on the page and find the one that contains only the acting roles. We then create a selector for this table, loop through the entries of the table, and yield a dictionary of the actor’s name and the role for each role in the table.\nThe code for this method is below.\n\n\ndef parse_actor_page(self, response):\n    '''\n    Starts on the TMDB page of an actor and yields a dictionary containing the \n    actor's name and the movie or TV show for each of their acting roles.\n    '''\n    # get the actor's name from their page\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # loop through the types of credit lists on the page\n    for item in response.css('div.credits_list h3'):\n        # if the credit list is for acting\n        if 'Acting' in item.xpath('./text()').get():\n            # get the table of acting roles\n            acting_table = item.xpath('following-sibling::table[1]').get()\n                \n    # create a selector for the table of acting roles\n    acting_selector = scrapy.Selector(text = acting_table)\n        \n    # loop through the table of acting roles\n    for acting in acting_selector.css('table.credit_group tr'):\n        # get the movie or TV show name from the current acting role\n        movie_or_TV_name = acting.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\n\nWe now have a fully functional web spraper that yields various dictionaries for each acting role of each actor in the selected movie.\n\nIMPORTANT NOTE: the methods parse, parse_full_credits, and parse_actor_page are defined within the TmdbSpider class initialized in part 2(a)"
  },
  {
    "objectID": "posts/Homework2/HW2.html#test-1-harry-potter-and-the-philosophers-stone",
    "href": "posts/Homework2/HW2.html#test-1-harry-potter-and-the-philosophers-stone",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Test 1: Harry Potter and the Philosopher’s Stone",
    "text": "Test 1: Harry Potter and the Philosopher’s Stone\n\nThe goal of this test is to create a .csv file that contains all of the {actor:role} pairs that are yielded by our TmdbSpider class for the movie Harry Potter and the Philosopher’s Stone.\nThis was accomplished by running the follwing line in the terminal, while still in the same TMDB_scraper directory as in step 1.\n\nscrapy crawl tmdb_spider -o results1.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nAfter this line is run, a results1.csv file will be created in the TMDB_scraper folder.\nWe need to make sure this file is of the correct format, which is done below.\n\n\nimport pandas as pd\n\nresults1 = pd.read_csv(\"results1.csv\")\nresults1\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2952\nJames Phelps\nKingdom\n\n\n2953\nJames Phelps\nHarry Potter and the Goblet of Fire\n\n\n2954\nJames Phelps\nHarry Potter and the Prisoner of Azkaban\n\n\n2955\nJames Phelps\nHarry Potter and the Chamber of Secrets\n\n\n2956\nJames Phelps\nHarry Potter and the Philosopher's Stone\n\n\n\n\n2957 rows × 2 columns\n\n\n\n\nWe can se that the results1.csv file is in the correct format, with one column for actor name and one column for the movie or TV show they acted in. There are 2957 observations.\nWe will now check is our dataset contains only the actors in the movie (i.e. it excludes the crew members).\n\n\nresults1['actor'].nunique()\n\n63\n\n\n\nHere, we see that there are 63 total actors in the dataset. When viewing the TMDB webpage for the movie, there are exactly 63 actors in the movie, so our scraper successfully weeded out the crew members and only included the actors.\nWe will now check that each actor is only in the dataset for their acting roles (not their roles in production, directing, etc.). We use Daniel Radcliffe as our test.\n\n\nresults1.groupby('actor').size()['Daniel Radcliffe']\n\n97\n\n\n\nWe can see here that the actor Daniel Radcliffe is found 97 times in the dataset, meaning that he should have 97 acting roles in his career. When viewing the TMD webpage for Daniel Radcliffe, he does in fact have exactly 97 acting roles, so our scraper has successfully weeded out non-acting roles for each actor.\nOur scraper has worked as expected, so our test is successul."
  },
  {
    "objectID": "posts/Homework2/HW2.html#test-2-good-will-hunting",
    "href": "posts/Homework2/HW2.html#test-2-good-will-hunting",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Test 2: Good Will Hunting",
    "text": "Test 2: Good Will Hunting\n\nWe will now test the scraper on my favorite movie, Good Will Hunting.\nThe goal of this test is to create a .csv file that contains all of the {actor:role} pairs that are yielded by our TmdbSpider class for the movie Good Will Hunting.\nThis was accomplished by running the follwing line in the terminal, while still in the same TMDB_scraper directory as in step 1.\n\nscrapy crawl tmdb_spider -o results2.csv -a subdir=489-good-will-hunting\n\nAfter this line is run, a results2.csv file is created in the TMDB_scraper folder.\nWe just need to make sure this file is of the correct format, which is done below.\n\n\nresults2 = pd.read_csv(\"results2.csv\")\nresults2\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nChristian Harmony\nThe Doorway\n\n\n1\nChristian Harmony\nSex and the City\n\n\n2\nChristian Harmony\nGood Will Hunting\n\n\n3\nStephen L'Heureux\nGood Will Hunting\n\n\n4\nStephen L'Heureux\nScent of a Woman\n\n\n...\n...\n...\n\n\n1500\nCasey Affleck\nLIVE with Kelly and Mark\n\n\n1501\nCasey Affleck\nLemon Sky\n\n\n1502\nCasey Affleck\nSaturday Night Live\n\n\n1503\nMatt Mercier\nGood Will Hunting\n\n\n1504\nRachel Majorowski\nGood Will Hunting\n\n\n\n\n1505 rows × 2 columns\n\n\n\n\nWe can se that the results2.csv file is in the correct format, with one column for actor name and one column for the movie or TV show they acted in. There are 1505 observations.\nWe will now check is our dataset contains only the actors in the movie (i.e it excludes the crew members).\n\n\nresults2['actor'].nunique()\n\n49\n\n\n\nHere, we see that there are 49 total actors in the dataset. When viewing the TMDB webpage for the movie, there are exactly 49 actors in the movie, so our scraper successfully weeded out the crew members and only included the actors.\nWe will now check that each actor is only in the dataset for their acting roles, and not their roles in production, directing, etc. We use Matt Damon as our test.\n\n\nresults2.groupby('actor').size()['Matt Damon']\n\n163\n\n\n\nWe can see here that the actor Matt Damon is found 163 times in the dataset, meaning that he should have 163 acting roles in his career. When viewing the TMD webpage for Matt Damon, he does in fact have exactly 163 acting roles, so our scraper has successfully weeded out non-acting roles for each actor.\nOur scraper has worked as expected, so our test is successul."
  },
  {
    "objectID": "posts/Homework2/HW2.html#recommendations-for-harry-potter-and-the-philosophers-stone",
    "href": "posts/Homework2/HW2.html#recommendations-for-harry-potter-and-the-philosophers-stone",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Recommendations for Harry Potter and the Philosopher’s Stone",
    "text": "Recommendations for Harry Potter and the Philosopher’s Stone\n\nUsing the results1.csv dataframe imported above, we create a dataframe with two columns “movie or TV name” and “number of shared actors” using the recommend_dataframe() function.\n\n\nrecommendations1 = recommend_dataframe(results1)\nrecommendations1\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n0\n(K)nox: The Rob Knox Story\n4\n\n\n1\n10 Days to War\n1\n\n\n2\n10 Rillington Place\n1\n\n\n3\n100 Years of Warner Bros.\n1\n\n\n4\n13Hrs\n1\n\n\n...\n...\n...\n\n\n2270\nYour Christmas or Mine 2\n1\n\n\n2271\nYour Christmas or Mine?\n1\n\n\n2272\nYour Ticket Is No Longer Valid\n1\n\n\n2273\nYub-Nub! The Forgotten Ewok Adventures\n1\n\n\n2274\nZastrozzi: A Romance\n1\n\n\n\n\n2275 rows × 2 columns\n\n\n\n\nWe can see that our function works in creating the dataframe with the two desired columns.\nWe will now sort by movies or tv shows that have more than 5 shared actors with Harry Potter and the Philosopher’s Stone.\n\n\nrecommendations1.loc[recommendations1[\"number of shared actors\"] &gt; 5,:]\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n116\nAn Audience with...\n6\n\n\n382\nCreating the World of Harry Potter\n36\n\n\n415\nDavid Holmes: The Boy Who Lived\n6\n\n\n455\nDoctor Who\n14\n\n\n693\nHarry Potter 20th Anniversary: Return to Hogwarts\n11\n\n\n694\nHarry Potter and the Chamber of Secrets\n37\n\n\n695\nHarry Potter and the Deathly Hallows: Part 1\n20\n\n\n696\nHarry Potter and the Deathly Hallows: Part 2\n23\n\n\n697\nHarry Potter and the Goblet of Fire\n19\n\n\n698\nHarry Potter and the Half-Blood Prince\n19\n\n\n699\nHarry Potter and the Order of the Phoenix\n24\n\n\n700\nHarry Potter and the Philosopher's Stone\n63\n\n\n701\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n1241\nPerformance\n7\n\n\n1839\nThe Magic Touch of Harry Potter\n10\n\n\n2050\nThe Wonderful World of Disney: Magical Holiday...\n11\n\n\n2101\nTony Awards\n6\n\n\n\n\n\n\n\n\nWe can see that there are 16 movies and TV shows that have more than 5 shared actors with Harry Potter and the Philosopher’s Stone.\nWe would reccomend these movies to people who enjoy Harry Potter and the Philosopher’s Stone because the actors are very similar (more than 5 matches).\nWe will now use the recommend_barplot() function to visualize the data presented above.\n\n\nrecommend_barplot(df = recommendations1, \n                  movie = \"Harry Potter and the Philosopher's Stone\", \n                  min_num_shared_actors = 5)\n\n\n\n\n\n\n\n\n\nFrom the bar plot above, we can see that the movies or TV shows that share the most actors with Harry Potter and the Philosopher’s Stone are “Harry Potter and the Chamber of Secrets” and “Creating the World of Harry Potter.” This makes sense because all of the Harry Potter movies share many actors."
  },
  {
    "objectID": "posts/Homework2/HW2.html#recommendations-for-good-will-hunting",
    "href": "posts/Homework2/HW2.html#recommendations-for-good-will-hunting",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Recommendations for Good Will Hunting",
    "text": "Recommendations for Good Will Hunting\n\nUsing the results2.csv dataframe imported above, I will create a dataframe with two columns “movie or TV name” and “number of shared actors” using the recommend_dataframe() function.\n\n\nrecommendations2 = recommend_dataframe(results2)\nrecommendations2\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n0\n'Saving Private Ryan': Boot Camp\n1\n\n\n1\n...First Do No Harm\n1\n\n\n2\n10-8: Officers on Duty\n1\n\n\n3\n11 Colours of the Bird\n1\n\n\n4\n12 Dates of Christmas\n1\n\n\n...\n...\n...\n\n\n1275\nZerophilia\n1\n\n\n1276\neXistenZ\n1\n\n\n1277\nmid90s\n1\n\n\n1278\nÅke and His World\n1\n\n\n1279\n알아두면 쓸데없는 지구별 잡학사전\n1\n\n\n\n\n1280 rows × 2 columns\n\n\n\n\nWe can again see that our function works in creating the dataframe with the two desired columns.\nWe will now sort by movies or tv shows that have more than 3 shared actors with Good Will Hunting.\n\n\nrecommendations2.loc[recommendations2[\"number of shared actors\"] &gt; 3,:]\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n285\nDue South\n6\n\n\n316\nFX: The Series\n4\n\n\n394\nGood Will Hunting\n49\n\n\n512\nJimmy Kimmel Live!\n4\n\n\n541\nLIVE with Kelly and Mark\n4\n\n\n606\nMayday\n5\n\n\n803\nSaturday Night Live\n5\n\n\n812\nSchool Ties\n4\n\n\n885\nSue Thomas: F.B.Eye\n4\n\n\n888\nSuits\n4\n\n\n1003\nThe Graham Norton Show\n4\n\n\n1071\nThe Oscars\n4\n\n\n1131\nThe Tonight Show with Jay Leno\n4\n\n\n1140\nThe View\n4\n\n\n1228\nWar of the Worlds\n4\n\n\n\n\n\n\n\n\nWe can see that there are 14 movies and TV shows that have more than 3 shared actors with Good Will Hunting.\nI would reccomend these movies to people who enjoy Good Will Hunting because the actors are very similar (more than 3 matches).\nWe will now use the recommend_barplot() function to visualize the data presented above.\n\n\nrecommend_barplot(df = recommendations2, \n                  movie = \"Good Will Hunting\", \n                  min_num_shared_actors = 3)\n\n\n\n\n\n\n\n\n\nFrom the bar plot above, we can see that the movies or TV shows that share the most actors with Good Will Hunting are “Due South”, “Mayday”, and “Saturday Night Live”. We would recommend these shows the most to Good Will Hunting fans."
  },
  {
    "objectID": "posts/Homework1/HW1.html",
    "href": "posts/Homework1/HW1.html",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "",
    "text": "Before beginning the coding for the assignment, I loaded in all the necessary packages. The packages are loaded in below, and have a short description of what they were used for in the assignment.\n\n\n# used to read in data as dataframes and in each function\nimport pandas as pd\n\n# used in part 1 to create the database and in the query functions\nimport sqlite3\n\n# used in the temperature_coefficient_plot function\nimport numpy as np\n\n# used for visualizations\nfrom plotly import express as px\n\n# used to obtain coefficients in the temperature_coefficient_plot function\nfrom sklearn.linear_model import LinearRegression\n\n# used to properly show figures in the quarto blog\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/Homework1/HW1.html#stations-dataset",
    "href": "posts/Homework1/HW1.html#stations-dataset",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Stations Dataset:",
    "text": "Stations Dataset:\n\nThis is a dataset that contains the ID, latitude, longitude, elevation, and name of each station that we will be looking at.\nWe are particularly interested in the longitude and latitude columns of this datast, which can be used to create visualizations where the locations of the stations are accurately represented on a world map.\nThe data is read in below along with a preview of what it looks like.\n\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR"
  },
  {
    "objectID": "posts/Homework1/HW1.html#temperatures-dataset",
    "href": "posts/Homework1/HW1.html#temperatures-dataset",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Temperatures Dataset:",
    "text": "Temperatures Dataset:\n\nThis is a dataset containing the ID, year, and temperature at the 12 months during the year for each station.\nWe are especially interested in looking at the temperature fluctuations at each station, so this dataset will be very useful for that.\nThe dataset is read in below along with a preview of what it looks like.\n\n\ntemps = pd.read_csv(\"temps.csv\")\ntemps.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\nLooking at the temperatures dataset above, it is clear that it will not be very easy to work with if we want to look at temperatures by month and year because years and months are all in different columns.\nWe will solve this issue by creating function to prepare the temps dataset into a more suitable form for our database.\nThis function, created in lecture, is defined below and is called on the temps dataset. A preview of the new dataset that we will use for database creation is also shown.\n\n\ndef prepare_df(df):\n    '''\n    Returns a temperature dataframe in a more suitable format for analysis.\n    \n    Arguments:\n        df (Pandas dataframe): the temps dataframe shown above\n    \n    Returns:\n        df (Pandas dataframe): a temps dataframe with columns ID, Year, Month, and Temp\n    '''\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\ntemps = prepare_df(temps)\ntemps.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28"
  },
  {
    "objectID": "posts/Homework1/HW1.html#countries-dataset",
    "href": "posts/Homework1/HW1.html#countries-dataset",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Countries Dataset:",
    "text": "Countries Dataset:\n\nThis is a dataset containing the names of every country along with their 2-letter abbreviations.\nThis will be used to determine which stations are in which country using the ID columns in the stations and temperatures datasets.\nThe data is read in below along with a preview of what the data looks like.\n\n\ncountries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa"
  },
  {
    "objectID": "posts/Homework1/HW1.html#we-now-create-the-database-with-one-table-for-each-of-the-three-datasets-that-we-have.",
    "href": "posts/Homework1/HW1.html#we-now-create-the-database-with-one-table-for-each-of-the-three-datasets-that-we-have.",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "We now create the database with one table for each of the three datasets that we have.",
    "text": "We now create the database with one table for each of the three datasets that we have.\n\nWe start by establishing a connection to a file “hw1.db” which is the main file for the database.\nWe then add each dataset that we read in above into the database using the to_sql method.\nThe connection to the database is then closed.\n\n\nconn = sqlite3.connect(\"hw1.db\")\n\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\ntemps.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n    \nconn.close()"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-1",
    "href": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-1",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing temperature_coefficient_plot() Function (Part 1):",
    "text": "Testing temperature_coefficient_plot() Function (Part 1):\n\nWe will now test the function, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), country = “India”, year_begin = 1980, year_end = 2020, month = 1, min_obs = 10, zoom = 2, mapbox_style = “carto-positron”, color_continuous_scale = px.colors.diverging.RdGy_r.\nThis will give us a visualization of the changes in station temperature recordings from 1980 to 2020 in India with specifications of the plot being zoom, mapbox style, and color continuous scale.\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"hw1.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\nFrom the plot above, we can see that there are 8 to 12 points that are noticably dark, indicating a high change in temperature coefficient in years 1980-2020 in India. This is alarming because it could be indicative of climate issues in those areas."
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-2",
    "href": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-2",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing temperature_coefficient_plot() Function (Part 2):",
    "text": "Testing temperature_coefficient_plot() Function (Part 2):\n\nWe will now test the function again, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), country = “United States”, year_begin = 2010, year_end = 2020, month = 1, min_obs = 20, zoom = 2, mapbox_style = “carto-positron”, color_continuous_scale = px.colors.diverging.RdGy_r.\nThis will give us a visualization of the changes in station temperature recordings from 2010 to 2020 in the United States with specifications of the plot being zoom, mapbox style, and color continuous scale.\n\n\nfig2 = temperature_coefficient_plot(\"hw1.db\", \"United States\", 2010, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig2.show()\n\n\n\n\n\nFrom the plot above, we can see that there are at least 4 points that are extremely dark, indicating a high change in temperature coefficient in years 2010-2020 in the United States. Overall, this is not too bad becasue thare are many more dots that are not dark color, but it should raise alarm for those areas."
  },
  {
    "objectID": "posts/Homework1/HW1.html#question-1-how-does-a-countrys-latitude-affect-its-average-monthly-temperature-in-a-specific-month-and-year",
    "href": "posts/Homework1/HW1.html#question-1-how-does-a-countrys-latitude-affect-its-average-monthly-temperature-in-a-specific-month-and-year",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Question 1: How does a country’s latitude affect its average monthly temperature in a specific month and year?",
    "text": "Question 1: How does a country’s latitude affect its average monthly temperature in a specific month and year?"
  },
  {
    "objectID": "posts/Homework1/HW1.html#query_climate_database2-function",
    "href": "posts/Homework1/HW1.html#query_climate_database2-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "query_climate_database2() Function:",
    "text": "query_climate_database2() Function:\n\nWe create a function called query_climate_database2() with the following specifications:\nArguments:\n\n\ndb_file: a file of a sql database\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\nmin_latitude: the minimum latitude of stations for which should be returned\nmax_latitude: the maximum latitude of stations for which should be returned\n\n\nReturns: a dataframe corresponding to the above specifications, with columns:\n\n\nNAME: the station name\nLATITUDE: the latitude of the station\nCountry: the name of the country where the station is located\nYear: the year when the temperature reading was taken\nMonth: the month when the temperature reading was taken\nTemp: the average temperature at the specified station during the specified year and month\n\n\nThe code for the query_climate_database2() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import query_climate_database2\nprint(inspect.getsource(query_climate_database2))\n\ndef query_climate_database2(db_file, year, month, min_latitude, max_latitude):\n    '''\n    Returns a Pandas dataframe of temperature readings in the specified month of the specified year\n    between the minimum and maximum specified latitudes. This data comes from a SQL database.\n    \n    Arguments:\n        db_file (str): a file of a sql database\n        year (int): the year that you want to observe data from\n        month (str): an integer for the month of the year which the data should be returned\n        min_latitude (dbl): the minimum latitude of stations for which should be returned\n        max_latitude (dbl): the maximum latitude of stations for which should be returned\n        \n    Returns: a dataframe corresponding to the above specifications, with columns:\n        NAME (str): the station name\n        LATITUDE (dbl): the latitude of the station\n        Country (str): the name of the country where the station is located\n        Year (int): the year when the temperature reading was taken\n        Month (int): the month when the temperature reading was taken\n        Temp (dbl): the average temperature at the specified station during the specified year and month\n    '''\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT \n            stations.NAME, stations.LATITUDE, countries.Name \"Country\", temperatures.Year, temperatures.Month, temperatures.Temp\n        FROM \n            stations\n        INNER JOIN \n            countries ON SUBSTRING(stations.ID, 1, 2) = countries.[FIPS 10-4]\n        JOIN\n            temperatures ON stations.ID = temperatures.ID\n        WHERE\n            temperatures.Year = {year}\n            AND\n            temperatures.Month = {month}\n            AND\n            stations.LATITUDE BETWEEN {min_latitude} AND {max_latitude}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    \n    return df"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-query_climate_database2-function",
    "href": "posts/Homework1/HW1.html#testing-query_climate_database2-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing query_climate_database2() Function:",
    "text": "Testing query_climate_database2() Function:\n\nWe will now test the function, displaying the dataframe that we get using db_file = “hw1.db” (the database created above, year = 2003, month = 1, min_latitude = 0, max_latitude = 30.\nThis will give us a dataframe of all the database recordings in January of 2003 from stations with latitude between 0 and 30.\n\n\nquery_climate_database2(db_file = \"hw1.db\", \n                        year = 2003, \n                        month = 1, \n                        min_latitude = 0, \n                        max_latitude = 30)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nDUBAI_INTL\n25.2550\nUnited Arab Emirates\n2003\n1\n19.39\n\n\n1\nABU_DHABI_INTL\n24.4330\nUnited Arab Emirates\n2003\n1\n19.64\n\n\n2\nAL_AIN_INTL\n24.2620\nUnited Arab Emirates\n2003\n1\n18.45\n\n\n3\nIN_AMENAS\n28.0500\nAlgeria\n2003\n1\n11.95\n\n\n4\nTAMANRASSET\n22.8000\nAlgeria\n2003\n1\n14.20\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1115\nCHRISTIANSTED_AP\n17.7028\nVirgin Islands, U.S.\n2003\n1\n26.28\n\n\n1116\nCHARLOTTE_AMALIE_AP\n18.3331\nVirgin Islands, U.S.\n2003\n1\n26.36\n\n\n1117\nVILLA_CISNEROSMIL\n23.7000\nWestern Sahara\n2003\n1\n18.35\n\n\n1118\nDAKHLA\n23.7110\nWestern Sahara\n2003\n1\n17.53\n\n\n1119\nWAKE_ISLAND\n19.2833\nWake Island\n2003\n1\n26.16\n\n\n\n\n1120 rows × 6 columns"
  },
  {
    "objectID": "posts/Homework1/HW1.html#temperature_latitude_plot-function",
    "href": "posts/Homework1/HW1.html#temperature_latitude_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "temperature_latitude_plot() Function:",
    "text": "temperature_latitude_plot() Function:\n\nArguments:\n\n\ndb_file: a file of a sql database\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\nmin_latitude: the minimum latitude of stations for which should be returned\nmax_latitude: the maximum latitude of stations for which should be returned\n**kwargs: addition arguments to be used in plt.plot function\n\n\nReturns:\n\n\nAn interactive scatterplot with station latitude on the x-axis and station average temperature for the specified month and year on the y-axis. The points are colored by country.\n\n\nfrom climate_database import temperature_latitude_plot\nprint(inspect.getsource(temperature_latitude_plot))\n\ndef temperature_latitude_plot(db_file, year, month, min_latitude, max_latitude, **kwargs):\n    '''\n    Returns an interactive scatterplot of average temperature vs. station latitude for stations in a \n    specified latitude range and a specified month and year.\n    \n    Arguments:\n        db_file: a file of a sql database\n        year: the year that you want to observe data from\n        month: an integer for the month of the year which the data should be returned\n        min_latitude: the minimum latitude of stations for which should be returned\n        max_latitude: the maximum latitude of stations for which should be returned\n        **kwargs: addition arguments to be used in plt.plot function\n        \n    Returns: \n        An interactive scatterplot with station latitude on the x-axis and station \n        average temperature for the specified month and year on the y-axis. The points are \n        colored by country.\n    '''\n    df = query_climate_database2(db_file, year, month, min_latitude, max_latitude)\n    \n    return px.scatter(data_frame = df, x = \"LATITUDE\", y = \"Temp\", \n                      color = \"Country\",\n                      hover_name = \"NAME\", \n                      title = f\"Station Average Temperature by Latitude in Month {month} of {year}\",\n                      labels = {\"LATITUDE\":\"Latitude\", \"Temp\":\"Average Temperature\"},\n                      **kwargs)"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-temperature_latitude_plot-function",
    "href": "posts/Homework1/HW1.html#testing-temperature_latitude_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing temperature_latitude_plot() Function:",
    "text": "Testing temperature_latitude_plot() Function:\n\nWe will now test the function, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), year = 2003, month = 1, min_latitude = 0, max_latitude = 30, size = “Year”, size_max = 7.\nThis will give us a scatterplot of the station temperature recordings in stations with latitudes between 0 and 30 vs the latitude of the station. The points will be colored by country. The specifications of the scatterplot plot are marker size (I made the marker size year so they would all be the same size) and max marker size (7).\n\n\ntemperature_latitude_plot(db_file = \"hw1.db\", \n                          year = 2003, \n                          month = 1, \n                          min_latitude = 0, \n                          max_latitude = 30, \n                          size = \"Year\", \n                          size_max = 7)\n\n\n\n\n\nThis visualization makes it apparent that the average temperature in January of 2003 decreases as we move up in latitude. This makes sense intuitively because a greater latitude means a further distance from the equator, so we would expect temperatures to decrease as we move away from the equator.\nHowever, this relationship shows a lot of variation, which indicates that distance from the equator is not the only factor in the increasing temperatures."
  },
  {
    "objectID": "posts/Homework1/HW1.html#question-2-can-we-compare-the-average-temperature-recorded-at-stations-in-different-selected-countries-in-a-given-year",
    "href": "posts/Homework1/HW1.html#question-2-can-we-compare-the-average-temperature-recorded-at-stations-in-different-selected-countries-in-a-given-year",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Question 2: Can we compare the average temperature recorded at stations in different selected countries in a given year?",
    "text": "Question 2: Can we compare the average temperature recorded at stations in different selected countries in a given year?"
  },
  {
    "objectID": "posts/Homework1/HW1.html#query_climate_database3-function",
    "href": "posts/Homework1/HW1.html#query_climate_database3-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "query_climate_database3() Function:",
    "text": "query_climate_database3() Function:\n\nWe create a function called query_climate_database3() with the following specifications:\nArguments:\n\n\ndb_file: a file of a sql database\ncountries: a list of countries that will be compared\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\n\n\nReturns: a dataframe corresponding to the above specifications, with columns:\n\n\nNAME: the station name\nCountry: the name of the country where the station is located\nYear: the year when the temperature reading was taken\nMonth: the month when the temperature reading was taken\nTemp: the average temperature at the specified station during the specified year and month\n\n\nThe code for the query_climate_database3() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import query_climate_database3\nprint(inspect.getsource(query_climate_database3))\n\ndef query_climate_database3(db_file, countries, year):\n    '''\n    Returns a Pandas dataframe of temperature readings in the specified year\n    in specified countries. This data comes from a SQL database.\n    \n    Arguments:\n        db_file (str): a file of a sql database\n        countries (list): a list of strings representing country names\n        year (int): the year that you want to observe data from\n        \n    Returns: a dataframe corresponding to the above specifications, with columns:\n        NAME (str): the station name\n        Country (str): the name of the country where the station is located\n        Year (int): the year when the temperature reading was taken\n        Month (int): the month when the temperature reading was taken\n        Temp (dbl): the average temperature at the specified station during the specified year and month\n    '''\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT \n            stations.NAME, countries.Name \"Country\", temperatures.Year, temperatures.Month, temperatures.Temp\n        FROM \n            stations\n        INNER JOIN \n            countries ON SUBSTRING(stations.ID, 1, 2) = countries.[FIPS 10-4]\n        JOIN\n            temperatures ON stations.ID = temperatures.ID\n        WHERE \n            countries.Name IN {prepare_list_for_sql(countries)}\n            AND \n            temperatures.Year = {year}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    \n    return(df)"
  },
  {
    "objectID": "posts/Homework1/HW1.html#prepare_list_for_sql-helper-function",
    "href": "posts/Homework1/HW1.html#prepare_list_for_sql-helper-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "prepare_list_for_sql() Helper Function:",
    "text": "prepare_list_for_sql() Helper Function:\n\nLooking at the code for the query_climate_database3() function above, we had to apply a function to the countries list that is inputted to allow it to be used the the query. The specifics of that function are outlined here.\nArgument:\n\n\ncharacter_list: a list of country names\n\n\nReturns:\n\n\nA string of the list elements enclosed in parentheses and separated by commas, which is suitable for usage in a sql query.\n\n\nThe code for the prepare_list_for_sql() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import prepare_list_for_sql\nprint(inspect.getsource(prepare_list_for_sql))\n\ndef prepare_list_for_sql(character_list):\n    '''\n    Returns a list that is acceptable for subsetting in a SQL query.\n    \n    Arguments:\n        character_list (list): a list of strings representing country names\n        \n    Returns:\n        A list format that is acceptable for subsetting in a SQL query.\n    '''\n    return f\"\"\"('{\"', '\".join(character_list)}')\"\"\""
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-prepare_list_for_sql-helper-function",
    "href": "posts/Homework1/HW1.html#testing-prepare_list_for_sql-helper-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing prepare_list_for_sql() Helper Function:",
    "text": "Testing prepare_list_for_sql() Helper Function:\n\nWe will now test the function, displaying the output that the function gives for the list of countries [“Egypt”, “India”, “United States”].\n\n\nprepare_list_for_sql([\"Egypt\", \"India\", \"United States\"])\n\n\"('Egypt', 'India', 'United States')\"\n\n\n\nThis form is suitable for the IN function in a sql query."
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-query_climate_database3-function",
    "href": "posts/Homework1/HW1.html#testing-query_climate_database3-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing query_climate_database3() Function:",
    "text": "Testing query_climate_database3() Function:\n\nWe will now test the function, displaying the dataframe that we get using db_file = “hw1.db” (the database created above, year = 2003, and countries = [“Egypt”, “India”, “United States”].\nThis will give us a dataframe of all the database temeperature recordings in 2003 from Egypt, India, and the United States.\n\n\nquery_climate_database3(db_file = \"hw1.db\", \n                        year = 2003, \n                        countries = [\"Egypt\", \"India\", \"United States\"])\n\n\n\n\n\n\n\n\nNAME\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nMERSA_MATRUH\nEgypt\n2003\n1\n15.50\n\n\n1\nMERSA_MATRUH\nEgypt\n2003\n2\n12.53\n\n\n2\nMERSA_MATRUH\nEgypt\n2003\n3\n13.50\n\n\n3\nMERSA_MATRUH\nEgypt\n2003\n4\n17.30\n\n\n4\nMERSA_MATRUH\nEgypt\n2003\n5\n21.30\n\n\n...\n...\n...\n...\n...\n...\n\n\n83128\nLINCOLN_11_SW\nUnited States\n2003\n8\n25.01\n\n\n83129\nLINCOLN_11_SW\nUnited States\n2003\n9\n17.08\n\n\n83130\nLINCOLN_11_SW\nUnited States\n2003\n10\n13.31\n\n\n83131\nLINCOLN_11_SW\nUnited States\n2003\n11\n2.76\n\n\n83132\nLINCOLN_11_SW\nUnited States\n2003\n12\n-0.65\n\n\n\n\n83133 rows × 5 columns"
  },
  {
    "objectID": "posts/Homework1/HW1.html#countries_compare_temp_plot-function",
    "href": "posts/Homework1/HW1.html#countries_compare_temp_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "countries_compare_temp_plot() Function:",
    "text": "countries_compare_temp_plot() Function:\n\nThis plot will show multiple facets of the data. There will be a different line plot shown for each selected country.\nArguments:\n\n\ndb_file: a file of a sql database\ncountries: a list of countries that will be compared\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\n\n\nReturns:\n\n\nAn array of interactive line plots, one for each country. These line plots show the change in average temperature throughout the selected year in the countries.\n\n\nThe code for the countries_compare_temp_plot() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import countries_compare_temp_plot\nprint(inspect.getsource(countries_compare_temp_plot))\n\ndef countries_compare_temp_plot(db_file, countries, year, **kwargs):\n    '''\n    Returns multiple line plots, one for each country, that show the temperature in each month.\n    \n    Arguments:\n        db_file (str): a file of a sql database\n        countries (list): a list of countries that will be compared\n        year (int): the year that you want to observe data from\n        **kwargs: additional arguments for the px.line function\n        \n    Returns: \n        An array of line plots, one for each country. These line plots show the change in average temperature\n        throughout the countries.\n    '''\n    df = query_climate_database3(db_file, countries, year)\n    \n    # get the median temperature recording for each month in each country\n    df_temp = df.groupby([\"Country\", \"Month\"])[\"Temp\"].median()\n    \n    # create a dataframe that can be used in the px.line function\n    df_temp = pd.DataFrame({\"Country\":[ele for ele in countries for i in range(12)], \n                            \"Month\":[1,2,3,4,5,6,7,8,9,10,11,12] * len(countries), \n                            \"Avg Temp\":df_temp.values})\n    \n    return px.line(df_temp, x=\"Month\", y=\"Avg Temp\", color=\"Country\",\n                   facet_col=\"Country\",\n                   title=\"Chaining Multiple Figure Operations With A Plotly Express Figure\", \n                   hover_data = \"Avg Temp\",\n                   **kwargs)"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-countries_compare_temp_plot-function",
    "href": "posts/Homework1/HW1.html#testing-countries_compare_temp_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing countries_compare_temp_plot() Function:",
    "text": "Testing countries_compare_temp_plot() Function:\n\nWe will now test the function, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), year = 2020, month = 1, opacity = 0.8, color = “Country”.\nThis will give us a barplot with each bar representing a country and the heights of the bars representing the average temperature in that country in January of 2020. The specifications of the scatterplot plot are bar opacity (0.8) and bar color (different for each country).\n\n\nfig = countries_compare_temp_plot(db_file = \"hw1.db\", \n                                  countries = [\"India\", \"Egypt\", \"United States\"], \n                                  year = 2020, \n                                  markers = True)\nfig.show()\n\n\n\n\n\nThe function works as intended, clearly displaying the difference in average temperatures in 2020 between Egypt, India, and the United States.\nIt is apparent that both India and Egypt were significantly hotter than the United States in every month of 2020.\nIt is also interesting to note that India shows a strange drop in temperature in August and Egypt shows a strange drop in temperature in July, whereas the United States temperatures show a clear bell-shapes pattern throughout the year.\nComparisons such as there can be very useful when considering differences in climate between different countries.\nThis plot shows multiple facets of the data because the year and country are subsetted in the plots."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC 16B Website - Trent Bellinger",
    "section": "",
    "text": "Fake News Classification\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nDog and Cat Image Classification\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Dimensional Heat Diffusion Models\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nFlask Website With User Inputs\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraper for Movie and TV Show Reccomendations\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Historic Temperature Data for Different Countries\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\nNo matching items"
  }
]