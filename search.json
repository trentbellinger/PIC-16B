[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html",
    "href": "posts/FinalProjectWriteUp/index.html",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive/')\n\nMounted at /content/drive/"
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#overview",
    "href": "posts/FinalProjectWriteUp/index.html#overview",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "Overview",
    "text": "Overview\nThe Delayed Flight Site is a web-based application designed to predict flight departure delays within the United States. It aims to assist frequent flyers in efficiently organizing their flight schedules and travel plans by allowing them to anticipate and plan for potential delays."
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#objective",
    "href": "posts/FinalProjectWriteUp/index.html#objective",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "Objective",
    "text": "Objective\nOur objective was to use predictive analytics to enhance travel planning. Using historical flight data, our platform gives travelers insight to potential delays on their upcoming flights, minimizing the inconvenience of delays. The ultimate goal is to transform how travelers approach flying, shifting from reactive to proactive planning."
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#features",
    "href": "posts/FinalProjectWriteUp/index.html#features",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "Features",
    "text": "Features\n\nItinerary to Save User-Inputted Flights\nFlight Delay Predictions Based on User input\nInteractive Visualizations for Additional Insight"
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#data-acquisition-and-sql-database-creation",
    "href": "posts/FinalProjectWriteUp/index.html#data-acquisition-and-sql-database-creation",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "1. Data Acquisition and SQL Database Creation",
    "text": "1. Data Acquisition and SQL Database Creation\nWe started by importing flights data from the US Bureau of Transportation. This data was obtained through the following link: https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGK&QO_fu146_anzr=b0-gvzr The data that we got was organized by month, so we imported data from November of 2022 to November of 2023. The data contains all flights that have departure and arrival location in the United States. Each of the monthly datasets are loaded in below. We could not put these datasets into our GitHub because they were too large, so this process will only work on computers where the data is already uploaded. You can manually load in the data using the above link. We started with a separate pandas dataframe for each month, and concatenated it into a single dataframe containing all the data we collected. As we can see, the data has 7,818,349 rows and 30 columns, and was about 3GB when downloaded as a csv file. Each column contains different information about the flight, and a brief summary of each column is given in the FlightDelayModel.ibynp in the GitHub.\n\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\nnov22 = pd.read_csv(\"nov22.csv\")\ndec22 = pd.read_csv(\"dec22.csv\")\njan23 = pd.read_csv(\"jan23.csv\")\nfeb23 = pd.read_csv(\"feb23.csv\")\nmar23 = pd.read_csv(\"mar23.csv\")\napr23 = pd.read_csv(\"apr23.csv\")\nmay23 = pd.read_csv(\"may23.csv\")\njun23 = pd.read_csv(\"jun23.csv\")\njul23 = pd.read_csv(\"jul23.csv\")\naug23 = pd.read_csv(\"aug23.csv\")\nsep23 = pd.read_csv(\"sep23.csv\")\noct23 = pd.read_csv(\"oct23.csv\")\nnov23 = pd.read_csv(\"nov23.csv\")\nall_data = pd.concat([nov22, dec22, jan23, feb23, mar23, apr23, may23, jun23, jul23, aug23, sep23, oct23, nov23],\n                     ignore_index = True, axis = 0)\nall_data.shape\n\nDtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jan23 = pd.read_csv(\"jan23.csv\")\n&lt;ipython-input-3-d6c0b08a5729&gt;:14: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jul23 = pd.read_csv(\"jul23.csv\")\n\n\n(7818349, 50)\n\n\nAfter getting data about each flight, we also wanted to get longitude and latitude data for each airport to be able to eaily plot our observations. This was done through an API called airportsdata. We will create a pandas dataframe using the airports data that contains the longitude and latitude of every airport in the United States. This process is outlined below.\n\n!pip install -U airportsdata\nimport airportsdata\nairports = airportsdata.load('IATA')  # key is the ICAO identifier (the default)\nairports_df = pd.DataFrame([(airport, dic[\"lat\"], dic[\"lon\"]) for airport, dic in airports.items()])\nairports_df.rename(columns = {0:\"AIRPORT_ID\", 1:\"LATITUDE\", 2:\"LONGITUDE\"}, inplace = True)\nairports_df.head()\n\nRequirement already satisfied: airportsdata in /usr/local/lib/python3.10/dist-packages (20240316.1)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nOCA\n25.324317\n-80.275729\n\n\n1\nCYT\n60.080849\n-142.495494\n\n\n2\nFWL\n62.509183\n-153.890626\n\n\n3\nCSE\n38.851937\n-106.932821\n\n\n4\nCUS\n31.823711\n-107.626967\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow that we have two large datasets, we will create a SQL database to store the data. The creation of a SQL database will allow us to easily look at subsets of the data without having to load in the whole dataframe. This will significantly speed up the data loading for the rest of our project. We will use this SQL database in every aspect of our project from now on. This SQL database is created below.\n\nimport sqlite3\n\nconn = sqlite3.connect(\"flights.db\")\n\nall_data.to_sql(\"flights\", conn, if_exists = \"replace\", index = False)\nairports_df.to_sql(\"airports\", conn, if_exists = \"replace\", index = False)\n\nconn.close()\n\nWe will now perform a couple small tests to ensure that the adtabase is working as intended. First, we will try to select the origin, destination, flight number, and departure delay for all United Airlines flights on Noverber 1, 2022 that were delayed over 15 minutes.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    f\"\"\"\n    SELECT\n        ORIGIN, DEST, OP_CARRIER_FL_NUM, DEP_DELAY\n    FROM\n        flights\n    WHERE\n        YEAR = \"2022\"\n        AND\n        MONTH = 11\n        AND\n        DAY_OF_MONTH = 1\n        AND\n        OP_UNIQUE_CARRIER = \"9E\"\n        AND\n        DEP_DEL15 = 1\n    \"\"\"\n    df_test = pd.read_sql_query(cmd, conn)\n\nprint(df_test.shape)\ndf_test.head()\n\n(31, 4)\n\n\n\n  \n    \n\n\n\n\n\n\nORIGIN\nDEST\nOP_CARRIER_FL_NUM\nDEP_DELAY\n\n\n\n\n0\nLGA\nCVG\n4635\n22.0\n\n\n1\nCAE\nATL\n4675\n22.0\n\n\n2\nIND\nJFK\n4694\n20.0\n\n\n3\nBNA\nLGA\n4737\n105.0\n\n\n4\nLGA\nMCI\n4743\n146.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe will not preform a small test to make sure the table is functional in our database. We will output a dataframe of the airport id, latitude, and longitude of all airports lower than -100 longitude.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    \"\"\"\n    SELECT\n        AIRPORT_ID, LATITUDE, LONGITUDE\n    FROM\n        airports\n    WHERE\n        LONGITUDE &lt; -100\n    \"\"\"\n    df_airports_test = pd.read_sql_query(cmd, conn)\n\nprint(df_airports_test.shape)\ndf_airports_test.head()\n\n(1114, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nCYT\n60.080849\n-142.495494\n\n\n1\nFWL\n62.509183\n-153.890626\n\n\n2\nCSE\n38.851937\n-106.932821\n\n\n3\nCUS\n31.823711\n-107.626967\n\n\n4\nICY\n59.969019\n-141.661770\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can see that the database works as intended. This database will be very important for our project because it will allow us to quickly subset our dataset that contains about 8 million observations. The general structure of the database is displayed in the visualization below.\n\n\n\nsql_db.png"
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#data-preprocessing-and-building-a-model",
    "href": "posts/FinalProjectWriteUp/index.html#data-preprocessing-and-building-a-model",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "2. Data Preprocessing and Building a Model",
    "text": "2. Data Preprocessing and Building a Model\nNow that we have all the data that we need to proceed with our project, our goal for this section is to create a model that predicts whether a flight will be delayed by over 15 minutes. This is a binary classification problem. Thiw will correspond to the binary varuable DEP_DEL15 in the flights table of our SQL database. To predict this, we plan to use the flight departure date and time, arrival date and time, departure airport, arrival airport, carrier, and distance. This should give us enough insight because most delays are destination or airline specific. These predictors correspond to the columns YEAR, MONTH, DAY_OF_MONTH, CARRIER, DEP_TIME, ARR_TIME, DISTANCE in the flights table and the columns LATITUDE and LONGITUDE in the airports table. We will create a function to output these variables from the SQL database. This function will select the desired predictors from the flights table while joining the airports table by airport ID. The code for the function is shown and it is tested below.\n\nimport sqlite3\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\ndef get_flight_model_data():\n    '''\n    Returns a pandas dataframe of all the necessary predictors to predict flight delay.\n    (note: must have the flights.db database)\n    Args:\n        none\n    Returns:\n        a pandas dataframe with all necessary predictors\n    '''\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.YEAR, flights.MONTH, flights.DAY_OF_MONTH, flights.DEP_TIME, flights.ARR_TIME,\n            flights.OP_UNIQUE_CARRIER, flights.ORIGIN, flights.DEST, flights.DISTANCE, flights.DEP_DEL15,\n            airports.LATITUDE \"ORIGIN_LATITUDE\", airports.LONGITUDE \"ORIGIN_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.ORIGIN = airports.AIRPORT_ID\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.DEST, airports.LATITUDE \"DEST_LATITUDE\", airports.LONGITUDE \"DEST_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.DEST = airports.AIRPORT_ID\n        \"\"\"\n        df_dest = pd.read_sql_query(cmd, conn)\n    df[\"DEST_LATITUDE\"] = df_dest[\"DEST_LATITUDE\"]\n    df[\"DEST_LONGITUDE\"] = df_dest[\"DEST_LONGITUDE\"]\n    return df\n\ndf = get_flight_model_data()\nprint(df.shape)\ndf.head()\n\n(7818349, 14)\n\n\n\n  \n    \n\n\n\n\n\n\nYEAR\nMONTH\nDAY_OF_MONTH\nDEP_TIME\nARR_TIME\nOP_UNIQUE_CARRIER\nORIGIN\nDEST\nDISTANCE\nDEP_DEL15\nORIGIN_LATITUDE\nORIGIN_LONGITUDE\nDEST_LATITUDE\nDEST_LONGITUDE\n\n\n\n\n0\n2022\n11\n1\n1355.0\n1747.0\n9E\nXNA\nLGA\n1147.0\n0.0\n36.281579\n-94.307766\n40.777250\n-73.872611\n\n\n1\n2022\n11\n1\n1412.0\n1609.0\n9E\nLGA\nCVG\n585.0\n1.0\n40.777250\n-73.872611\n39.048837\n-84.667821\n\n\n2\n2022\n11\n1\n1345.0\n1550.0\n9E\nLGA\nXNA\n1147.0\n0.0\n40.777250\n-73.872611\n36.281579\n-94.307766\n\n\n3\n2022\n11\n1\n1550.0\n1648.0\n9E\nLSE\nMSP\n119.0\n0.0\n43.879266\n-91.256634\n44.881972\n-93.221778\n\n\n4\n2022\n11\n1\n1418.0\n1506.0\n9E\nMSP\nLSE\n119.0\n0.0\n44.881972\n-93.221778\n43.879266\n-91.256634\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow that we have our data, we must proceed to making our data ready to be inputted into our model. There is only one categorical predictor, OP_UNIQUE_CARRIER. We need to see if there are any similarities between carriers to encode the predictor. We will first look at the proportion of delayed flights with the same carrier to see if there is any grouping we can make. This could potentially give some feedback in the final model, but after the model was tested this was the most efficient way to encode these variables. We will also look at the average distance of flights with the same carrier to see if there is any grouping we can make. As shown in the scatterplot below, there are four main groups, which we will use to encode the carriers. There is one group in the bottom left, one group in the top right, one group in the bottom middle and one group in the top middle. We will encode the flight carriers PT, YX, 9E, QX, OH, OO, C5, G7, and MQ as 0, HA, ZW, YV, and WN as 1, DL, AA, G4, UA, and AS as 2, and B6, F9, and NK as 3.\n\nimport matplotlib.pyplot as plt\n\n# get proportion of delays for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DEP_DEL15\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_delays = carrier_groups.mean()\n\n# get median distance of flight for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DISTANCE\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_distances = carrier_groups.median()\n\n# create a plot of proportion of delays vs average diatance\nfig, ax = plt.subplots()\nax.scatter(x = carrier_delays, y = carrier_distances)\nax.set_title(\"Flight Delays and Distance for Carriers\")\nax.set_xlabel(\"Proportion of Delayed FLights\")\nax.set_ylabel(\"Average Distance of Flights\")\n\nfor i, txt in enumerate(carrier_delays.index):\n    ax.annotate(txt, (carrier_delays[\"DEP_DEL15\"][i], carrier_distances[\"DISTANCE\"][i]))\n\n# encode the OP_UNIQUE_CARRIER column\ndf[\"OP_UNIQUE_CARRIER\"].replace({\"PT\":0, \"YX\":0, \"9E\":0, \"QX\":0, \"OH\":0, \"OO\":0, \"C5\":0, \"G7\":0, \"MQ\":0, \"HA\":1, \"ZW\":1, \"YV\":1, \"WN\":1, \"DL\":2, \"AA\":2, \"G4\":2, \"UA\":2, \"AS\":2, \"B6\":3, \"F9\":3, \"NK\":3}, inplace = True)\ndf[\"OP_UNIQUE_CARRIER\"].value_counts()\n\n2    3244790\n0    2013241\n1    1786664\n3     773654\nName: OP_UNIQUE_CARRIER, dtype: int64\n\n\n\n\n\n\n\n\n\nNow that we have all of our variables prepped to use in our model, we must figure out how to deal with NA values in our data. There were a small number in proportion to the size of the data, as shown below. We can see that about 0.35% of the data is NA values. This is an extremely small amount, so we will deal with the NAs by simply removing any rows that contain NA values, which is also done below.\n\nprint(\"Proportion of NAs: \", (df.isna().sum().sum()) / df.size)\nprint(\"Shape before dropping NAs: \", df.shape)\ndf = df.dropna()\nprint(\"Shape after dropping NAs: \", df.shape)\n\nProportion of NAs:  0.0034782736282119335\nShape before dropping NAs:  (7818349, 14)\nShape after dropping NAs:  (7687386, 14)\n\n\nWe will now check the rate at which our model should perform. This will be done by looking at the proportion of flights which are delayed, done below. As we can see, the data is extremely imbalanced. There are way more non-delayed flights than theer are delayed flights. The base rate is about 0.8, which is going to be very high for the model that we plan to create because of outside variability. Seeing that we have over 7 million observations, we can certainly even out this category in the data to make the model more accurate. We will remove some of the data of the non-delayed flights to make the number of flights in each category equal. This is done below.\n\nprint(\"Proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"Proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\n# make the number of delayed and not delayed flights equal\ng = df.groupby('DEP_DEL15')\ndf = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\nprint(\"New data shape: \", df.shape)\n\nprint(\"New proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"New proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\nProportion of delayed flights:  0.20882612112882065\nProportion of non-delayed flights:  0.7911738788711793\nNew data shape:  (3210654, 14)\nNew proportion of delayed flights:  0.5\nNew proportion of non-delayed flights:  0.5\n\n\nWe will now create a training and testing dataset for our model, done below. We will set aside 20% for testing and keep 80% for training.\n\nfrom sklearn.model_selection import train_test_split\n\nmodel_X = df[['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DEP_TIME', 'ARR_TIME', 'OP_UNIQUE_CARRIER', 'DISTANCE', 'ORIGIN_LATITUDE', 'ORIGIN_LONGITUDE', 'DEST_LATITUDE', 'DEST_LONGITUDE']]\nmodel_y = df[['DEP_DEL15']]\nX_train, X_test, y_train, y_test = train_test_split(model_X, model_y, test_size = 0.2, random_state = 50)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n(2568523, 11) (642131, 11) (2568523, 1) (642131, 1)\n\n\nWe can now move on to model fitting. We started with training many deep neural network models, which took very long to train and were ultimately not as successful as we hoped. Our best model ended up being a random forest classifier, as shown below. we tested different values for n_estimators, max_depth, min_samples_split, and min_samples leaf, but the best accuracy with the default values used by RandomForestClassifier(). As we can see, the model predicts whether a flight will be delayed with 83% accuracy. This is very good, expecially considering the amount of variabiliy that goes into flight delays.\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nrf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0)\nrf.fit(X_train, np.array(y_train).flatten())\nrf.score(X_test, np.array(y_test).flatten())\n\nThe above code was not able to be run due to the size of the data, but the model has 83% test accuracy."
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#making-a-flask-web-app",
    "href": "posts/FinalProjectWriteUp/index.html#making-a-flask-web-app",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "3. Making a Flask Web App",
    "text": "3. Making a Flask Web App\nOur Flask web app is designed to grant a user intuitive access to the features that our project provides. Primarily, checking the delay status of individual flights as well as creating itineraries that can be visualized using the Plotly Dash apps described in the following sections. Additionally, we added the option for a user to create an account in order to save flights and itineraries to be viewed at a later time.\n\nSite Database Creation\nThe first step taken in the web app was to create a database that stored the information for each user. This database contained three tables: user, itineraryCounter, and itineraries. The user table stored the usernames as well as the hashed passwords of each user, a table called when logging in or creating a new account. The itinerary counter table served as a way to help index our final table, itineraries. This itinerary page contained several columns: itin_id, author_id, origin, destination, airline, depDate, and arrTime. Each row in this table represents a single flight, with the itin_id and author_id letting us know which itinerary that flight belongs to. The code for The creation of our database is contained below within our get_db() function, which simultaneously opens a connection with the database, as well as creating the tables if they have not already been created.\n\n'''\nOpens up connection with the database, creating the necessary tables if they have not already been created.\n'''\ndef get_db():\n    if 'db' not in g:\n        #connecting to database\n        g.db = sqlite3.connect(\"webProj.sqlite\")\n        #creating cursor so we can interact with database\n        cursor = g.db.cursor()\n        #execute creation of tables using (CREATE TABLE IF NOT EXISTS)\n        #creatin table for user login information\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS user ( id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE NOT NULL, password TEXT NOT NULL)\")\n        #creating table for itinerary identification\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraryCounter ( id INTEGER PRIMARY KEY AUTOINCREMENT, counter TEXT)\")\n        #creating table holding information to be stored in itineraries (ex. flight info), to be tied back to each itinerary in previous table via itin_id value\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraries ( id INTEGER PRIMARY KEY AUTOINCREMENT, itin_id INTEGER NOT NULL, author_id TEXT, origin TEXT, destination TEXT, airline TEXT, depTime TEXT, arrTime TEXT)\")\n        #allowing us to access columns by their name\n        g.db.row_factory = sqlite3.Row\n        #commiting cursor changes\n        g.db.commit()\n    return g.db\n\n\n\nRegistration and Login\nNow that we have our database set up, we can begin to register users and save itineraries. In order to register, we must take in the input from the user, ensuring that their requested username is not already in use by another user, and hash their password for security purposes. Additionally, after their successful registration, we reroute them to the login page where they can test that their new login information is working correctly. The functions are pictured below.\n\n'''\nPage used for registering new users into the database. Takes in user input for\nusername and password, ensuring username is not already taken, before rerouting to\nlogin page.\n'''\n@auth_bp.route('/register', methods=('GET', 'POST'))\ndef register():\n    if request.method == 'POST':\n        #getting user input for their desired username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opening connection with the database\n        db = get_db()\n        error = None\n\n        #checks to make sure username and password are filled out\n        if not username:\n            error = 'Username is required.'\n        elif not password:\n            error = 'Password is required.'\n\n\n        if error is None:\n            #inserts user information into database\n            try:\n                db.execute(\"INSERT INTO user (username, password) VALUES (?,?)\", (username, generate_password_hash(password)),\n                )\n                db.commit()\n            #checks if username already exists\n            except db.IntegrityError:\n                error = f\"Username {username} is already registered.\"\n            else:\n                #sends user to login page where they can login with their newly created account\n                return redirect(url_for(\"auth.login\"))\n        #error shown to user if there is one\n        flash(error)\n    return render_template('auth/register.html')\n\nWe can see that we first gather user input by assigning our username and password variables as the input from request forms displayed on the resgister page. We then open up a connection to the database using the get_db() function we creating previously, which will allow us to check that the username is not already in use. To accomplish this, we use a try statement in which we insert the username and the hashed password into the user table, excepting an IntegrityError, which would imply that the username is already in use. It that is the case, we send a message to the user that their chosen username is already in use, prompting them to choose a new one. If not, the username and password combination will be placed in the user table, and the user will be redirected to the login page, as shown by the redirect near the end of the code block. Below is an example of the error message shown when trying to register a username that is already taken. \n\n'''\nPage where users are able to log back in.\n'''\n@auth_bp.route('/login', methods = ('GET', 'POST'))\ndef login():\n    if request.method == 'POST':\n        #user enters their username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opens connection with database\n        db = get_db()\n        error = None\n        #checks for username in database\n        user = db.execute('SELECT * FROM user WHERE username = ?', (username,)).fetchone()\n        #gives error if username not found\n        if user is None:\n            error = 'Incorrect username.'\n        #checks username password against inputted password\n        elif not check_password_hash(user['password'],password):\n            error = 'Incorrect password.'\n        #resets session and sets user id to user who just logged in\n        if error is None:\n            session.clear()\n            session['user_id'] = user['id']\n            #sends us back to main page, now as a logged in user\n            return redirect(url_for('index'))\n        #flashes error if one exists\n        flash(error)\n    return render_template('auth/login.html')\n\nSimilar to the register page, we gather user information using request.form.get() commands, and then open a connection with the database using get_db(). We then check to ensure that the inputted username is contained within the user table, using the cursor db that is returned by the get_db() function. If nothing is returned, that means the username is not contained within the table, which means that username is not associated with any existing user. Therefore, we send an incorrect user message, and prompt them to enter a different username. If the username is contained within the table, we can then move on to checking that the inputted password is correct. However, as we hashed the password for security reasons on the register page, we cannot simply check that user[‘password’]==password, as it would almost always return false. Instead, we call the function check_password_hash(), and pass in the hashed user password contained within the database, as well as the inputted password. This will now check that the hashed versions of the passwords match up, which if true means that the user has successfully inputted their login information and should now be fully logged in. In order to accomplish this, we set the session[‘user_id’] to be the inputted username. Note that we clear the session ahead of time just in case. The session stores variables that will be able to be used across all pages contained within the app, and as the information of the logged in user will be used across most pages, it makes sense to store it as a session variable.\n\n\nHandling User Input\nNow that we have finished the registration and login functionality of our site, we can begin to create the ability for the user to input flights and itineraries. Let us first look at taking in the input for an individual flight, which is displayed on our ‘flights’ page. We plan on taking in information regarding the origin, destination, airline, departure time, and arrival time of each flight. Therefore we will need to initialize five variables for this function, assigning them the values inputted by the user by again using request.form.get(). Now that we have obtained the input from the user, we can perform a bit of data processing by formatting the date into a more visually appealing format, accomplished by use of the strptime and strftime functions. After checking that each form has been filled out, we can begin to utilize this data. We first create a dictionary containing all of the flight data called flightDict, and append it to an empty list flightList. While it appears that creating a list with a single dictionary within it seems somewhat redundant, this helps to fit the format of when we have multiple flights from an itinerary. We then use the inputs we have received and use them to run the model we created, described in section two of the post, which will tell us whether or not we expect the flight to be delayed. Finally, we utilize a redirect to the ‘flightDisp’ page which will display all the user inputs, as well as the results of the model for our user to see. It is critical that we also pass the flight information as well as the result of the model that we just ran to the next page as well. You may be confused as to way we are passing each variable individually as opposed to the list we created, and it is due simply to the fact that when passing values this way, it is much simpler to obtain them on the next page when they are passed one by one.\n\n'''\nPage where user enters a single flight to check the estimate of it being delayed. Page is reached via the link in the navbar and will redirect to /flightDisp page where flight information is to be displayed.\n'''\n@server.route('/flights', methods = ['GET', 'POST'])\ndef flights():\n    if request.method == 'POST':\n\n        # Obtaining flight input from user\n        origin = request.form.get('origin')\n        destination = request.form.get('destination')\n        airline = request.form.get('airline')\n        date = request.form.get('date')\n        arrivalTime = request.form.get('arrivalTime')\n\n        # Altering format of date to make it more readable\n        date = dt.strptime(date, '%Y-%m-%dT%H:%M')\n        date = date.strftime('%d/%m/%Y %H:%M')\n\n        # Altering formate of arrivalTime to make it more readable\n        arrivalTime = dt.strptime(arrivalTime, '%Y-%m-%dT%H:%M')\n        arrivalTime = arrivalTime.strftime('%d/%m/%Y %H:%M')\n\n        # Initializing error to be none\n        error = None\n\n        # Checking all fields have been filled out, yielding an error if not\n        if not origin:\n            error = 'Please enter origin.'\n        elif not destination:\n            error = 'Please enter destination.'\n        elif not airline:\n            error = 'Please select an airline.'\n        elif not date:\n            error = 'Please input a departure date and time.'\n        elif not arrivalTime:\n            error = 'Please enter your estimated arrival time.'\n        if error is None:\n            # Creating flightList that will be passed to our layout function\n            flightList = []\n\n            # Initializing flightDict that contains all of the entered flight information\n            flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrDate':arrivalTime}\n\n            # Placing flight information into flightList\n            flightList.append(flightDict)\n\n            # Calling layoutDash to pass the flight information to Dash app, passed in name of app and flightList\n            #layoutDash(dash1, flightList)\n\n            # get airline abbreviation\n            airline_abr = airlineDict[airline]\n\n            # get day, month, year, and time as integers\n            dep_date_split = date.replace('/', ' ').replace(':', ' ').split()\n            day_of_month = int(dep_date_split[0])\n            month = int(dep_date_split[1])\n            year = int(dep_date_split[2])\n            dep_time = int(dep_date_split[3] + dep_date_split[4])\n\n            # get arrival time\n            arr_date_split = arrivalTime.replace('/', ' ').replace(':', ' ').split()\n            arr_time = int(arr_date_split[3] + arr_date_split[4])\n\n            # get origin longitude and latitude\n            origin_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lon\"].tolist()[0]\n            origin_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lat\"].tolist()[0]\n\n            # get destination longitude and latitude\n            dest_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lon\"].tolist()[0]\n            dest_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lat\"].tolist()[0]\n\n            distance = distances.loc[(distances[\"ORIGIN\"] == origin) & (distances[\"DEST\"] == destination), \"AVG_DISTANCE\"].tolist()[0]\n\n            if airline in ['PT', 'YX', '9E', 'QX', 'OH', 'OO', 'C5', 'G7', 'MQ']:\n                carrier = 0\n            elif airline in ['HA', 'ZW', 'YV', 'WN']:\n                carrier = 1\n            elif airline in ['DL', 'AA', 'G4', 'UA', 'AS']:\n                carrier = 2\n            else:\n                carrier = 3\n\n            X_new = pd.DataFrame({'YEAR':year, 'MONTH':month, 'DAY_OF_MONTH':day_of_month,\n                                  'DEP_TIME':dep_time, 'ARR_TIME':arr_time,\n                                  'OP_UNIQUE_CARRIER':carrier, 'DISTANCE':distance,\n                                  'ORIGIN_LATITUDE':origin_lat, 'ORIGIN_LONGITUDE':origin_lon,\n                                  'DEST_LATITUDE':dest_lat, 'DEST_LONGITUDE':dest_lon}, index = [0])\n\n            pred = rf_model.predict(X_new).tolist()[0]\n\n            if pred == 0:\n                delay = \"our model predicts no delays for your flight.\"\n            elif pred == 1:\n                delay = \"our model predicts that your flight will be delayed at least fifteen minutes.\"\n            else:\n                delay = \"our model returned inconclusive results.\"\n\n            # Send user to Dash app for visualization\n            #return redirect('/dashFlight/')\n            return redirect(url_for(\"flightDisp\", origin=origin, destination=destination, airline=airline, date=date, arrivalTime=arrivalTime, delay=delay, pred = pred))\n\n        # Flash error if one was present\n        flash(error)\n\n    #Rendering template, passing in airlineDict and flightInputDict to provide options in the searchable dropdown menus\n    return render_template('blog/flights.html', airlineDict = airlineDict, flightInputDict = flightInputDict)\n\nNote that our itinerary flights page is very similar to the above page, with a couple key exceptions. As our user is able to select how many flights they wish to place into their itinerary, we must run a for loop in order to ensure we take in all of their inputs. Additionally, we do not run the model as there are multiple flights in the itinerary, and it makes it much simpler and more efficient to do it in this manner.\n\n'''\nPage where user is able to input the information for the number of flights they specified on the /itinNum page. Page will forward to /itinDisp page where the complete itinerary is to be displayed.\n'''\n@server.route('/itinFlights', methods = ('GET', 'POST'))\ndef itinFlights():\n    # Get the number of flights passed from previous page\n    numFlight = int(request.args.get('numFlight'))\n\n    if request.method=='POST':\n        # Instructions if user selects the 'See itinerary' button\n        if request.form.get('action') == \"See itinerary\":\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting first depDate to a datetime object that we will be able to extract the hour value\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving variables to session so that the Dash app will be able to access them\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Sending user to Dash app for visualization\n            return redirect(url_for('/dashFlight/', flightList = flightList))\n\n        # Instructions if user clicks 'Save itinerary' button\n        elif request.form.get('action') == \"Save itinerary\":\n\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting sesDate to a datetime object\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving certain variables to session so they can be used by next page\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Redirecting to save page\n            return redirect(url_for('saveItin'))\n\n    # Rendering template, passing in numFlight for iterative purposes, as well as two dictionaries that the searchable dropdowns will access for their options\n    return render_template('blog/itinFlights.html', numFlight = numFlight, airlineDict=airlineDict, flightInputDict = flightInputDict)\n\n\n\nSaving Itineraries\nSaving a flight or itinerary is relatively straightforward. The trickiest part of this process is ensuring that each flight in an itinerary is given the same itin_id so that they can be accessed all together when we wish to display them. The first thing we do is open a connection to the database as we will need to place new information into our database in order to save it. In order to increment our itin_id by one each time, we check the maximum itin_id from itineraries and then increment it by one to ensure we are not accidentally placing our new itinerary into an already existing itinerary. We then loop through each flight within our flightList, that we access using the session variable ‘dashboard_id’, and reformat our departure and arrival dates and times into a readable format. Note that we know how many flights are contained within the list by simply checking the length of the list, using that as the range of our for loop. Finally, we run another for loop within the same range, inserting a new row for each flight within our list, passing in the itin_id we found earlier as our itin_id, session[‘user_id’] as the author_id, and then the corressponding value from each flight to its respective column in the database. To finish it all off, we commit the changes to the database using db.commit() and then close our connection to the databse before redirecting to our ‘itinDisp’ page where the user will see their saved itineraries.\n\n'''\nSaves a created itinerary and then reroutes to /allItins page. Only possible to be called if user is logged in.\nStill need to write code and finish HTML file.\nNOTE: May or may not be implemented\n'''\n@server.route('/saveItin')\n@login_required\ndef saveItin():\n    # Opening database connection\n    db = get_db()\n    # Using dummy variable to help assign itinerary ids\n    dumVar = 'textCount'\n    # Adding dummy variable to itineraryCounter table so we can figure out how many itineraries we have\n    db.execute(\n        'INSERT INTO itineraryCounter (counter) VALUES (?)',\n        (dumVar,))\n    # Commiting insertion into itineraryCounter\n    db.commit()\n    # Finding max itin_id from table and incrementing our variable by one for new itinerary\n    itin_id = db.execute('SELECT MAX(itin_id) FROM itineraries').fetchone()[0]\n    if itin_id is None:\n        itin_id = 1\n    else:\n        itin_id += 1\n\n    # Changing format of time for disply\n    for i in range(0, len(session['dashboard_id'])):\n        session['dashboard_id'][i]['depDate'] = dt.strptime(session['dashboard_id'][i]['depDate'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['depDate'] = session['dashboard_id'][i]['depDate'].strftime(\"%d/%m/%Y %H:%M\")\n        session['dashboard_id'][i]['arrTime'] = dt.strptime(session['dashboard_id'][i]['arrTime'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['arrTime'] = session['dashboard_id'][i]['arrTime'].strftime(\"%d/%m/%Y %H:%M\")\n\n    # Placing information into database for each flight in itinerary, all with same itin_id\n    for i in range (0, len(session['dashboard_id'])):\n        db.execute(\n            'INSERT INTO itineraries (itin_id, author_id, origin, destination, airline, depTime, arrTime) VALUES (?,?,?,?,?,?,?)',\n            (itin_id, session['user_id'], session['dashboard_id'][i]['origin'], session['dashboard_id'][i]['destination'], session['dashboard_id'][i]['airline'], session['dashboard_id'][i]['depDate'], session['dashboard_id'][i]['arrTime']))\n    # Commit and close the database\n    db.commit()\n    db.close()\n    # Sending user to itinerary page where they can see their newly saved itinerary\n    return redirect(url_for('itinsDisp'))\n\n\n\nDisplaying Saved Information\nNow that we have allowed the user to save their itineraries, we need a way for them to be able to see and access these itineraries. This is accomplished on our ‘dispItins’ page. In order to access a user’s itineraries, we need to access the database, hence we will again use the get_db() function to open up a connection with the database. We now need to create a list of all the flights saved under a certain user in our itineraries table. To accomplish this, we use our cursor from get_db() to execute a command that selects all of the columns from each flight, given that the author_id of each flight is the same as the user that is currently logged in. Remember that when a user is logged in, we set the session variable ‘user_id’ to be the username of the user, so we again use this session variable to select the flights to be included. We then utilize the command fetchall() to ensure that we are returned a list of all of the flights in itineraries saved by our desired user. Then, for ease of use when sending information from this page to other pages, we loop through our flights list and create a dictionary for each flight, similar to what we did in the previous pages, and append all of these dictionaries to list titled ‘flights_list’. Additionally, we create a list of all the unique itin_ids contained within the flight list. This helps us to determine how many itineraries are going to be displayed. This is critical as for each itinerary there is a button that allows the user to see a visualization of that itinerary, so we now know how many buttons we will need to consider. With this information, we can now check what button the user inputted by using a for loop, with the button identification being an f-string, utilizing the i value. Now, we use the button value as a way to index our list containing the itinerary ids to find which itinerary the user would like to see. We then run through the flights_list and append any flight with the correct itin_id to a list titled dashList that will contain all the flights we wish to visualize. Now that we have selected all the flights in the itinerary that we plan on visualizing, we set the session variable ‘dashboard_id’ equal to our dashList, and obtain the hour value for the departure time of our first flight to be ‘dateDash’, allowing the Dash app to access and visualize this itinerary. Finally, we reroute the user to the Dash app where they will be shown the visualization for their itinerary.\n\n'''\nPage displays all the itineraries for the logged in user.\n'''\n@server.route('/dispItins', methods =('GET', 'POST'))\n@login_required\ndef itinsDisp():\n    # Opening connection with database\n    db = get_db()\n\n    # Getting list of all flights in itineraries created by current user\n    flights = db.execute(\n        'SELECT f.id, f.itin_id, author_id, origin, destination, airline, depTime, arrTime'\n        ' FROM itineraries f WHERE author_id = ?', (session['user_id'],)\n    ).fetchall()\n\n    # Initializing empty list that will contain flight information\n    flights_list = []\n    # Converting information for flights from database into a dictionary for each flight\n    for flight in flights:\n        flight_dict = {\n            'id': flight['id'],\n            'itin_id': flight['itin_id'],\n            'author_id': flight['author_id'],\n            'origin': flight['origin'],\n            'destination': flight['destination'],\n            'airline': flight['airline'],\n            'depTime': flight['depTime'],\n            'arrTime': flight['arrTime']\n        }\n\n        # Adding flight dictionaries to flights_list\n        flights_list.append(flight_dict)\n\n    # Obtaining all unique itinerary ids in user's itineraries\n    itin_ids=[]\n    for flight in flights:\n        if flight['itin_id'] not in itin_ids:\n            itin_ids.append(flight['itin_id'])\n\n\n    if request.method=='POST':\n        dashList = []\n        # Run through number of buttons\n        for i in range(0,len(itin_ids)+1):\n\n            if request.form.get('action') == f'See Itinerary {i}':\n                # Get itinID for selected itinerary\n                retItinID = itin_ids[i-1]\n                # Compile all flights in that itinerary\n                for el in flights_list:\n                    if el['itin_id']==retItinID:\n                        dashList.append(el)\n\n                # Assign session variables to be used by the dash app\n                sesDate = dt.strptime(dashList[0]['depTime'], '%d/%m/%Y %H:%M')\n                session['dashboard_id'] = dashList\n                session['dateDash'] = sesDate.hour\n                # Redirecting to Dash app\n                return redirect(url_for('/dashFlight/'))\n\n    return render_template('blog/itinsDisp.html', flights = flights)\n\nBelow is a screenshot of a user’s itinerary page. We can see that there are multiple ‘See itinerary’ buttons, which are handled within the code block above. \n\n\n\nitinDispScreenshot.png"
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#adding-interactive-visualizations-with-plotly-dash",
    "href": "posts/FinalProjectWriteUp/index.html#adding-interactive-visualizations-with-plotly-dash",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "4. Adding Interactive Visualizations with Plotly Dash",
    "text": "4. Adding Interactive Visualizations with Plotly Dash\nThis Plotly Dash app is an interactive tool designed to give users insights into flight delays and travel patterns across U.S. airports.\nWe pulled the data from the database we created as described in part 2, and used this data to create our visualizations.\nFor faster processing time, we decided to process the data first and writing them out to separate CSV files that were suited for each visualization instead of processing the data within our Dash app.\nIt features three core visualizations:\n\nFlight Routes Visualization: Users can input up to ten pairs of departure and arrival airport codes to plot the routes on a map. The routes are color-coded by average delay proportions, helping users identify which routes typically experience more delays.\nHeatmap: This displays a heatmap overlay on a U.S. map, showing the volume of flights departing from each airport (denoted by the size of the circles) and the proportion of those flights that are delayed (indicated by the color intensity).\nRush Hour Analysis: By entering an airport code and a specific hour, users can generate a bar chart that reveals the busiest times for departures at that airport, providing insights into peak travel hours and potential rush times.\n\nThe app’s layout includes a markdown block at the top that explains the functionalities and how to use the app. A RadioItems selection lets users choose between the flight routes, heatmap, or rush hour visualizations, dynamically updating the display content based on their choice.\nCallbacks are set up to respond to user interactions, such as entering airport codes and clicking the “Plot Routes” button, which generates the visualizations accordingly. For the heatmap and rush hour charts, the app processes flight count and delay data, providing a detailed analysis of travel patterns for better planning and decision-making. The app is equipped to handle data transformations and plotting, making it a comprehensive tool for travelers looking to optimize their itineraries.\n\nHere is a more detailed look at each visualization\n\n\nFlight Routes Visualization:\nThis app allows you to visualize flight routes between airports and the average proportion of delays. Enter the airport codes for departures and arrivals, and press “Plot Routes” to see the routes on the map.\nEach number in the legend is a group number that represents the proportion of delayed flights on average for each route.\nHere is what each number in the legend means:\n    - 0: delay proportion &lt;= 0.1\n    - 1: 0.1 &lt; delay proportion &lt;= 0.15\n    - 2: 0.15 &lt; delay proportion &lt;= 0.2\n    - 3: 0.2 &lt; delay proportion &lt;= 0.25\n    - 4: 0.25 &lt; delay proportion &lt;= 0.3\n    - 5: delay proportion &gt; 0.3\n\n# @title Flight Routes Visualization\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path = '/content/drive/MyDrive/PIC16B_Datasets/routes.png'\n\n# Display the image\ndisplay(Image(filename=image_path))\n\n\n\n\n\n\n\n\nCallback: update_map\nThis callback listens for user interaction with the ‘Plot Routes’ button or changes in the visualization mode selector. Upon activation, it processes user input to plot flight routes between selected departure and arrival airports. The callback assembles a list of route data based on the input fields for up to 10 routes.\n\n# Callback to update the map based on the inputs\n@app.callback(\n    #Output('content-route', 'children'),\n    Output('map', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('plot-button', 'n_clicks')\n    ],\n    [State({'type': 'departure-input', 'index': ALL}, 'value'),\n     State({'type': 'arrival-input', 'index': ALL}, 'value'),\n    ]\n)\n\nFunction: update_map\nThis function is the core of the update_map callback. It transforms user input into uppercase to match the database format, constructs a key for each route combining departure and arrival airport codes, and retrieves relevant data like group classification, flight count, and delay proportion. It then checks for the existence of coordinates for the given airports and, if found, prepares a structured dictionary of route information to be plotted.\n\ndef update_map(n_clicks, vis_mode, departures, arrivals):\n    \"\"\"\n    Responds to user inputs to generate and update a flight route map.\n\n    Parameters:\n    - n_clicks (int): Number of times the plot button has been clicked.\n    - vis_mode (str): The visualization mode selected by the user.\n    - departures (list): List of user-input departure airport codes.\n    - arrivals (list): List of user-input arrival airport codes.\n\n    Returns:\n    - Plotly Figure: A figure object that contains the updated flight routes map.\n    \"\"\"\n    routes = []\n\n    departures = [dep.upper() for dep in departures if dep is not None]\n    arrivals = [arr.upper() for arr in arrivals if arr is not None]\n\n    # loop through each pair of depature and arrival inputs\n    for dep, arr in zip(departures, arrivals):\n        if dep and arr:  # Ensure both inputs are provided\n            # Generate the composite key for the current route\n            route_key = f\"{dep}_{arr}\"\n            # Look up the group for the current route\n            group = route_dict.get(route_key)\n            count = count_dict.get(route_key)\n            delay_proportion = dep_del_dict.get(route_key)\n            dep_coords = airport_coordinates.get(dep)\n            arr_coords = airport_coordinates.get(arr)\n\n            # Proceed only if coordinates for both airports are found\n            if dep_coords and arr_coords:\n                # Construct the route data structure\n                route = {\n                    \"departure_airport\": dep,\n                    \"arrival_airport\": arr,\n                    \"departure_lat\": dep_coords['lat'],\n                    \"departure_lon\": dep_coords['lon'],\n                    \"arrival_lat\": arr_coords['lat'],\n                    \"arrival_lon\": arr_coords['lon'],\n                    \"delay_proportion\": delay_proportion,\n                    \"group\": group,\n                    \"flight_count\": count\n                }\n                routes.append(route)\n\n    fig = create_figure_with_routes(routes)\n\n    return fig\n\nFunction: create_figure_with_routes\nThis function takes the list of routes created by update_map and visualizes them on a map. Each route is represented as a line between its departure and arrival coordinates with markers at each airport. The lines and markers are color-coded by delay proportion group. The function sets up the map’s appearance, including its geographic projection (set to the United States) and styling details. The resulting figure is then returned to the update_map callback to update the ‘map’ component in the app’s layout.\n\ndef create_figure_with_routes(routes):\n    \"\"\"\n    Creates a Plotly map visualization for the given flight routes.\n\n    Parameters:\n    - routes (list): A list of dictionaries, each containing data for a flight route.\n\n    Returns:\n    - Plotly Figure: A figure object that visualizes the flight routes on a map.\n    \"\"\"\n    fig = go.Figure()\n    # Define a color scheme for the different groups\n    group_colors = {\n        0: \"#1f77b4\",  # Muted blue\n        1: \"#ff7f0e\",  # Safety orange\n        2: \"#2ca02c\",  # Cooked asparagus green\n        3: \"#d62728\",  # Brick red\n        4: \"#9467bd\",  # Muted purple\n        5: \"#8c564b\",  # Chestnut brown\n    }\n    # Loop through each route and add it to the figure with the respective color\n    for route in routes:\n        # Get the color for the current group or default to black if not found\n        route_color = group_colors.get(route[\"group\"])\n\n        fig.add_trace(\n            go.Scattergeo(\n                lon = [route[\"departure_lon\"], route[\"arrival_lon\"]],\n                lat = [route[\"departure_lat\"], route[\"arrival_lat\"]],\n                text = [f\"{route['departure_airport']}\", f\"{route['arrival_airport']}\"],\n                hoverinfo='text',\n                mode = \"lines+markers\",\n                line = dict(width = 2, color = route_color),\n                marker = dict(size = 4, color = route_color),\n                name = route[\"group\"],\n            )\n        )\n        # Update layout of the map\n    fig.update_layout(\n        title_text = \"Flight Routes and Delay Proportions\",\n        showlegend = True,\n        geo = dict(\n            projection_type = \"albers usa\",\n            showland = True,\n            landcolor = \"rgb(200, 200, 200)\",\n            countrycolor = \"rgb(204, 204, 204)\",\n            showsubunits=True,  # Show state lines and other subunits\n            subunitwidth=1  # Width of the subunit lines (state lines)\n        ),\n    )\n    return fig\n\n\n\nHeatmap Visualization:\nThe heatmap illustrates U.S. airport departures, highlighting flight volume and delay frequency.\nLarger circles denote more flights; color intensity reflects higher delay percentages.\n\n# @title Heatmap\n\n# Correct path with no spaces in folder names\nimage_path1 = '/content/drive/MyDrive/PIC16B_Datasets/heatmap.png'\n\n# Display the image\ndisplay(Image(filename=image_path1))\n\n\n\n\n\n\n\n\nCallback: create_composite_map\nThis callback updates the content of the ‘content-heatmap’ division based on the visualization mode selected by the user. When the ‘Heatmap’ mode is selected, it triggers the create_composite_map function to generate and display a heatmap.\n\n@app.callback(\n    Output('content-heatmap', 'children'),\n    Input('vis-mode-selector', 'value')\n)\n\nFunction: create_composite_map\nThe create_composite_map function constructs a heatmap visualization of U.S. flight departures. It uses the Scattergeo trace of Plotly to plot the longitude and latitude of origin airports as points on a map.\nEach point’s size represents the delay proportion, offering a visual representation of the flight volume and delay frequency. The color intensity corresponds to higher delay percentages.\nThe hovertemplate enriches the points with interactive data display on hover, showing the flight count and delay proportion for each airport. The function returns a Plotly figure object configured with a title and a stylized geographical layout, ready to be rendered in the app.\n\ndef create_composite_map():\n    \"\"\"\n    Generates a heatmap Plotly figure displaying U.S. airport flight delays.\n\n    Size of points reflects flight count; color indicates delay proportions.\n\n    Returns:\n    - Plotly Figure: A map with scaled markers for visualizing flight delays.\n    \"\"\"\n\n    fig = go.Figure(data=go.Scattergeo(\n        lon = dep_delay['ORIGIN_LONGITUDE'],\n        lat = dep_delay['ORIGIN_LATITUDE'],\n        text = dep_delay['ORIGIN'],\n        customdata = dep_delay[['flight_count', 'DEP_DEL15']],  # Add flight count and delay proportions to the custom data\n        hovertemplate = (\n            \"&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;\"\n            \"Flight Count: %{customdata[0]}&lt;br&gt;\"\n            \"Delay Proportion: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"  # Format delay proportion to show two decimal places\n        ),\n        marker = dict(\n            size = dep_delay['DEP_DEL15'] * 50,  # Scale the points based on delay proportion\n            color = dep_delay['DEP_DEL15'],\n            colorscale = 'Viridis',\n            showscale = True,\n            colorbar_title = 'Delay Proportion'\n        )\n    ))\n\n    fig.update_layout(\n        title = 'Heatmap of Flight Delay Proportions',\n        geo = dict(\n            scope = 'usa',\n            projection_type = 'albers usa',\n            showland = True,\n            landcolor = 'rgb(217, 217, 217)',\n            subunitcolor = \"rgb(255, 255, 255)\"\n        )\n    )\n\n\n\n    return fig\n\n\n\nRush Hour:\nThis app offers insights into the frequency and peak hours of flight departures from specific airports.\nBy inputting an airport code and a flight’s departure time, users can generate a bar chart that reveals the airport’s busiest periods, aiding in understanding rush hour trends.\n\n# @title Rush Hour\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path2 = '/content/drive/MyDrive/PIC16B_Datasets/rush_hour.png'\n\n# Display the image\ndisplay(Image(filename=image_path2))\n\n\n\n\n\n\n\n\nCallback: update_hourly_activity\nThis callback updates the histogram visualization for hourly flight activity based on user interaction. It triggers when the user selects a visualization mode and clicks the ‘Update’ button. The callback receives the airport code and hour input by the user and passes this information to the update_hourly_activity function to refresh the histogram display.\n\n@app.callback(\n    Output('hist', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('update-button', 'n_clicks')],\n    [State('origin-input', 'value'), State('hour-input', 'value')]\n)\n\nFunction: update_hourly_activity\nThis function creates a histogram to display the number of flights for each hour from a specified airport. It takes user inputs for the airport and hour, converts the airport code to uppercase, filters the dataset for the selected airport, and constructs a bar chart. If an hour is provided, it highlights that hour on the chart. The function returns a Plotly figure object with the updated histogram for rendering in the app.\n\ndef update_hourly_activity(n_clicks, vis_mode, selected_origin, selected_hour):\n    \"\"\"\n    Generates a histogram figure of flight counts for each hour of the day based on user-selected origin and hour.\n\n    This function filters the data for the specified airport origin and hour, then produces a bar plot showing the\n    number of flights departing at each hour of the day. If an hour is selected, it highlights that hour on the histogram.\n\n    Parameters:\n    - n_clicks (int): Number of clicks received. This parameter can be used to trigger the function in a callback.\n    - vis_mode (str): The visualization mode. Currently unused in the function but can be utilized for future modes.\n    - selected_origin (str): The airport origin code selected by the user.\n    - selected_hour (int/str): The hour selected by the user for highlighting in the histogram.\n\n    Returns:\n    - fig (plotly.graph_objs._figure.Figure): A Plotly figure object containing the histogram of hourly flight activity.\n    \"\"\"\n\n    if selected_origin is not None:\n        selected_origin = selected_origin.upper()\n\n    # Otherwise, generate the histogram for the selected origin and hour\n    filtered_df = dep_count[dep_count['ORIGIN'] == selected_origin]\n\n    # Create a barplot of flights by hour\n    # First, we create the text that will be displayed on each bar\n    filtered_df['text'] = 'Airport: ' + filtered_df['ORIGIN'] \\\n                      + '&lt;br&gt;Hour: ' + filtered_df['dep_hour'].astype(str) \\\n                      + '&lt;br&gt;Flights: ' + filtered_df['dep_count'].astype(str)\n\n    # Now we can create the bar plot\n    fig = px.bar(filtered_df, x='dep_hour', y='dep_count', title='Hourly Flight Activity')\n\n    fig.update_layout(xaxis_title='Departure Hour', yaxis_title='Numer of Flights')\n    # To add hover text, you can use the hover_data parameter\n    fig.update_traces(hovertemplate=filtered_df['text'])\n\n    # Highlight the selected hour if one is selected\n\n    try:\n        selected_hour = int(selected_hour)\n        if 0 &lt;= selected_hour &lt;= 23:\n            fig.add_vline(x=selected_hour, line_color=\"red\", annotation_text=\"Selected Hour\")\n    except (ValueError, TypeError):\n        pass\n\n    return fig"
  },
  {
    "objectID": "posts/FinalProjectWriteUp/index.html#conclusions",
    "href": "posts/FinalProjectWriteUp/index.html#conclusions",
    "title": "Final Project Write-Up: Flight Delay Predictions",
    "section": "6. Conclusions",
    "text": "6. Conclusions\nIn conclusion, we created a Flask web app that helps users predict whether or not their flight will be delayed. Our project aims to predict flight delays, offering substantial benefits to travelers and airports by providing more accurate departure times and enhancing planning efficiency. However, it faces challenges such as data quality and availability, computational limitations, and the complexity of developing predictive models. Mitigating these risks requires thorough data assessment, scalable processing solutions, and flexible project scope management. Ethically, while the project presents an opportunity to improve the travel experience for passengers and operational efficiency for airports, it may also highlight airlines’ operational shortcomings, potentially impacting their reputation. Despite these considerations, the project stands to make a positive impact by enabling better-informed travel decisions and streamlining airport traffic management, contributing to a more predictable and less stressful travel experience for all involved. As far as any ethical concerns, there are very few that we have been able to identify. The only such concern may be that the airlines would not be happy with their constant delays being exposed."
  },
  {
    "objectID": "posts/Homework1/HW1.html",
    "href": "posts/Homework1/HW1.html",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "",
    "text": "Before beginning the coding for the assignment, I loaded in all the necessary packages. The packages are loaded in below, and have a short description of what they were used for in the assignment.\n\n\n# used to read in data as dataframes and in each function\nimport pandas as pd\n\n# used in part 1 to create the database and in the query functions\nimport sqlite3\n\n# used in the temperature_coefficient_plot function\nimport numpy as np\n\n# used for visualizations\nfrom plotly import express as px\n\n# used to obtain coefficients in the temperature_coefficient_plot function\nfrom sklearn.linear_model import LinearRegression\n\n# used to properly show figures in the quarto blog\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/Homework1/HW1.html#stations-dataset",
    "href": "posts/Homework1/HW1.html#stations-dataset",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Stations Dataset:",
    "text": "Stations Dataset:\n\nThis is a dataset that contains the ID, latitude, longitude, elevation, and name of each station that we will be looking at.\nWe are particularly interested in the longitude and latitude columns of this datast, which can be used to create visualizations where the locations of the stations are accurately represented on a world map.\nThe data is read in below along with a preview of what it looks like.\n\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR"
  },
  {
    "objectID": "posts/Homework1/HW1.html#temperatures-dataset",
    "href": "posts/Homework1/HW1.html#temperatures-dataset",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Temperatures Dataset:",
    "text": "Temperatures Dataset:\n\nThis is a dataset containing the ID, year, and temperature at the 12 months during the year for each station.\nWe are especially interested in looking at the temperature fluctuations at each station, so this dataset will be very useful for that.\nThe dataset is read in below along with a preview of what it looks like.\n\n\ntemps = pd.read_csv(\"temps.csv\")\ntemps.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\nLooking at the temperatures dataset above, it is clear that it will not be very easy to work with if we want to look at temperatures by month and year because years and months are all in different columns.\nWe will solve this issue by creating function to prepare the temps dataset into a more suitable form for our database.\nThis function, created in lecture, is defined below and is called on the temps dataset. A preview of the new dataset that we will use for database creation is also shown.\n\n\ndef prepare_df(df):\n    '''\n    Returns a temperature dataframe in a more suitable format for analysis.\n    \n    Arguments:\n        df (Pandas dataframe): the temps dataframe shown above\n    \n    Returns:\n        df (Pandas dataframe): a temps dataframe with columns ID, Year, Month, and Temp\n    '''\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\ntemps = prepare_df(temps)\ntemps.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28"
  },
  {
    "objectID": "posts/Homework1/HW1.html#countries-dataset",
    "href": "posts/Homework1/HW1.html#countries-dataset",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Countries Dataset:",
    "text": "Countries Dataset:\n\nThis is a dataset containing the names of every country along with their 2-letter abbreviations.\nThis will be used to determine which stations are in which country using the ID columns in the stations and temperatures datasets.\nThe data is read in below along with a preview of what the data looks like.\n\n\ncountries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa"
  },
  {
    "objectID": "posts/Homework1/HW1.html#we-now-create-the-database-with-one-table-for-each-of-the-three-datasets-that-we-have.",
    "href": "posts/Homework1/HW1.html#we-now-create-the-database-with-one-table-for-each-of-the-three-datasets-that-we-have.",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "We now create the database with one table for each of the three datasets that we have.",
    "text": "We now create the database with one table for each of the three datasets that we have.\n\nWe start by establishing a connection to a file “hw1.db” which is the main file for the database.\nWe then add each dataset that we read in above into the database using the to_sql method.\nThe connection to the database is then closed.\n\n\nconn = sqlite3.connect(\"hw1.db\")\n\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\ntemps.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n    \nconn.close()"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-1",
    "href": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-1",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing temperature_coefficient_plot() Function (Part 1):",
    "text": "Testing temperature_coefficient_plot() Function (Part 1):\n\nWe will now test the function, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), country = “India”, year_begin = 1980, year_end = 2020, month = 1, min_obs = 10, zoom = 2, mapbox_style = “carto-positron”, color_continuous_scale = px.colors.diverging.RdGy_r.\nThis will give us a visualization of the changes in station temperature recordings from 1980 to 2020 in India with specifications of the plot being zoom, mapbox style, and color continuous scale.\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"hw1.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\nFrom the plot above, we can see that there are 8 to 12 points that are noticably dark, indicating a high change in temperature coefficient in years 1980-2020 in India. This is alarming because it could be indicative of climate issues in those areas."
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-2",
    "href": "posts/Homework1/HW1.html#testing-temperature_coefficient_plot-function-part-2",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing temperature_coefficient_plot() Function (Part 2):",
    "text": "Testing temperature_coefficient_plot() Function (Part 2):\n\nWe will now test the function again, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), country = “United States”, year_begin = 2010, year_end = 2020, month = 1, min_obs = 20, zoom = 2, mapbox_style = “carto-positron”, color_continuous_scale = px.colors.diverging.RdGy_r.\nThis will give us a visualization of the changes in station temperature recordings from 2010 to 2020 in the United States with specifications of the plot being zoom, mapbox style, and color continuous scale.\n\n\nfig2 = temperature_coefficient_plot(\"hw1.db\", \"United States\", 2010, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig2.show()\n\n\n\n\n\nFrom the plot above, we can see that there are at least 4 points that are extremely dark, indicating a high change in temperature coefficient in years 2010-2020 in the United States. Overall, this is not too bad becasue thare are many more dots that are not dark color, but it should raise alarm for those areas."
  },
  {
    "objectID": "posts/Homework1/HW1.html#question-1-how-does-a-countrys-latitude-affect-its-average-monthly-temperature-in-a-specific-month-and-year",
    "href": "posts/Homework1/HW1.html#question-1-how-does-a-countrys-latitude-affect-its-average-monthly-temperature-in-a-specific-month-and-year",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Question 1: How does a country’s latitude affect its average monthly temperature in a specific month and year?",
    "text": "Question 1: How does a country’s latitude affect its average monthly temperature in a specific month and year?"
  },
  {
    "objectID": "posts/Homework1/HW1.html#query_climate_database2-function",
    "href": "posts/Homework1/HW1.html#query_climate_database2-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "query_climate_database2() Function:",
    "text": "query_climate_database2() Function:\n\nWe create a function called query_climate_database2() with the following specifications:\nArguments:\n\n\ndb_file: a file of a sql database\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\nmin_latitude: the minimum latitude of stations for which should be returned\nmax_latitude: the maximum latitude of stations for which should be returned\n\n\nReturns: a dataframe corresponding to the above specifications, with columns:\n\n\nNAME: the station name\nLATITUDE: the latitude of the station\nCountry: the name of the country where the station is located\nYear: the year when the temperature reading was taken\nMonth: the month when the temperature reading was taken\nTemp: the average temperature at the specified station during the specified year and month\n\n\nThe code for the query_climate_database2() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import query_climate_database2\nprint(inspect.getsource(query_climate_database2))\n\ndef query_climate_database2(db_file, year, month, min_latitude, max_latitude):\n    '''\n    Returns a Pandas dataframe of temperature readings in the specified month of the specified year\n    between the minimum and maximum specified latitudes. This data comes from a SQL database.\n    \n    Arguments:\n        db_file (str): a file of a sql database\n        year (int): the year that you want to observe data from\n        month (str): an integer for the month of the year which the data should be returned\n        min_latitude (dbl): the minimum latitude of stations for which should be returned\n        max_latitude (dbl): the maximum latitude of stations for which should be returned\n        \n    Returns: a dataframe corresponding to the above specifications, with columns:\n        NAME (str): the station name\n        LATITUDE (dbl): the latitude of the station\n        Country (str): the name of the country where the station is located\n        Year (int): the year when the temperature reading was taken\n        Month (int): the month when the temperature reading was taken\n        Temp (dbl): the average temperature at the specified station during the specified year and month\n    '''\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT \n            stations.NAME, stations.LATITUDE, countries.Name \"Country\", temperatures.Year, temperatures.Month, temperatures.Temp\n        FROM \n            stations\n        INNER JOIN \n            countries ON SUBSTRING(stations.ID, 1, 2) = countries.[FIPS 10-4]\n        JOIN\n            temperatures ON stations.ID = temperatures.ID\n        WHERE\n            temperatures.Year = {year}\n            AND\n            temperatures.Month = {month}\n            AND\n            stations.LATITUDE BETWEEN {min_latitude} AND {max_latitude}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    \n    return df"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-query_climate_database2-function",
    "href": "posts/Homework1/HW1.html#testing-query_climate_database2-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing query_climate_database2() Function:",
    "text": "Testing query_climate_database2() Function:\n\nWe will now test the function, displaying the dataframe that we get using db_file = “hw1.db” (the database created above, year = 2003, month = 1, min_latitude = 0, max_latitude = 30.\nThis will give us a dataframe of all the database recordings in January of 2003 from stations with latitude between 0 and 30.\n\n\nquery_climate_database2(db_file = \"hw1.db\", \n                        year = 2003, \n                        month = 1, \n                        min_latitude = 0, \n                        max_latitude = 30)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nDUBAI_INTL\n25.2550\nUnited Arab Emirates\n2003\n1\n19.39\n\n\n1\nABU_DHABI_INTL\n24.4330\nUnited Arab Emirates\n2003\n1\n19.64\n\n\n2\nAL_AIN_INTL\n24.2620\nUnited Arab Emirates\n2003\n1\n18.45\n\n\n3\nIN_AMENAS\n28.0500\nAlgeria\n2003\n1\n11.95\n\n\n4\nTAMANRASSET\n22.8000\nAlgeria\n2003\n1\n14.20\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1115\nCHRISTIANSTED_AP\n17.7028\nVirgin Islands, U.S.\n2003\n1\n26.28\n\n\n1116\nCHARLOTTE_AMALIE_AP\n18.3331\nVirgin Islands, U.S.\n2003\n1\n26.36\n\n\n1117\nVILLA_CISNEROSMIL\n23.7000\nWestern Sahara\n2003\n1\n18.35\n\n\n1118\nDAKHLA\n23.7110\nWestern Sahara\n2003\n1\n17.53\n\n\n1119\nWAKE_ISLAND\n19.2833\nWake Island\n2003\n1\n26.16\n\n\n\n\n1120 rows × 6 columns"
  },
  {
    "objectID": "posts/Homework1/HW1.html#temperature_latitude_plot-function",
    "href": "posts/Homework1/HW1.html#temperature_latitude_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "temperature_latitude_plot() Function:",
    "text": "temperature_latitude_plot() Function:\n\nArguments:\n\n\ndb_file: a file of a sql database\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\nmin_latitude: the minimum latitude of stations for which should be returned\nmax_latitude: the maximum latitude of stations for which should be returned\n**kwargs: addition arguments to be used in plt.plot function\n\n\nReturns:\n\n\nAn interactive scatterplot with station latitude on the x-axis and station average temperature for the specified month and year on the y-axis. The points are colored by country.\n\n\nfrom climate_database import temperature_latitude_plot\nprint(inspect.getsource(temperature_latitude_plot))\n\ndef temperature_latitude_plot(db_file, year, month, min_latitude, max_latitude, **kwargs):\n    '''\n    Returns an interactive scatterplot of average temperature vs. station latitude for stations in a \n    specified latitude range and a specified month and year.\n    \n    Arguments:\n        db_file: a file of a sql database\n        year: the year that you want to observe data from\n        month: an integer for the month of the year which the data should be returned\n        min_latitude: the minimum latitude of stations for which should be returned\n        max_latitude: the maximum latitude of stations for which should be returned\n        **kwargs: addition arguments to be used in plt.plot function\n        \n    Returns: \n        An interactive scatterplot with station latitude on the x-axis and station \n        average temperature for the specified month and year on the y-axis. The points are \n        colored by country.\n    '''\n    df = query_climate_database2(db_file, year, month, min_latitude, max_latitude)\n    \n    return px.scatter(data_frame = df, x = \"LATITUDE\", y = \"Temp\", \n                      color = \"Country\",\n                      hover_name = \"NAME\", \n                      title = f\"Station Average Temperature by Latitude in Month {month} of {year}\",\n                      labels = {\"LATITUDE\":\"Latitude\", \"Temp\":\"Average Temperature\"},\n                      **kwargs)"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-temperature_latitude_plot-function",
    "href": "posts/Homework1/HW1.html#testing-temperature_latitude_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing temperature_latitude_plot() Function:",
    "text": "Testing temperature_latitude_plot() Function:\n\nWe will now test the function, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), year = 2003, month = 1, min_latitude = 0, max_latitude = 30, size = “Year”, size_max = 7.\nThis will give us a scatterplot of the station temperature recordings in stations with latitudes between 0 and 30 vs the latitude of the station. The points will be colored by country. The specifications of the scatterplot plot are marker size (I made the marker size year so they would all be the same size) and max marker size (7).\n\n\ntemperature_latitude_plot(db_file = \"hw1.db\", \n                          year = 2003, \n                          month = 1, \n                          min_latitude = 0, \n                          max_latitude = 30, \n                          size = \"Year\", \n                          size_max = 7)\n\n\n\n\n\nThis visualization makes it apparent that the average temperature in January of 2003 decreases as we move up in latitude. This makes sense intuitively because a greater latitude means a further distance from the equator, so we would expect temperatures to decrease as we move away from the equator.\nHowever, this relationship shows a lot of variation, which indicates that distance from the equator is not the only factor in the increasing temperatures."
  },
  {
    "objectID": "posts/Homework1/HW1.html#question-2-can-we-compare-the-average-temperature-recorded-at-stations-in-different-selected-countries-in-a-given-year",
    "href": "posts/Homework1/HW1.html#question-2-can-we-compare-the-average-temperature-recorded-at-stations-in-different-selected-countries-in-a-given-year",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Question 2: Can we compare the average temperature recorded at stations in different selected countries in a given year?",
    "text": "Question 2: Can we compare the average temperature recorded at stations in different selected countries in a given year?"
  },
  {
    "objectID": "posts/Homework1/HW1.html#query_climate_database3-function",
    "href": "posts/Homework1/HW1.html#query_climate_database3-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "query_climate_database3() Function:",
    "text": "query_climate_database3() Function:\n\nWe create a function called query_climate_database3() with the following specifications:\nArguments:\n\n\ndb_file: a file of a sql database\ncountries: a list of countries that will be compared\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\n\n\nReturns: a dataframe corresponding to the above specifications, with columns:\n\n\nNAME: the station name\nCountry: the name of the country where the station is located\nYear: the year when the temperature reading was taken\nMonth: the month when the temperature reading was taken\nTemp: the average temperature at the specified station during the specified year and month\n\n\nThe code for the query_climate_database3() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import query_climate_database3\nprint(inspect.getsource(query_climate_database3))\n\ndef query_climate_database3(db_file, countries, year):\n    '''\n    Returns a Pandas dataframe of temperature readings in the specified year\n    in specified countries. This data comes from a SQL database.\n    \n    Arguments:\n        db_file (str): a file of a sql database\n        countries (list): a list of strings representing country names\n        year (int): the year that you want to observe data from\n        \n    Returns: a dataframe corresponding to the above specifications, with columns:\n        NAME (str): the station name\n        Country (str): the name of the country where the station is located\n        Year (int): the year when the temperature reading was taken\n        Month (int): the month when the temperature reading was taken\n        Temp (dbl): the average temperature at the specified station during the specified year and month\n    '''\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT \n            stations.NAME, countries.Name \"Country\", temperatures.Year, temperatures.Month, temperatures.Temp\n        FROM \n            stations\n        INNER JOIN \n            countries ON SUBSTRING(stations.ID, 1, 2) = countries.[FIPS 10-4]\n        JOIN\n            temperatures ON stations.ID = temperatures.ID\n        WHERE \n            countries.Name IN {prepare_list_for_sql(countries)}\n            AND \n            temperatures.Year = {year}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    \n    return(df)"
  },
  {
    "objectID": "posts/Homework1/HW1.html#prepare_list_for_sql-helper-function",
    "href": "posts/Homework1/HW1.html#prepare_list_for_sql-helper-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "prepare_list_for_sql() Helper Function:",
    "text": "prepare_list_for_sql() Helper Function:\n\nLooking at the code for the query_climate_database3() function above, we had to apply a function to the countries list that is inputted to allow it to be used the the query. The specifics of that function are outlined here.\nArgument:\n\n\ncharacter_list: a list of country names\n\n\nReturns:\n\n\nA string of the list elements enclosed in parentheses and separated by commas, which is suitable for usage in a sql query.\n\n\nThe code for the prepare_list_for_sql() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import prepare_list_for_sql\nprint(inspect.getsource(prepare_list_for_sql))\n\ndef prepare_list_for_sql(character_list):\n    '''\n    Returns a list that is acceptable for subsetting in a SQL query.\n    \n    Arguments:\n        character_list (list): a list of strings representing country names\n        \n    Returns:\n        A list format that is acceptable for subsetting in a SQL query.\n    '''\n    return f\"\"\"('{\"', '\".join(character_list)}')\"\"\""
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-prepare_list_for_sql-helper-function",
    "href": "posts/Homework1/HW1.html#testing-prepare_list_for_sql-helper-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing prepare_list_for_sql() Helper Function:",
    "text": "Testing prepare_list_for_sql() Helper Function:\n\nWe will now test the function, displaying the output that the function gives for the list of countries [“Egypt”, “India”, “United States”].\n\n\nprepare_list_for_sql([\"Egypt\", \"India\", \"United States\"])\n\n\"('Egypt', 'India', 'United States')\"\n\n\n\nThis form is suitable for the IN function in a sql query."
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-query_climate_database3-function",
    "href": "posts/Homework1/HW1.html#testing-query_climate_database3-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing query_climate_database3() Function:",
    "text": "Testing query_climate_database3() Function:\n\nWe will now test the function, displaying the dataframe that we get using db_file = “hw1.db” (the database created above, year = 2003, and countries = [“Egypt”, “India”, “United States”].\nThis will give us a dataframe of all the database temeperature recordings in 2003 from Egypt, India, and the United States.\n\n\nquery_climate_database3(db_file = \"hw1.db\", \n                        year = 2003, \n                        countries = [\"Egypt\", \"India\", \"United States\"])\n\n\n\n\n\n\n\n\nNAME\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nMERSA_MATRUH\nEgypt\n2003\n1\n15.50\n\n\n1\nMERSA_MATRUH\nEgypt\n2003\n2\n12.53\n\n\n2\nMERSA_MATRUH\nEgypt\n2003\n3\n13.50\n\n\n3\nMERSA_MATRUH\nEgypt\n2003\n4\n17.30\n\n\n4\nMERSA_MATRUH\nEgypt\n2003\n5\n21.30\n\n\n...\n...\n...\n...\n...\n...\n\n\n83128\nLINCOLN_11_SW\nUnited States\n2003\n8\n25.01\n\n\n83129\nLINCOLN_11_SW\nUnited States\n2003\n9\n17.08\n\n\n83130\nLINCOLN_11_SW\nUnited States\n2003\n10\n13.31\n\n\n83131\nLINCOLN_11_SW\nUnited States\n2003\n11\n2.76\n\n\n83132\nLINCOLN_11_SW\nUnited States\n2003\n12\n-0.65\n\n\n\n\n83133 rows × 5 columns"
  },
  {
    "objectID": "posts/Homework1/HW1.html#countries_compare_temp_plot-function",
    "href": "posts/Homework1/HW1.html#countries_compare_temp_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "countries_compare_temp_plot() Function:",
    "text": "countries_compare_temp_plot() Function:\n\nThis plot will show multiple facets of the data. There will be a different line plot shown for each selected country.\nArguments:\n\n\ndb_file: a file of a sql database\ncountries: a list of countries that will be compared\nyear: the year that you want to observe data from\nmonth: an integer for the month of the year which the data should be returned\n\n\nReturns:\n\n\nAn array of interactive line plots, one for each country. These line plots show the change in average temperature throughout the selected year in the countries.\n\n\nThe code for the countries_compare_temp_plot() function is contained in a separate file called “climate_database.py”.\nWe load in the function and display the code below.\n\n\nfrom climate_database import countries_compare_temp_plot\nprint(inspect.getsource(countries_compare_temp_plot))\n\ndef countries_compare_temp_plot(db_file, countries, year, **kwargs):\n    '''\n    Returns multiple line plots, one for each country, that show the temperature in each month.\n    \n    Arguments:\n        db_file (str): a file of a sql database\n        countries (list): a list of countries that will be compared\n        year (int): the year that you want to observe data from\n        **kwargs: additional arguments for the px.line function\n        \n    Returns: \n        An array of line plots, one for each country. These line plots show the change in average temperature\n        throughout the countries.\n    '''\n    df = query_climate_database3(db_file, countries, year)\n    \n    # get the median temperature recording for each month in each country\n    df_temp = df.groupby([\"Country\", \"Month\"])[\"Temp\"].median()\n    \n    # create a dataframe that can be used in the px.line function\n    df_temp = pd.DataFrame({\"Country\":[ele for ele in countries for i in range(12)], \n                            \"Month\":[1,2,3,4,5,6,7,8,9,10,11,12] * len(countries), \n                            \"Avg Temp\":df_temp.values})\n    \n    return px.line(df_temp, x=\"Month\", y=\"Avg Temp\", color=\"Country\",\n                   facet_col=\"Country\",\n                   title=\"Chaining Multiple Figure Operations With A Plotly Express Figure\", \n                   hover_data = \"Avg Temp\",\n                   **kwargs)"
  },
  {
    "objectID": "posts/Homework1/HW1.html#testing-countries_compare_temp_plot-function",
    "href": "posts/Homework1/HW1.html#testing-countries_compare_temp_plot-function",
    "title": "Analysis of Historic Temperature Data for Different Countries",
    "section": "Testing countries_compare_temp_plot() Function:",
    "text": "Testing countries_compare_temp_plot() Function:\n\nWe will now test the function, displaying the visualization that the function gives using db_file = “hw1.db” (the database created above), year = 2020, month = 1, opacity = 0.8, color = “Country”.\nThis will give us a barplot with each bar representing a country and the heights of the bars representing the average temperature in that country in January of 2020. The specifications of the scatterplot plot are bar opacity (0.8) and bar color (different for each country).\n\n\nfig = countries_compare_temp_plot(db_file = \"hw1.db\", \n                                  countries = [\"India\", \"Egypt\", \"United States\"], \n                                  year = 2020, \n                                  markers = True)\nfig.show()\n\n\n\n\n\nThe function works as intended, clearly displaying the difference in average temperatures in 2020 between Egypt, India, and the United States.\nIt is apparent that both India and Egypt were significantly hotter than the United States in every month of 2020.\nIt is also interesting to note that India shows a strange drop in temperature in August and Egypt shows a strange drop in temperature in July, whereas the United States temperatures show a clear bell-shapes pattern throughout the year.\nComparisons such as there can be very useful when considering differences in climate between different countries.\nThis plot shows multiple facets of the data because the year and country are subsetted in the plots."
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2a-class-initialization",
    "href": "posts/Homework2/HW2.html#step-2a-class-initialization",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(a): Class Initialization",
    "text": "Step 2(a): Class Initialization\n\nThe first code that we write in this file is the class initialization, which is shown below.\n\n\nimport scrapy\nclass TmdbSpider(scrapy.Spider):\n    '''\n    This class is for a spider that scrapes a movie website from TMDB and finds other \n    movies and TV shows that have similar actors to the selected movie.\n    '''\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n\nAs we can see, the class is initialized with global variable name=‘tmdb_spider’. The variables subdir, *args, and **kwargs are passed into the initialization of the class. The subdir instance variable is the subdirectory of the movie that we want to scrape data from on the TMDB website. We then initialize the instance variable start_urls which contains the url of the movie that we want to scrape data from."
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2b-the-parse-method",
    "href": "posts/Homework2/HW2.html#step-2b-the-parse-method",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(b): The parse() Method",
    "text": "Step 2(b): The parse() Method\n\nWe now move on to creating the parsing methods for the scraper.\nThe first of these methods is the parse method. This method assumes that we are on the selected movie’s TMDB page and navigates the spider to the Full Cast & Crew page. Once on this page, we call the parse_full_credits method, which will be defined later.\nThe code for the parse method is below.\n\n\ndef parse(self, response):\n    '''\n    Yields the cast page of the selected movie and calls the parse_full_credits \n    method.\n    '''  \n    # go the the cast page of the selected movie and call the parse_full_credits method\n    cast_url = f\"{self.start_urls[0]}cast\"\n    yield scrapy.Request(cast_url, callback = self.parse_full_credits)"
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2c-the-parse_full_credits-method",
    "href": "posts/Homework2/HW2.html#step-2c-the-parse_full_credits-method",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(c): The parse_full_credits() Method",
    "text": "Step 2(c): The parse_full_credits() Method\n\nThe parse_full_credits method assumes that we are on the selected movie’s Full Cast & Crew page on TMDB. It yields a scrapy request for each actor listed on the page (crew members are excluded). When each actor’s request is yielded, we call the parse_actor_page method, which will be defined later.\nTo accomplish this, we must first find a list of all the actor’s URLs on the page and then loop through this list and yield the URL to each actor’s page on TMDB.\nThe code for this method is below.\n\n\ndef parse_full_credits(self, response):\n    '''\n    Starts at the cast page for the selected movie and yields requests for the \n    TMDB page of each actor on the cast page (excluding crew members), calling \n    the parse_actor_page method.\n    '''\n    # get all the links for the actors on the cast page of the movie\n    actors_css = 'ol.people.credits:not(.crew) li div.info a::attr(href)'\n    actors = response.css(actors_css).extract()\n    \n    # loop through the actor links on the page, excluding crew members\n    for actor_link in actors:\n        \n        # join the actor url with the response url\n        url = response.urljoin(actor_link) \n        # create a request using the above url and call the parse_actor_page method\n        yield scrapy.Request(url, callback = self.parse_actor_page)"
  },
  {
    "objectID": "posts/Homework2/HW2.html#step-2d-the-parse_actor_page-method",
    "href": "posts/Homework2/HW2.html#step-2d-the-parse_actor_page-method",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Step 2(d): The parse_actor_page() Method",
    "text": "Step 2(d): The parse_actor_page() Method\n\nThe parse_actor_page method assumes that we are on the TMDB page for an actor. The method yields a dictionary of the form {“actor” : actor_name, “movie_or_TV_name” : movie_or_TV_name} for each acting role that the actor has had in their career (non-acting roles are excluded).\nTo accomplish this, we first find the actor’s name on the actor’s TMDB page. We then need to loop through the different credits tables on the page and find the one that contains only the acting roles. We then create a selector for this table, loop through the entries of the table, and yield a dictionary of the actor’s name and the role for each role in the table.\nThe code for this method is below.\n\n\ndef parse_actor_page(self, response):\n    '''\n    Starts on the TMDB page of an actor and yields a dictionary containing the \n    actor's name and the movie or TV show for each of their acting roles.\n    '''\n    # get the actor's name from their page\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # loop through the types of credit lists on the page\n    for item in response.css('div.credits_list h3'):\n        # if the credit list is for acting\n        if 'Acting' in item.xpath('./text()').get():\n            # get the table of acting roles\n            acting_table = item.xpath('following-sibling::table[1]').get()\n                \n    # create a selector for the table of acting roles\n    acting_selector = scrapy.Selector(text = acting_table)\n        \n    # loop through the table of acting roles\n    for acting in acting_selector.css('table.credit_group tr'):\n        # get the movie or TV show name from the current acting role\n        movie_or_TV_name = acting.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\n\nWe now have a fully functional web spraper that yields various dictionaries for each acting role of each actor in the selected movie.\n\nIMPORTANT NOTE: the methods parse, parse_full_credits, and parse_actor_page are defined within the TmdbSpider class initialized in part 2(a)"
  },
  {
    "objectID": "posts/Homework2/HW2.html#test-1-harry-potter-and-the-philosophers-stone",
    "href": "posts/Homework2/HW2.html#test-1-harry-potter-and-the-philosophers-stone",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Test 1: Harry Potter and the Philosopher’s Stone",
    "text": "Test 1: Harry Potter and the Philosopher’s Stone\n\nThe goal of this test is to create a .csv file that contains all of the {actor:role} pairs that are yielded by our TmdbSpider class for the movie Harry Potter and the Philosopher’s Stone.\nThis was accomplished by running the follwing line in the terminal, while still in the same TMDB_scraper directory as in step 1.\n\nscrapy crawl tmdb_spider -o results1.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nAfter this line is run, a results1.csv file will be created in the TMDB_scraper folder.\nWe need to make sure this file is of the correct format, which is done below.\n\n\nimport pandas as pd\n\nresults1 = pd.read_csv(\"results1.csv\")\nresults1\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2952\nJames Phelps\nKingdom\n\n\n2953\nJames Phelps\nHarry Potter and the Goblet of Fire\n\n\n2954\nJames Phelps\nHarry Potter and the Prisoner of Azkaban\n\n\n2955\nJames Phelps\nHarry Potter and the Chamber of Secrets\n\n\n2956\nJames Phelps\nHarry Potter and the Philosopher's Stone\n\n\n\n\n2957 rows × 2 columns\n\n\n\n\nWe can se that the results1.csv file is in the correct format, with one column for actor name and one column for the movie or TV show they acted in. There are 2957 observations.\nWe will now check is our dataset contains only the actors in the movie (i.e. it excludes the crew members).\n\n\nresults1['actor'].nunique()\n\n63\n\n\n\nHere, we see that there are 63 total actors in the dataset. When viewing the TMDB webpage for the movie, there are exactly 63 actors in the movie, so our scraper successfully weeded out the crew members and only included the actors.\nWe will now check that each actor is only in the dataset for their acting roles (not their roles in production, directing, etc.). We use Daniel Radcliffe as our test.\n\n\nresults1.groupby('actor').size()['Daniel Radcliffe']\n\n97\n\n\n\nWe can see here that the actor Daniel Radcliffe is found 97 times in the dataset, meaning that he should have 97 acting roles in his career. When viewing the TMD webpage for Daniel Radcliffe, he does in fact have exactly 97 acting roles, so our scraper has successfully weeded out non-acting roles for each actor.\nOur scraper has worked as expected, so our test is successul."
  },
  {
    "objectID": "posts/Homework2/HW2.html#test-2-good-will-hunting",
    "href": "posts/Homework2/HW2.html#test-2-good-will-hunting",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Test 2: Good Will Hunting",
    "text": "Test 2: Good Will Hunting\n\nWe will now test the scraper on my favorite movie, Good Will Hunting.\nThe goal of this test is to create a .csv file that contains all of the {actor:role} pairs that are yielded by our TmdbSpider class for the movie Good Will Hunting.\nThis was accomplished by running the follwing line in the terminal, while still in the same TMDB_scraper directory as in step 1.\n\nscrapy crawl tmdb_spider -o results2.csv -a subdir=489-good-will-hunting\n\nAfter this line is run, a results2.csv file is created in the TMDB_scraper folder.\nWe just need to make sure this file is of the correct format, which is done below.\n\n\nresults2 = pd.read_csv(\"results2.csv\")\nresults2\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nChristian Harmony\nThe Doorway\n\n\n1\nChristian Harmony\nSex and the City\n\n\n2\nChristian Harmony\nGood Will Hunting\n\n\n3\nStephen L'Heureux\nGood Will Hunting\n\n\n4\nStephen L'Heureux\nScent of a Woman\n\n\n...\n...\n...\n\n\n1500\nCasey Affleck\nLIVE with Kelly and Mark\n\n\n1501\nCasey Affleck\nLemon Sky\n\n\n1502\nCasey Affleck\nSaturday Night Live\n\n\n1503\nMatt Mercier\nGood Will Hunting\n\n\n1504\nRachel Majorowski\nGood Will Hunting\n\n\n\n\n1505 rows × 2 columns\n\n\n\n\nWe can se that the results2.csv file is in the correct format, with one column for actor name and one column for the movie or TV show they acted in. There are 1505 observations.\nWe will now check is our dataset contains only the actors in the movie (i.e it excludes the crew members).\n\n\nresults2['actor'].nunique()\n\n49\n\n\n\nHere, we see that there are 49 total actors in the dataset. When viewing the TMDB webpage for the movie, there are exactly 49 actors in the movie, so our scraper successfully weeded out the crew members and only included the actors.\nWe will now check that each actor is only in the dataset for their acting roles, and not their roles in production, directing, etc. We use Matt Damon as our test.\n\n\nresults2.groupby('actor').size()['Matt Damon']\n\n163\n\n\n\nWe can see here that the actor Matt Damon is found 163 times in the dataset, meaning that he should have 163 acting roles in his career. When viewing the TMD webpage for Matt Damon, he does in fact have exactly 163 acting roles, so our scraper has successfully weeded out non-acting roles for each actor.\nOur scraper has worked as expected, so our test is successul."
  },
  {
    "objectID": "posts/Homework2/HW2.html#recommendations-for-harry-potter-and-the-philosophers-stone",
    "href": "posts/Homework2/HW2.html#recommendations-for-harry-potter-and-the-philosophers-stone",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Recommendations for Harry Potter and the Philosopher’s Stone",
    "text": "Recommendations for Harry Potter and the Philosopher’s Stone\n\nUsing the results1.csv dataframe imported above, we create a dataframe with two columns “movie or TV name” and “number of shared actors” using the recommend_dataframe() function.\n\n\nrecommendations1 = recommend_dataframe(results1)\nrecommendations1\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n0\n(K)nox: The Rob Knox Story\n4\n\n\n1\n10 Days to War\n1\n\n\n2\n10 Rillington Place\n1\n\n\n3\n100 Years of Warner Bros.\n1\n\n\n4\n13Hrs\n1\n\n\n...\n...\n...\n\n\n2270\nYour Christmas or Mine 2\n1\n\n\n2271\nYour Christmas or Mine?\n1\n\n\n2272\nYour Ticket Is No Longer Valid\n1\n\n\n2273\nYub-Nub! The Forgotten Ewok Adventures\n1\n\n\n2274\nZastrozzi: A Romance\n1\n\n\n\n\n2275 rows × 2 columns\n\n\n\n\nWe can see that our function works in creating the dataframe with the two desired columns.\nWe will now sort by movies or tv shows that have more than 5 shared actors with Harry Potter and the Philosopher’s Stone.\n\n\nrecommendations1.loc[recommendations1[\"number of shared actors\"] &gt; 5,:]\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n116\nAn Audience with...\n6\n\n\n382\nCreating the World of Harry Potter\n36\n\n\n415\nDavid Holmes: The Boy Who Lived\n6\n\n\n455\nDoctor Who\n14\n\n\n693\nHarry Potter 20th Anniversary: Return to Hogwarts\n11\n\n\n694\nHarry Potter and the Chamber of Secrets\n37\n\n\n695\nHarry Potter and the Deathly Hallows: Part 1\n20\n\n\n696\nHarry Potter and the Deathly Hallows: Part 2\n23\n\n\n697\nHarry Potter and the Goblet of Fire\n19\n\n\n698\nHarry Potter and the Half-Blood Prince\n19\n\n\n699\nHarry Potter and the Order of the Phoenix\n24\n\n\n700\nHarry Potter and the Philosopher's Stone\n63\n\n\n701\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n1241\nPerformance\n7\n\n\n1839\nThe Magic Touch of Harry Potter\n10\n\n\n2050\nThe Wonderful World of Disney: Magical Holiday...\n11\n\n\n2101\nTony Awards\n6\n\n\n\n\n\n\n\n\nWe can see that there are 16 movies and TV shows that have more than 5 shared actors with Harry Potter and the Philosopher’s Stone.\nWe would reccomend these movies to people who enjoy Harry Potter and the Philosopher’s Stone because the actors are very similar (more than 5 matches).\nWe will now use the recommend_barplot() function to visualize the data presented above.\n\n\nrecommend_barplot(df = recommendations1, \n                  movie = \"Harry Potter and the Philosopher's Stone\", \n                  min_num_shared_actors = 5)\n\n\n\n\n\n\n\n\n\nFrom the bar plot above, we can see that the movies or TV shows that share the most actors with Harry Potter and the Philosopher’s Stone are “Harry Potter and the Chamber of Secrets” and “Creating the World of Harry Potter.” This makes sense because all of the Harry Potter movies share many actors."
  },
  {
    "objectID": "posts/Homework2/HW2.html#recommendations-for-good-will-hunting",
    "href": "posts/Homework2/HW2.html#recommendations-for-good-will-hunting",
    "title": "Web Scraper for Movie and TV Show Reccomendations",
    "section": "Recommendations for Good Will Hunting",
    "text": "Recommendations for Good Will Hunting\n\nUsing the results2.csv dataframe imported above, I will create a dataframe with two columns “movie or TV name” and “number of shared actors” using the recommend_dataframe() function.\n\n\nrecommendations2 = recommend_dataframe(results2)\nrecommendations2\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n0\n'Saving Private Ryan': Boot Camp\n1\n\n\n1\n...First Do No Harm\n1\n\n\n2\n10-8: Officers on Duty\n1\n\n\n3\n11 Colours of the Bird\n1\n\n\n4\n12 Dates of Christmas\n1\n\n\n...\n...\n...\n\n\n1275\nZerophilia\n1\n\n\n1276\neXistenZ\n1\n\n\n1277\nmid90s\n1\n\n\n1278\nÅke and His World\n1\n\n\n1279\n알아두면 쓸데없는 지구별 잡학사전\n1\n\n\n\n\n1280 rows × 2 columns\n\n\n\n\nWe can again see that our function works in creating the dataframe with the two desired columns.\nWe will now sort by movies or tv shows that have more than 3 shared actors with Good Will Hunting.\n\n\nrecommendations2.loc[recommendations2[\"number of shared actors\"] &gt; 3,:]\n\n\n\n\n\n\n\n\nmovie or TV name\nnumber of shared actors\n\n\n\n\n285\nDue South\n6\n\n\n316\nFX: The Series\n4\n\n\n394\nGood Will Hunting\n49\n\n\n512\nJimmy Kimmel Live!\n4\n\n\n541\nLIVE with Kelly and Mark\n4\n\n\n606\nMayday\n5\n\n\n803\nSaturday Night Live\n5\n\n\n812\nSchool Ties\n4\n\n\n885\nSue Thomas: F.B.Eye\n4\n\n\n888\nSuits\n4\n\n\n1003\nThe Graham Norton Show\n4\n\n\n1071\nThe Oscars\n4\n\n\n1131\nThe Tonight Show with Jay Leno\n4\n\n\n1140\nThe View\n4\n\n\n1228\nWar of the Worlds\n4\n\n\n\n\n\n\n\n\nWe can see that there are 14 movies and TV shows that have more than 3 shared actors with Good Will Hunting.\nI would reccomend these movies to people who enjoy Good Will Hunting because the actors are very similar (more than 3 matches).\nWe will now use the recommend_barplot() function to visualize the data presented above.\n\n\nrecommend_barplot(df = recommendations2, \n                  movie = \"Good Will Hunting\", \n                  min_num_shared_actors = 3)\n\n\n\n\n\n\n\n\n\nFrom the bar plot above, we can see that the movies or TV shows that share the most actors with Good Will Hunting are “Due South”, “Mayday”, and “Saturday Night Live”. We would recommend these shows the most to Good Will Hunting fans."
  },
  {
    "objectID": "posts/Homework4/HW4.html",
    "href": "posts/Homework4/HW4.html",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "",
    "text": "First, we import all packages necessary to complete the diffusion models.\n\n\n# create arrays to model the diffusion process\nimport numpy as np\n\n# visualize the arrays that are created\nfrom matplotlib import pyplot as plt\n\n# optimize the code, significantly decreasing run time\nimport jax\nfrom jax.experimental import sparse\nfrom jax import jit\nimport jax.numpy as jnp\n\n# display the code for functions created in a seperate .py file\nimport inspect\n\n\nWe will now set some global parameters for the diffusion model.\nN is the width and height of the grid that the model will be displayed on.\nepsilon is a small parameter that represents a time step in the discretized process.\n\n\nN = 101\nepsilon = 0.2\n\n\nThe diffusion model will begin with an initial numpy array containing heat centralized in the array.\nThe creation of this array is given in the function below along with a depiction of the initial state of the diffusion model.\n\n\ndef create_u0(N):\n    '''\n    Creates the initial array in our two-dimensional heat diffusion model.\n    Args:\n        N: an integer of how large the array should be\n    Returns:\n        u0: a N x N numpy array to represent the start of a 2D heat diffusion\n    '''\n    u0 = np.zeros((N, N))\n    u0[int(N/2), int(N/2)] = 1.0\n    return u0\n\nplt.imshow(create_u0(N))\n\n\n\n\n\n\n\n\n\nAfter creating the initial array, we want to model the diffusion through many (2700) time steps.\nWe also want to save every 300 steps of the process to visualize the differences as the heat diffusion progresses.\nThe function that will allow us to advance the simulation by 2700 steps is shown below.\n\n\ndef advance_2700(advance_fun, u0, **kwargs):\n    '''\n    Advances the heat diffusion process by 2700 steps, saving every 300 steps.\n    Args:\n        advance_fun: a function that will advance the diffusion process by 1 step\n        u0: a N x N numpy array that represents that start of the diffusion\n        **kwargs: additional arguments needed for the advance_fun function\n    Returns:\n        a list of the numpy array generated at every 300th step of the diffusion\n    '''\n    output = []\n    # for numbers 1 to 2700\n    for i in range(1, 2701):\n        \n        # advance the simulation by one step\n        u0 = advance_fun(u = u0, **kwargs)\n        \n        # if we are in one of the 300th steps\n        if i % 300 == 0:\n            # save the output in a list and print a message\n            output.append(u0)\n            print(f\"u{i} created\")\n    return output\n\n\nNow that we have a way to model 2700 diffusion steps, we want a way to visualize every 300th step of the process.\nThe function that accomplishes this is shown below.\n\n\ndef plot_every_300(u_list):\n    '''\n    Outputs a 3 by 3 visualization that shows every 300th step of a modeled \n    diffusion process with 2700 steps.\n    Args:\n        u_list: a list created by the advance_2700 function that contains every \n                300th step of the modeled diffusion process\n    Returns:\n        3 by 3 visualization that shows every 300th step of the modeled diffusion\n    '''\n    # create the figure\n    fig = plt.figure()\n    # for numbers 1 to 10\n    for i in range(1, 10):\n        \n        # add a subplot and display the (i*300)th step\n        fig.add_subplot(3, 3, i)\n        plt.imshow(u_list[i-1])\n        plt.axis(\"off\")\n        plt.title(f\"{i*300}th Iteration\")\n\n\nNow that we have a way to advance our simulation and plot our results, we will experiment with different advance_fun functions using different processes."
  },
  {
    "objectID": "posts/Homework4/HW4.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-4.-i-will-describe-my-modeling-of-a-two-dimensional-heat-diffusion-using-four-different-methods.",
    "href": "posts/Homework4/HW4.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-4.-i-will-describe-my-modeling-of-a-two-dimensional-heat-diffusion-using-four-different-methods.",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "",
    "text": "First, we import all packages necessary to complete the diffusion models.\n\n\n# create arrays to model the diffusion process\nimport numpy as np\n\n# visualize the arrays that are created\nfrom matplotlib import pyplot as plt\n\n# optimize the code, significantly decreasing run time\nimport jax\nfrom jax.experimental import sparse\nfrom jax import jit\nimport jax.numpy as jnp\n\n# display the code for functions created in a seperate .py file\nimport inspect\n\n\nWe will now set some global parameters for the diffusion model.\nN is the width and height of the grid that the model will be displayed on.\nepsilon is a small parameter that represents a time step in the discretized process.\n\n\nN = 101\nepsilon = 0.2\n\n\nThe diffusion model will begin with an initial numpy array containing heat centralized in the array.\nThe creation of this array is given in the function below along with a depiction of the initial state of the diffusion model.\n\n\ndef create_u0(N):\n    '''\n    Creates the initial array in our two-dimensional heat diffusion model.\n    Args:\n        N: an integer of how large the array should be\n    Returns:\n        u0: a N x N numpy array to represent the start of a 2D heat diffusion\n    '''\n    u0 = np.zeros((N, N))\n    u0[int(N/2), int(N/2)] = 1.0\n    return u0\n\nplt.imshow(create_u0(N))\n\n\n\n\n\n\n\n\n\nAfter creating the initial array, we want to model the diffusion through many (2700) time steps.\nWe also want to save every 300 steps of the process to visualize the differences as the heat diffusion progresses.\nThe function that will allow us to advance the simulation by 2700 steps is shown below.\n\n\ndef advance_2700(advance_fun, u0, **kwargs):\n    '''\n    Advances the heat diffusion process by 2700 steps, saving every 300 steps.\n    Args:\n        advance_fun: a function that will advance the diffusion process by 1 step\n        u0: a N x N numpy array that represents that start of the diffusion\n        **kwargs: additional arguments needed for the advance_fun function\n    Returns:\n        a list of the numpy array generated at every 300th step of the diffusion\n    '''\n    output = []\n    # for numbers 1 to 2700\n    for i in range(1, 2701):\n        \n        # advance the simulation by one step\n        u0 = advance_fun(u = u0, **kwargs)\n        \n        # if we are in one of the 300th steps\n        if i % 300 == 0:\n            # save the output in a list and print a message\n            output.append(u0)\n            print(f\"u{i} created\")\n    return output\n\n\nNow that we have a way to model 2700 diffusion steps, we want a way to visualize every 300th step of the process.\nThe function that accomplishes this is shown below.\n\n\ndef plot_every_300(u_list):\n    '''\n    Outputs a 3 by 3 visualization that shows every 300th step of a modeled \n    diffusion process with 2700 steps.\n    Args:\n        u_list: a list created by the advance_2700 function that contains every \n                300th step of the modeled diffusion process\n    Returns:\n        3 by 3 visualization that shows every 300th step of the modeled diffusion\n    '''\n    # create the figure\n    fig = plt.figure()\n    # for numbers 1 to 10\n    for i in range(1, 10):\n        \n        # add a subplot and display the (i*300)th step\n        fig.add_subplot(3, 3, i)\n        plt.imshow(u_list[i-1])\n        plt.axis(\"off\")\n        plt.title(f\"{i*300}th Iteration\")\n\n\nNow that we have a way to advance our simulation and plot our results, we will experiment with different advance_fun functions using different processes."
  },
  {
    "objectID": "posts/Homework4/HW4.html#speed",
    "href": "posts/Homework4/HW4.html#speed",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "Speed:",
    "text": "Speed:\n\nPart 1: 30.5 seconds\nPart 2: 0.984 seconds\nPart 3: 0.231 seconds\nPart 4: 0.082 seconds\nPart 2 is about 31 times faster than part 1.\nPart 3 is about 132 times faster than part 1 and about 4.3 times faster than part 2.\nPart 4 is about 372 times faster than part 1, about 12 times faster than part 3, and about 2.8 times faster than part 3.\nIn conclusion, part 4 is significantly preferred from a speed standpoint."
  },
  {
    "objectID": "posts/Homework4/HW4.html#ease-of-coding",
    "href": "posts/Homework4/HW4.html#ease-of-coding",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "Ease of Coding:",
    "text": "Ease of Coding:\n\nPart 1: This part is the hardest to code because it involves two functions and matrix multiplication.\nPart 2: This part was equally hard as part 1 to code because it also involves 2 functions and matrix multiplication.\nPart 3: This part was much easier than parts 1 and 2 to code because it was just following along with the heat diffusion equation.\nPart 4: This part was equally as difficult as part 3 to code because the logic was the exact same.\nIn conclusion, the functions that ran to fastest were actually much easier in terms of coding."
  },
  {
    "objectID": "posts/Homework4/HW4.html#part-4-is-preferred-due-to-its-speed-and-ease-of-coding.",
    "href": "posts/Homework4/HW4.html#part-4-is-preferred-due-to-its-speed-and-ease-of-coding.",
    "title": "Two-Dimensional Heat Diffusion Models",
    "section": "Part 4 is preferred due to its speed and ease of coding.",
    "text": "Part 4 is preferred due to its speed and ease of coding."
  },
  {
    "objectID": "posts/Homework3/HW3.html#github-link",
    "href": "posts/Homework3/HW3.html#github-link",
    "title": "Flask Website With User Inputs",
    "section": "Github Link:",
    "text": "Github Link:\nhttps://github.com/trentbellinger/PIC16B-HW3"
  },
  {
    "objectID": "posts/Homework3/HW3.html#blog-post-link",
    "href": "posts/Homework3/HW3.html#blog-post-link",
    "title": "Flask Website With User Inputs",
    "section": "Blog Post Link:",
    "text": "Blog Post Link:\nhttps://trentbellinger.github.io/PIC-16B/posts/Homework3/HW3.html"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-app.py-header",
    "href": "posts/Homework3/HW3.html#the-app.py-header",
    "title": "Flask Website With User Inputs",
    "section": "The app.py Header",
    "text": "The app.py Header\n\nThe first thing that is needed in the python file for the app is an appropriate header that imports the necessary packages/functions and initializes the app. This header is shown below:\n\n\nfrom flask import Flask, render_template, request, g\nimport sqlite3\napp = Flask(__name__)"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-get_message_db-function",
    "href": "posts/Homework3/HW3.html#the-get_message_db-function",
    "title": "Flask Website With User Inputs",
    "section": "The get_message_db() Function:",
    "text": "The get_message_db() Function:\n\nThen, we move on to the creation of the first function in the python file. This function will create a SQL database for the messages that are inputted in the website if none has been created already. If a SQL database already exists, it will simply return that database. The code for this function is shown below:\n\n\ndef get_message_db():\n    '''\n    Handles the creation of a SQL database for the messages that are presented \n    in the website.\n    '''\n    try:\n        return g.message_db\n    except:\n        # if a database is not present, we create one\n        g.message_db = sqlite3.connect(\"messages_db.db\")\n        \n        # if a messages does not exist, create table with columns for handle and text\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (message TEXT, handle TEXT)'\n        \n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        \n        return g.message_db"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-insert_message-function",
    "href": "posts/Homework3/HW3.html#the-insert_message-function",
    "title": "Flask Website With User Inputs",
    "section": "The insert_message() Function:",
    "text": "The insert_message() Function:\n\nWe will now create a function that takes in a user’s input in the form of a request and inserts the user’s handle into the SQL database that is created with the get_message_db() function created above. The code for this function is shown below:\n\n\ndef insert_message(request):\n    '''\n    Extracts the message and handle from a request and inserts them into the messages \n    database.\n    Arguments:\n        request: a request that the user inputs to the webpage\n    Returns:\n        the message and handle from the request\n    '''\n    message = request.form[\"message\"]\n    handle = request.form[\"handle\"]\n    \n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # insert the handle and message into the database\n    ins = f'INSERT INTO messages (message, handle) VALUES (\"{message}\", \"{handle}\")'\n    cursor.execute(ins)\n    # commit the changes and close the connection\n    db.commit()\n    db.close()"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-random_messages-function",
    "href": "posts/Homework3/HW3.html#the-random_messages-function",
    "title": "Flask Website With User Inputs",
    "section": "The random_messages() Function:",
    "text": "The random_messages() Function:\n\nNow that we have a way to insert the user inputs into the database, we will create a function that allows us to extract random messages from the database.\nThe function random_messages(), shown below, outputs a list of the handles and messages for the n random entries of the database that are selected. If the database has less than n entries, then all of the handles and messages will be returned.\n\n\ndef random_messages(n):\n    '''\n    Returns a collection of n random messages that have been previously inputted \n    into the app.\n    Arguments:\n        n (int): the number of random messages to return\n    Returns:\n        messages (list): contains the name and message for the n random messages\n    '''\n    # Get the database connection\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # select the handle and message for n random entries in the messages table\n    cursor.execute(f\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT {n}\")\n    # save the message (output[0]) and the handle (output[1]) in a list\n    messages = [[output[0], output[1]] for output in cursor.fetchall()]\n    \n    # close the db connection\n    db.close()\n\n    return messages"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-render_submit_template-function",
    "href": "posts/Homework3/HW3.html#the-render_submit_template-function",
    "title": "Flask Website With User Inputs",
    "section": "The render_submit_template() Function:",
    "text": "The render_submit_template() Function:\n\nWe now have all the helper function for our website complete, so we can move on to creating functions to display certain pages in the website. OUr website will have two pages.\nThe first page will be a page that allows a user to submit their handle and message. This function most have two cases: GET and POST.\nIn the POST case, the function will insert the user’s inputted message into the database (this functionality is tied to the “Submit message” button in the website) and then render the submit.html file.\nIn the GET case, the function will simply render the submit.html file.\nThe creation of the submit.html file will be outlined later.\nThe code for the first page of the app is shown below:\n\n\n@app.route('/', methods = ['POST', 'GET'])\ndef render_submit_template():\n    '''Renders the submit.html file in the app, accounting for POST and GET requests.'''\n    if request.method == 'POST':\n        # for a POST request, we insert the message into the database\n        insert_message(request)\n        \n        msg = \"Thanks for your submission!\"\n        # render submit.html with a thank you message\n        return render_template('submit.html', msg = msg)\n    else:\n        # for a GET request, simply render submit.html\n        return render_template('submit.html')"
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-view_random_messages-function",
    "href": "posts/Homework3/HW3.html#the-view_random_messages-function",
    "title": "Flask Website With User Inputs",
    "section": "The view_random_messages() Function:",
    "text": "The view_random_messages() Function:\n\nWe now want to create a page that allows users to see past submissions on the website.\nThis will be done by calling the random_messages() function created above and then rendering the view.html file with the outputtes messages.\nThe creation of the view.html file will be outliner later.\nThis code is shown below.\n\n\n@app.route('/view')\ndef view_random_messages():\n    '''\n    Creates a /view page of the app that displays 4 random messages that have been \n    previously inputted along with the name/handle of the person who submitted them.\n    '''\n    # get 4 random messages\n    messages = random_messages(4)\n    # render view.html with the 4 random messages\n    return render_template('view.html', messages = messages)"
  },
  {
    "objectID": "posts/Homework3/HW3.html#allowing-the-app-to-be-run-locally",
    "href": "posts/Homework3/HW3.html#allowing-the-app-to-be-run-locally",
    "title": "Flask Website With User Inputs",
    "section": "Allowing the App to be Run Locally:",
    "text": "Allowing the App to be Run Locally:\n\nFinally, we need a place to run the app. The following function allows the app to be run on a local device by simply running the python file in the terminal.\n\n\nif __name__ == '__main__':\n    '''\n    How to run the app. I had to use port=4999 for it to run on my \n    device (5000 was not working).\n    '''\n    app.run(host='0.0.0.0', port=4999, debug=True)\n\n\nThis concludes the creation of the app.py file. We will now move on the the creation of the html files, which are the display of the website."
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-base.html-file",
    "href": "posts/Homework3/HW3.html#the-base.html-file",
    "title": "Flask Website With User Inputs",
    "section": "The base.html File:",
    "text": "The base.html File:\n\nThe first html file that we must write is called base.html, which will contain the header for each page of the website.\nThe html code for this file is shown here. A breakdown of this code is given below the code.\n\n\n#1:  &lt;!doctype html&gt;\n#2:  &lt;html&gt;\n#3:  &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n\n#4:  &lt;h2&gt;A Simple Message Bank&lt;/h2&gt;\n\n#5:  &lt;nav&gt;\n#6:     &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n#7:        &lt;ul&gt;\n#8:           &lt;li&gt;&lt;a href=\"{{ url_for('render_submit_template') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n#9:           &lt;li&gt;&lt;a href=\"{{ url_for('view_random_messages')}}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n#10:       &lt;/ul&gt;\n\n#11: &lt;section class=\"content\"&gt;\n#12:    &lt;header&gt;\n#13.       {% block header %}{% endblock %}\n#14.    &lt;/header&gt;\n#15.       {% block content %}{% endblock %}\n#16. &lt;/section&gt;\n#17. &lt;/html&gt;\n\n\nLine 3: This line establishes the style of the website. The guidelines for the style are contained in the style.css file which we will go over later.\nLine 4: The header starts with a title: Simple Message Bank.\nLines 5-10: These lines create a block that will show the urls to move to another page on the website.\nLine 8: This line creates a link called “Submit a Message” that calls the render_submit_template() function created in the app.py file.\nLine 9: This line creates a link called “View Messages” that calls the view_random_messages() function created in the app.py file.\nLines 11-16: Create a block that will contain the information from the other html files.\nLine 13: This will contain the “header” block of the other html files.\nLine 15: This will cintain the “content” block of the other html files."
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-submit.html-file",
    "href": "posts/Homework3/HW3.html#the-submit.html-file",
    "title": "Flask Website With User Inputs",
    "section": "The submit.html File:",
    "text": "The submit.html File:\n\nWe will now create the submit.html file, which creates a webpage where the user can submit their handle and message.\nThe html code for this file is shown here. A breakdown of this code is given below the code.\n\n\n#1.  &lt;!doctype html&gt;\n#2.  &lt;html&gt;\n#3.  {% extends 'base.html' %}\n#4.     &lt;body&gt;\n         \n#5.        {% block header %}\n#6.           &lt;h2&gt;Submit&lt;/h2&gt;\n#7.        {% endblock %}\n#8.        {% block content %}\n#9.           &lt;form action = \"{{ url_for('render_submit_template') }}\" method = \"POST\"&gt;\n\n#10.             Your message:&lt;br&gt;\n#11.             &lt;input type = \"text\" name = \"message\" id = \"message\" /&gt;&lt;/br&gt;\n\n#12.             Your name or handle:&lt;br&gt;\n#13.             &lt;input type = \"text\" name = \"handle\" id = \"handle\" /&gt;&lt;/br&gt;\n\n#14.             &lt;input type = \"submit\" value = \"Submit message\" /&gt;&lt;br&gt;\n#15.          &lt;/form&gt;\n         \n#16.          &lt;h3&gt;{{ msg }}&lt;/h3&gt;\n#17.       {% endblock %}\n\n#18.    &lt;/body&gt;\n#19. &lt;/html&gt;\n\n\nLine 3: Makes the header of this webpage the header that is outlined in the base.html file above.\nLines 5-7: Fill in the “block header” portion of the base.html file.\nLine 6: Make “Submit” the title of this webpage block.\nLines 8-17: Fill in the “block content” portion of the base.html file.\nLine 9: Specify that the function of this webpage is to run the render_submit_template() function in the app.py file with the POST method.\nLines 10-11: Create a text box labeled “Your massage:” that allows a user to submit a message, binding their input to the object names “message”.\nLines 12-13: Create a text box labeled “Your name or handle:” that allows a user to submit a their name or handle, binding their input to the object names “handle”.\nLine 14: Create a button labeles “Submit message” that a user presses to submit thier entry, running the render_submit_template() function, which adds the entry to the SQL database.\nLine 16: Option for an additional message to be outputted once the user submits their message."
  },
  {
    "objectID": "posts/Homework3/HW3.html#the-view.html-file",
    "href": "posts/Homework3/HW3.html#the-view.html-file",
    "title": "Flask Website With User Inputs",
    "section": "The view.html File:",
    "text": "The view.html File:\n\nWe will now create the view.html file, which creates a webpage where the user can view 4 random previous entries to the website. This is the last html file that is needed for our website.\nThe html code for this file is shown here. A breakdown of this code is given below the code.\n\n\n#1.  &lt;!doctype html&gt;\n#2.  &lt;html&gt;\n#3.  {% extends 'base.html' %}\n#4.     &lt;body&gt;\n\n#5.        {% block header %}\n#6.           &lt;h2&gt;Some Cool Messages&lt;/h2&gt;\n#7.        {% endblock %}\n         \n#8.        {% block content %}\n#9.           {% if messages %}\n#10.             &lt;p&gt;\n#11.                {% for message in messages %}\n#12.                   &lt;p&gt;\n#13.                      &lt;br&gt;&lt;strong&gt;\"{{message[1]}}\"&lt;/strong&gt;&lt;/br&gt;\n#14.                      &lt;i&gt;-{{message[0]}}&lt;/i&gt;\n#15.                   &lt;/p&gt;\n#16.                {% endfor %}\n#17.             &lt;/p&gt;\n#18.          {% else %}\n#19.             &lt;p&gt;No messages available.&lt;/p&gt;\n#20.          {% endif %}      \n#21.       {% endblock %}\n\n#22.       &lt;/form&gt;\n\n#23.    &lt;/body&gt;\n#24. &lt;/html&gt;\n\n\nLine 3: Makes the header of this webpage the header that is outlined in the base.html file above.\nLines 5-7: Fill in the “block header” portion of the base.html file.\nLine 6: Make “Some Cool Messages” the title of this webpage block.\nLines 8-21: Fill in the “block content” portion of the base.html file.\nLine 9: Create an if statement that only runs if a messages object is inputted.\nLine 11: Create a for loop that loops through the inputs of messages.\nLine 13: Put the message in quotes and make it bold.\nLine 14: Put the handle of the user under their quote in italics with a dash in front.\nLine 18: If there is no messages object inputted, display “No messages avaliable.”"
  },
  {
    "objectID": "posts/Homework3/HW3.html#where-to-place-the-created-files",
    "href": "posts/Homework3/HW3.html#where-to-place-the-created-files",
    "title": "Flask Website With User Inputs",
    "section": "Where to Place the Created Files:",
    "text": "Where to Place the Created Files:\n\nFirst, create a folder for the webpage on your device called hw3_flask, and put the app.py file in that folder.\nInside the hw3_flask folder, create another folder called static and put the style.css file in that folder.\nAlso inside the hw3_flask folder, create another folder called templates and put the base.html, submit.html, and view.html files in that folder."
  },
  {
    "objectID": "posts/Homework3/HW3.html#how-to-run-the-app",
    "href": "posts/Homework3/HW3.html#how-to-run-the-app",
    "title": "Flask Website With User Inputs",
    "section": "How to Run the App:",
    "text": "How to Run the App:\n\nGo to the terminal and set the working directory to the hw3_flask folder. I did this by running:\n\ncd /Users/trentbellinger/Desktop/PIC 16B/Homework/Homework 3/hw3_flask\n\nThen, run the following command terminal:\n\npython app.py\n\nThis will give you a line in the terminal output that says running on http://192.168.0.150:4999 (with different numbers).\nCopy and paste this link into your browser and interact with the website."
  },
  {
    "objectID": "posts/Homework0/HW0.html",
    "href": "posts/Homework0/HW0.html",
    "title": "Homework 0",
    "section": "",
    "text": "We start by importing the necessary packages for this task. We will use pandas to load in the data into a data frame object, numpy to create an array for the visualization, and matplotlib.pyplot to visualize the data.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nIn order to figure out what type of visualization to make using the penguins dataset, we will first read in the dataset and look at its columns. This will allow us to formulate a question about the data that our visualization will answer.\n\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nThe dataset has columns studyName, Sample Number, Species, Region, Island, Stage, Individual ID, Clutch Completion, Date Egg, Culmen Length, Culmen Depth, Flipper Length, Body Mass, Sea, Delta 15 N, Delta 13 C, and Comments.\nFor this tutorial, we will specifically be looking at the Body Mass column because this can be indicative of the overall health and strength of the penguins.\nMore specifically, we will attempt to visualize how the body mass of a penguin depends on the sex of the penguin and the island that the penguin lives on.\nNow that we have our area of interest, we can formulate the question that our visualization will attempt to answer.\nThat question will be: How does the body mass of a penguin change based on its sex and the island that it lives on?\n\n\n\n\n\nWe start by grouping the observations in the dataset by island and sex and finding the median body mass of penguins for each category combination. This simple process is outlined below.\n\n\npenguins_by_island = penguins.groupby([\"Island\", \"Sex\"])[\"Body Mass (g)\"].median()\npenguins_by_island\n\nIsland     Sex   \nBiscoe     .         4875.0\n           FEMALE    4587.5\n           MALE      5350.0\nDream      FEMALE    3450.0\n           MALE      3950.0\nTorgersen  FEMALE    3400.0\n           MALE      4000.0\nName: Body Mass (g), dtype: float64\n\n\n\nAt first glance, it appears that male penguins at each island have higher mass that females on each island.\nIt also seems like the penguins on Biscoe Island have much higher mass than the other two islands.\nWe will see if we can visualize these observations.\n\n\n\n\n\nWe now have three variables that we want to visualize: median body mass, island, and sex.\nBody mass is a numeric variable, whereas island and sex are categorical variables.\nWith this specific combination of one numeric and two categorical variables, we can create a double bar graph to display the trends among these three variables.\nThe code for this bar graph is below. In short, this code block goes through this process:\n\n\nCreate a numpy array for the x-axis of the double bar plot.\nCreate the bars for the bar graph. The median body mass is the height of the bars and we have one bar for each sex-island combination.\nAdd labels to the visualization to make it more clear. The x-axis will be the different islands, the y-axis will be the median body mass, and the bars will be colored based on sex.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We will create a bar plot that groups by Island and Sex and shows the median \n# body mass of the penguins.\n\n# We will have 3 different ticks on the x-axis, each coprresponding to an island.\nX_axis = np.arange(3)\n\n# We plot the bars for the median body mass of female penguins by island, as \n# calculated above.\nplt.bar(X_axis - 0.2, [4587.5, 3450, 3400], 0.4, label = 'Female') \n\n# We plot the bars for the median body mass of male penguins by island, as \n# calculated above.\nplt.bar(X_axis + 0.2, [5350, 3950, 4000], 0.4, label = 'Male') \n  \nplt.xticks([0,1,2], [\"Biscoe\", \"Dream\", \"Torgersen\"]) \nplt.xlabel(\"Island\") \nplt.ylabel(\"Median Body Mass of Penguins\") \nplt.title(\"Body Mass of Penguins by Island and Sex\") \nplt.legend() \nplt.show() \n\n\n\n\n\n\n\n\n\nAs we can see from the visualization, Biscoe island has penguins with higher body mass than Dream island and Torgersen island. Also, male penguins on average have a greater body mass that female penguins on each island.\nThis visualization does not explain exactly why these trends are apparent, but it does allow us to speculate a little bit about the trends. One possible explanation is that there is more food for the penguins on Biscoe island, allowing the penguins to weight more."
  },
  {
    "objectID": "posts/Homework0/HW0.html#loading-in-and-viewing-the-data",
    "href": "posts/Homework0/HW0.html#loading-in-and-viewing-the-data",
    "title": "Homework 0",
    "section": "",
    "text": "In order to figure out what type of visualization to make using the penguins dataset, we will first read in the dataset and look at its columns. This will allow us to formulate a question about the data that our visualization will answer.\n\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nThe dataset has columns studyName, Sample Number, Species, Region, Island, Stage, Individual ID, Clutch Completion, Date Egg, Culmen Length, Culmen Depth, Flipper Length, Body Mass, Sea, Delta 15 N, Delta 13 C, and Comments.\nFor this tutorial, we will specifically be looking at the Body Mass column because this can be indicative of the overall health and strength of the penguins.\nMore specifically, we will attempt to visualize how the body mass of a penguin depends on the sex of the penguin and the island that the penguin lives on.\nNow that we have our area of interest, we can formulate the question that our visualization will attempt to answer.\nThat question will be: How does the body mass of a penguin change based on its sex and the island that it lives on?"
  },
  {
    "objectID": "posts/Homework0/HW0.html#data-preparation",
    "href": "posts/Homework0/HW0.html#data-preparation",
    "title": "Homework 0",
    "section": "",
    "text": "We start by grouping the observations in the dataset by island and sex and finding the median body mass of penguins for each category combination. This simple process is outlined below.\n\n\npenguins_by_island = penguins.groupby([\"Island\", \"Sex\"])[\"Body Mass (g)\"].median()\npenguins_by_island\n\nIsland     Sex   \nBiscoe     .         4875.0\n           FEMALE    4587.5\n           MALE      5350.0\nDream      FEMALE    3450.0\n           MALE      3950.0\nTorgersen  FEMALE    3400.0\n           MALE      4000.0\nName: Body Mass (g), dtype: float64\n\n\n\nAt first glance, it appears that male penguins at each island have higher mass that females on each island.\nIt also seems like the penguins on Biscoe Island have much higher mass than the other two islands.\nWe will see if we can visualize these observations."
  },
  {
    "objectID": "posts/Homework0/HW0.html#data-visualization",
    "href": "posts/Homework0/HW0.html#data-visualization",
    "title": "Homework 0",
    "section": "",
    "text": "We now have three variables that we want to visualize: median body mass, island, and sex.\nBody mass is a numeric variable, whereas island and sex are categorical variables.\nWith this specific combination of one numeric and two categorical variables, we can create a double bar graph to display the trends among these three variables.\nThe code for this bar graph is below. In short, this code block goes through this process:\n\n\nCreate a numpy array for the x-axis of the double bar plot.\nCreate the bars for the bar graph. The median body mass is the height of the bars and we have one bar for each sex-island combination.\nAdd labels to the visualization to make it more clear. The x-axis will be the different islands, the y-axis will be the median body mass, and the bars will be colored based on sex.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We will create a bar plot that groups by Island and Sex and shows the median \n# body mass of the penguins.\n\n# We will have 3 different ticks on the x-axis, each coprresponding to an island.\nX_axis = np.arange(3)\n\n# We plot the bars for the median body mass of female penguins by island, as \n# calculated above.\nplt.bar(X_axis - 0.2, [4587.5, 3450, 3400], 0.4, label = 'Female') \n\n# We plot the bars for the median body mass of male penguins by island, as \n# calculated above.\nplt.bar(X_axis + 0.2, [5350, 3950, 4000], 0.4, label = 'Male') \n  \nplt.xticks([0,1,2], [\"Biscoe\", \"Dream\", \"Torgersen\"]) \nplt.xlabel(\"Island\") \nplt.ylabel(\"Median Body Mass of Penguins\") \nplt.title(\"Body Mass of Penguins by Island and Sex\") \nplt.legend() \nplt.show() \n\n\n\n\n\n\n\n\n\nAs we can see from the visualization, Biscoe island has penguins with higher body mass than Dream island and Torgersen island. Also, male penguins on average have a greater body mass that female penguins on each island.\nThis visualization does not explain exactly why these trends are apparent, but it does allow us to speculate a little bit about the trends. One possible explanation is that there is more food for the penguins on Biscoe island, allowing the penguins to weight more."
  },
  {
    "objectID": "posts/Homework6/HW6.html",
    "href": "posts/Homework6/HW6.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "We first must import all necessary packages that will be used in the creation of the model.\n\n\n# packages to form the datasets:\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\n# packages to build and test the models\nimport keras\nfrom keras import layers, losses\nfrom keras import utils\nfrom keras.layers import TextVectorization\n\n# packages to visualize results:\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nprint(\"keras version \", keras.__version__)\n\nkeras version  3.0.5\n\n\n\nIt is also specified above that we are using keras 3 in this homework.\nNow that we have all the necessary packages imported, we will start looking at the data."
  },
  {
    "objectID": "posts/Homework6/HW6.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-6.-i-will-describe-the-creation-of-a-machine-learning-model-that-can-distinguish-between-fake-and-real-news.",
    "href": "posts/Homework6/HW6.html#this-blog-post-will-outline-my-completion-of-pic-16b-homework-6.-i-will-describe-the-creation-of-a-machine-learning-model-that-can-distinguish-between-fake-and-real-news.",
    "title": "Fake News Classification",
    "section": "",
    "text": "We first must import all necessary packages that will be used in the creation of the model.\n\n\n# packages to form the datasets:\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\n# packages to build and test the models\nimport keras\nfrom keras import layers, losses\nfrom keras import utils\nfrom keras.layers import TextVectorization\n\n# packages to visualize results:\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nprint(\"keras version \", keras.__version__)\n\nkeras version  3.0.5\n\n\n\nIt is also specified above that we are using keras 3 in this homework.\nNow that we have all the necessary packages imported, we will start looking at the data."
  },
  {
    "objectID": "posts/Homework6/HW6.html#make_dataset-function",
    "href": "posts/Homework6/HW6.html#make_dataset-function",
    "title": "Fake News Classification",
    "section": "make_dataset() function",
    "text": "make_dataset() function\n\nWe will create a function that takes in a pandas dataframe, converts all of its strings to lowercase, removes stop words, and outputs a tensorflow dataset that is batched to optimize training time.\nThis function is outlined below.\n\n\ndef make_dataset(df):\n    '''\n    Converts all strings to lowercase, removes stopwords, and returns a\n    tensorflow dataset.\n    Args:\n        df: a pandas dataframe of article data\n    Returns:\n        a tensorflow dataset with inputs title and text and output fake\n    '''\n    # convert the title and text columns to lowercase\n    df['title'] = df['title'].str.lower()\n    df['text'] = df['text'].str.lower()\n\n    # remove all stopwords from the title and text columns\n    stop = stopwords.words('english')\n    stop_fun = lambda x: ' '.join([word for word in x.split() if word not in stop])\n    df['title'] = df['title'].apply(stop_fun)\n    df['text'] = df['text'].apply(stop_fun)\n\n    # create a tensorflow dataset with inputs title and text and output fake\n    output = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                'title':df[['title']],\n                'text':df[['text']]\n            },\n            {\n                'fake':df[['fake']]\n            }\n        )\n    )\n    # batch the dataset by 100 and return\n    return output.batch(100)\n\n\nNow that we have a function to create a dataset, we will run it on our pandas dataframe from before.\n\n\n# create our main tensorflow dataset\ndf = make_dataset(df)\n\n\nWe now have a tensorflow dataset of our data, so we need to split it into training and validation to proceed with model creation."
  },
  {
    "objectID": "posts/Homework6/HW6.html#validation-data",
    "href": "posts/Homework6/HW6.html#validation-data",
    "title": "Fake News Classification",
    "section": "Validation Data",
    "text": "Validation Data\n\nWe will split the data into 20% validation and 80% training data to prepare for the creation of our models.\n\n\n# specify validation size to be 20% of dataset\nval_size = int(0.2*len(df))\n\n# get validation and training datasets\nval = df.take(val_size)\ntrain = df.skip(val_size)\n\n\nNow we have two datasets that we can train our model with, so we should establish a baseline performance expectation for our model"
  },
  {
    "objectID": "posts/Homework6/HW6.html#base-rate",
    "href": "posts/Homework6/HW6.html#base-rate",
    "title": "Fake News Classification",
    "section": "Base Rate",
    "text": "Base Rate\n\nWe will examine the proportion of fake and real news in our training data.\n\n\nout = np.empty(0)\n\n# for each entry to the dataset\nfor article, fake in train:\n    # add the \"fake\" column entries to the output array\n    out = np.append(out, fake['fake'].numpy().flatten())\n\nprint(\"Proportion of fake news: \", np.mean(out))\nprint(\"Proportion of real news: \", 1 - np.mean(out))\n\nProportion of fake news:  0.5243746169703047\nProportion of real news:  0.47562538302969526\n\n\n\nThe proportion of fake news in our training data is 0.5244, so our model must have accuracy higher than 0.5244 to outperform the base rate."
  },
  {
    "objectID": "posts/Homework6/HW6.html#text-vectorization",
    "href": "posts/Homework6/HW6.html#text-vectorization",
    "title": "Fake News Classification",
    "section": "Text Vectorization",
    "text": "Text Vectorization\n\nWe now need to convert the string inputs in our model into integers in order to input our data into the model.\nWe will do this by ordering the words by most to least common, keeping only the top 2000 most common words.\nThis process is outlined below.\n\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\n# convert the strings to lowercase and remove all punctuation\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n# create a layer that converrts strings to integers\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# apply the layer to the title and text columns in the training data\nvectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\nvectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\n\nNow that we have successfully vectorized the strings in our training data, we can proceed with model testing."
  },
  {
    "objectID": "posts/Homework6/HW6.html#model-using-only-title",
    "href": "posts/Homework6/HW6.html#model-using-only-title",
    "title": "Fake News Classification",
    "section": "Model Using Only Title",
    "text": "Model Using Only Title\n\nOur first model will only use the title of the article to predict whether it is fake or real.\nThe model is created below and its summary is outputted.\n\n\n# create model using only title as a predictor and output its summary\nmodel_title = get_model(\"title\")\nmodel_title.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ title (InputLayer)              │ (None, 1)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ text_vectorization              │ (None, 500)            │             0 │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (Embedding)           │ (None, 500, 3)         │         6,000 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 500, 3)         │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (None, 3)              │             0 │\n│ (GlobalAveragePooling1D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 3)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 32)             │           128 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 32)             │         1,056 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fake (Dense)                    │ (None, 2)              │            66 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 7,250 (28.32 KB)\n\n\n\n Trainable params: 7,250 (28.32 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nWe will now display the model in an easy-to read manner, done below.\n\n\n# plot the model layers in a flow chart format\nutils.plot_model(model_title, \"model_title.png\",\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\nAs we can see, the model has the following layers: input, text vectorization, embedding, dropout, global average pooling 1D, dropout, 3 dense.\nNow that we have created the model, we will fit the model using the training and validation sets above and 50 epochs.\n\n\n# fit the model on our training data, testing it on the validation data\nhistory_title = fit_model(model_title, train, val, 50)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 7ms/step - accuracy: 0.5240 - loss: 0.6922 - val_accuracy: 0.5173 - val_loss: 0.6892\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5633 - loss: 0.6810 - val_accuracy: 0.5731 - val_loss: 0.6190\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6860 - loss: 0.5798 - val_accuracy: 0.7824 - val_loss: 0.4840\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.7370 - loss: 0.5227 - val_accuracy: 0.7853 - val_loss: 0.4626\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.7598 - loss: 0.4886 - val_accuracy: 0.7938 - val_loss: 0.4436\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.7673 - loss: 0.4812 - val_accuracy: 0.8069 - val_loss: 0.4216\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7782 - loss: 0.4603 - val_accuracy: 0.8218 - val_loss: 0.4080\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7811 - loss: 0.4598 - val_accuracy: 0.8156 - val_loss: 0.4096\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7982 - loss: 0.4363 - val_accuracy: 0.8358 - val_loss: 0.3883\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.7953 - loss: 0.4386 - val_accuracy: 0.8407 - val_loss: 0.3711\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8021 - loss: 0.4322 - val_accuracy: 0.8147 - val_loss: 0.4076\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8009 - loss: 0.4259 - val_accuracy: 0.8498 - val_loss: 0.3626\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8193 - loss: 0.4018 - val_accuracy: 0.8564 - val_loss: 0.3437\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8054 - loss: 0.4173 - val_accuracy: 0.7980 - val_loss: 0.4135\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8258 - loss: 0.3883 - val_accuracy: 0.8284 - val_loss: 0.3629\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8246 - loss: 0.3837 - val_accuracy: 0.7960 - val_loss: 0.4116\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8334 - loss: 0.3684 - val_accuracy: 0.8069 - val_loss: 0.3955\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8409 - loss: 0.3552 - val_accuracy: 0.8751 - val_loss: 0.2955\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8478 - loss: 0.3413 - val_accuracy: 0.8496 - val_loss: 0.3242\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8517 - loss: 0.3375 - val_accuracy: 0.8418 - val_loss: 0.3339\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8643 - loss: 0.3227 - val_accuracy: 0.8213 - val_loss: 0.3710\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8608 - loss: 0.3216 - val_accuracy: 0.8547 - val_loss: 0.3114\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8611 - loss: 0.3118 - val_accuracy: 0.8589 - val_loss: 0.3041\n\n\n\nThe stop condition has stopped the fitting of the model after 23 epochs.\nAs we can see, the model stabilized between 0.82 and 0.86 validation accuracy. This is pretty good, but we can certainly do better using more predictors.\nWe will now visualize the accuracy of the model on the training and validation data.\n\n\n# plot training and validation accuracy calculated during model fitting\nplot_model_accuracy(history_title, \"Title\")\n\n\n\n\n\n\n\n\n\nAs we can see, the validation accuracy is rather unpredictable throughout the model fitting process, but it stabilizes at about 0.85.\nThroughout the model training process, the validation accuracy is about the same as the training accuracy. Hence, we do not have an issue with overfitting in this model.\nWe will now attempt to create a better model."
  },
  {
    "objectID": "posts/Homework6/HW6.html#model-using-only-text",
    "href": "posts/Homework6/HW6.html#model-using-only-text",
    "title": "Fake News Classification",
    "section": "Model Using Only Text",
    "text": "Model Using Only Text\n\nOur second model will only use the text of the article to predict whether it is fake or real.\nThe model is created below and its summary is outputted.\n\n\n# create model using only title as a predictor and output its summary\nmodel_text = get_model(\"text\")\nmodel_text.summary()\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 32)                  │           1,056 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              66 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 7,250 (28.32 KB)\n\n\n\n Trainable params: 7,250 (28.32 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nWe will now display the model in an easy-to-read manner, done below.\n\n\n# plot the model layers in a flow chart format\nutils.plot_model(model_text, \"model_text.png\",\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\nAs we can see, the model has the following layers: input, text vectorization, embedding, dropout, global average pooling 1D, dropout, 3 dense.\nNow that we have created the model, we will fit the model using the training and validation sets above and 50 epochs.\n\n\n# fit the model on our training data, testing it on the validation data\nhistory_text = fit_model(model_text, train, val, 50)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.5838 - loss: 0.6519 - val_accuracy: 0.8429 - val_loss: 0.3745\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.8719 - loss: 0.3239 - val_accuracy: 0.8991 - val_loss: 0.2387\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9171 - loss: 0.2319 - val_accuracy: 0.9127 - val_loss: 0.2092\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9284 - loss: 0.1982 - val_accuracy: 0.9193 - val_loss: 0.1918\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9406 - loss: 0.1791 - val_accuracy: 0.9231 - val_loss: 0.1851\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9416 - loss: 0.1665 - val_accuracy: 0.9242 - val_loss: 0.1828\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9450 - loss: 0.1587 - val_accuracy: 0.9562 - val_loss: 0.1539\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9481 - loss: 0.1480 - val_accuracy: 0.9367 - val_loss: 0.1609\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9499 - loss: 0.1396 - val_accuracy: 0.9371 - val_loss: 0.1575\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9550 - loss: 0.1334 - val_accuracy: 0.9624 - val_loss: 0.1386\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9546 - loss: 0.1280 - val_accuracy: 0.9427 - val_loss: 0.1455\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9603 - loss: 0.1199 - val_accuracy: 0.9416 - val_loss: 0.1459\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9563 - loss: 0.1214 - val_accuracy: 0.9418 - val_loss: 0.1447\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9596 - loss: 0.1140 - val_accuracy: 0.9631 - val_loss: 0.1337\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9611 - loss: 0.1102 - val_accuracy: 0.9407 - val_loss: 0.1492\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9622 - loss: 0.1072 - val_accuracy: 0.9400 - val_loss: 0.1528\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9638 - loss: 0.1045 - val_accuracy: 0.9391 - val_loss: 0.1529\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9659 - loss: 0.1006 - val_accuracy: 0.9431 - val_loss: 0.1425\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9635 - loss: 0.1035 - val_accuracy: 0.9480 - val_loss: 0.1295\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9687 - loss: 0.0925 - val_accuracy: 0.9422 - val_loss: 0.1474\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9652 - loss: 0.0954 - val_accuracy: 0.9498 - val_loss: 0.1258\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9670 - loss: 0.0943 - val_accuracy: 0.9718 - val_loss: 0.1146\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9689 - loss: 0.0931 - val_accuracy: 0.9478 - val_loss: 0.1313\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9719 - loss: 0.0845 - val_accuracy: 0.9398 - val_loss: 0.1534\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9690 - loss: 0.0870 - val_accuracy: 0.9431 - val_loss: 0.1481\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9710 - loss: 0.0833 - val_accuracy: 0.9467 - val_loss: 0.1355\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9701 - loss: 0.0813 - val_accuracy: 0.9433 - val_loss: 0.1454\n\n\n\nThe stop condition has stopped the fitting of the model after 27 epochs.\nAs we can see, the model stabilized between 0.93 and 0.95 validation accuracy. This is very good, but we may be able to do even better using both the title and text predictors.\nWe will now visualize the accuracy of the model on the training and validation data.\n\n\n# plot training and validation accuracy calculated during model fitting\nplot_model_accuracy(history_text, \"Text\")\n\n\n\n\n\n\n\n\n\nThroughout the model fitting process, the validation accuracy is slightly lower than the trainign accuracy. Hence, we may have a slight issue with overfitting in this model.\nThe validation accuracy seems to be very steady throughout the fitting process, staying between 0.92 and 0.95. Ths is very promising.\nWe will now attempt to create a better model."
  },
  {
    "objectID": "posts/Homework6/HW6.html#model-using-both-title-and-text",
    "href": "posts/Homework6/HW6.html#model-using-both-title-and-text",
    "title": "Fake News Classification",
    "section": "Model Using Both Title and Text",
    "text": "Model Using Both Title and Text\n\nOur third and final model will use both the title and the text of an article to predict whether it is fake or real.\nThe model is created below and its summary is outputted.\n\n\n# create model using title and text as predictors and output its summary\nmodel_both = get_model(\"both\")\nmodel_both.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)  │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text (InputLayer)   │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text_vectorization  │ (None, 500)       │          0 │ title[0][0],      │\n│ (TextVectorization) │                   │            │ text[0][0]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (None, 500, 3)    │      6,000 │ text_vectorizati… │\n│ (Embedding)         │                   │            │ text_vectorizati… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (Dropout) │ (None, 500, 3)    │          0 │ embedding[3][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (Dropout) │ (None, 500, 3)    │          0 │ embedding[4][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 3)         │          0 │ dropout_6[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 3)         │          0 │ dropout_8[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (Dropout) │ (None, 3)         │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_9 (Dropout) │ (None, 3)         │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (Dense)     │ (None, 32)        │        128 │ dropout_7[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_6 (Dense)     │ (None, 32)        │        128 │ dropout_9[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (None, 64)        │          0 │ dense_5[0][0],    │\n│ (Concatenate)       │                   │            │ dense_6[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_7 (Dense)     │ (None, 32)        │      2,080 │ concatenate_1[0]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ fake (Dense)        │ (None, 2)         │         66 │ dense_7[0][0]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 8,402 (32.82 KB)\n\n\n\n Trainable params: 8,402 (32.82 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nWe will now display the model in an easy-to-read manner, done below.\n\n\n# plot the model layers in a flow chart format\nutils.plot_model(model_both, \"model_both.png\",\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\nAs we can see, the model has two input layers: one for title and one for text. These layers share a text vectorization layer and an embedding layer. They then have separate layers for dropout, global average pooling, dropout, and dense. They are then cocatenated into a single layer. Then there is another two dense layers.\nNow that we have created the model, we will fit the model using the training and validation sets above and 50 epochs.\n\n\n# fit the model on our training data, testing it on the validation data\nhistory_both = fit_model(model_both, train, val, 50)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.7725 - loss: 0.5533 - val_accuracy: 0.9736 - val_loss: 0.1377\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9685 - loss: 0.1189 - val_accuracy: 0.9709 - val_loss: 0.0991\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9762 - loss: 0.0790 - val_accuracy: 0.9729 - val_loss: 0.0940\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9769 - loss: 0.0666 - val_accuracy: 0.9784 - val_loss: 0.0937\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9797 - loss: 0.0594 - val_accuracy: 0.9747 - val_loss: 0.0935\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9786 - loss: 0.0572 - val_accuracy: 0.9764 - val_loss: 0.0923\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9800 - loss: 0.0539 - val_accuracy: 0.9709 - val_loss: 0.1038\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9802 - loss: 0.0522 - val_accuracy: 0.9780 - val_loss: 0.1008\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9751 - loss: 0.0595 - val_accuracy: 0.9773 - val_loss: 0.0941\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9794 - loss: 0.0561 - val_accuracy: 0.9769 - val_loss: 0.0962\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9817 - loss: 0.0538 - val_accuracy: 0.9798 - val_loss: 0.0951\n\n\n\nThe stop condition has stopped the fitting of the model after 11 epochs.\nAs we can see, the model stabilized between 0.975 and 0.98 validation accuracy. This is our best result yet, so this will be our final model.\nWe will now visualize the accuracy of the model on the training and validation data.\n\n\n# plot training and validation accuracy calculated during model fitting\nplot_model_accuracy(history_both, \"Both Text and Title\")\n\n\n\n\n\n\n\n\n\nAs we can see, the validation accuracy is slightly lower than the training accuracy throughout the model fitting. This indicates that we may have a slight issue with overfitting in this model. However, the training and validation accuracy never differs by more than 0.01, so we can neglect this concern.\nThe validation accuracy is steady between 0.975 and 0.98 after the first 2 epochs. Ths is our best model yet, so we will use it as our final model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC 16B Website - Trent Bellinger",
    "section": "",
    "text": "Final Project Write-Up: Flight Delay Predictions\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Classification\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nDog and Cat Image Classification\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Dimensional Heat Diffusion Models\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nFlask Website With User Inputs\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraper for Movie and TV Show Reccomendations\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Historic Temperature Data for Different Countries\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nTrent Bellinger\n\n\n\n\n\n\nNo matching items"
  }
]